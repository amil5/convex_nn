{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8930e19",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` with Lottery\n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88316c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822c9da",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5ef8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2551b91c110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P[\"print_freq\"] = 5\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 2\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "P[\"ncvx_prune_epochs\"] = 25\n",
    "P[\"ncvx_prune_rounds\"] = 5\n",
    "P[\"ncvx_prune_perc\"] = 0.2\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 100\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "P['cvx_test_len'] = 10000\n",
    "\n",
    "P[\"cvx_prune_epochs\"] = 25\n",
    "P[\"cvx_prune_rounds\"] = 5\n",
    "P[\"cvx_prune_perc\"] = 0.2\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f680d0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bca5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "if True:\n",
    "    train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "P[\"cvx_n\"] = n\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386aca",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(path,P):\n",
    "    model = FCNetwork(P[\"num_neurons\"],P[\"num_classes\"],P[\"dim_in\"])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct.item()\n",
    "\n",
    "def ncvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-6\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "        _y = Variable(_y).to(device) # shape 1000\n",
    "        yhat = model(_x).float()\n",
    "        \n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, P[\"beta\"])/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "def ncvx_train(model, ds, ds_test, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"ncvx_prune_epochs\"] if prune else P[\"ncvx_num_epochs\"]\n",
    "    rounds = P[\"ncvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "    scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"losses_test\":[],\"accs_test\":[], \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"ncvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "            scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "            \n",
    "            print(\"Pruning Round [{:>2}/{:}]\".format(p,num_epochs))\n",
    "            d_out[\"nonzero_perc\"].append(print_nonzeros(model))\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = ncvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at = validation_primal(model, ds_test, P[\"beta\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [d_out[\"nonzero_perc\"][-1]]*train_iters\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], loss: {:.3f} acc: {:.3f}, TEST loss: {:.3f} test acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"losses\"][-1],d_out[\"accs\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "\n",
    "            if P[\"ncvx_schedule\"] > 0:\n",
    "                scheduler.step(losses[iter_no-1])\n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22a4de",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba2192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "==========================\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "print_freq         : 5\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 2\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n",
      "ncvx_prune_epochs  : 25\n",
      "ncvx_prune_rounds  : 5\n",
      "ncvx_prune_perc    : 0.2\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=P[\"batch_size\"], shuffle=True, pin_memory=True, sampler=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f05730f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2209fd0c60e24d8d97b5ab35a525ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/2], loss: 0.779 acc: 0.238, TEST loss: 0.784 test acc: 0.240\n",
      "Epoch [ 1/2], loss: 0.649 acc: 0.288, TEST loss: 0.665 test acc: 0.288\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"models/ncvx_nn{:}_solver{:}_l1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "model.apply(weight_init)\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(model,ncvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_ncvx = ncvx_train(model, train_loader, test_loader, P, prune=False)\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(model,ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67c2d7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>losses</th>\n",
       "      <th>accs</th>\n",
       "      <th>losses_test</th>\n",
       "      <th>accs_test</th>\n",
       "      <th>times</th>\n",
       "      <th>epoch</th>\n",
       "      <th>round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.085722</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>1.044231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484919</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>1.006202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.297691</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>1.089338</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.945541</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.999180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.323293</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.783738</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.662155</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.664654</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.989944</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.649460</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.664654</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.977210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.668582</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.664654</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>1.002912</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.664169</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.664654</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.991915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.649229</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.664654</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>1.007439</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      losses   accs  losses_test  accs_test     times  epoch  round\n",
       "0   4.085722  0.086     0.783738     0.2398  1.044231      0      0\n",
       "1   2.484919  0.111     0.783738     0.2398  1.006202      0      0\n",
       "2   2.297691  0.110     0.783738     0.2398  1.089338      0      0\n",
       "3   2.945541  0.132     0.783738     0.2398  0.999180      0      0\n",
       "4   2.323293  0.125     0.783738     0.2398  0.999911      0      0\n",
       "..       ...    ...          ...        ...       ...    ...    ...\n",
       "95  0.662155  0.290     0.664654     0.2882  0.989944      1      0\n",
       "96  0.649460  0.293     0.664654     0.2882  0.977210      1      0\n",
       "97  0.668582  0.241     0.664654     0.2882  1.002912      1      0\n",
       "98  0.664169  0.280     0.664654     0.2882  0.991915      1      0\n",
       "99  0.649229  0.288     0.664654     0.2882  1.007439      1      0\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c786a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc1351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c272be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfacfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8047b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938316cd",
   "metadata": {},
   "source": [
    "## Convex Network\n",
    "Haven't cleaned these up as much given that I'm not 100% sure how it works.\n",
    "TODO: Figure out the hyperplane arangement stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5edf3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b0ebe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.v - self.w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.v)\n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.v, dim=1)**2) + torch.sum(torch.norm(model.w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "\n",
    "\n",
    "def cvxproblem(ds, ds_test, save_path='', num_epochs=100, num_neurons=4096, beta=1e-3, \n",
    "                          learning_rate=1e-2, batch_size=1000, rho=1e-2, u_vectors=None, \n",
    "                          solver_type='sgd', LBFGS_param=(10,4), verbose=False, num_classes=10,\n",
    "                          dim_in=3072, n=50000, test_len=10000, device='cuda'):\n",
    "    \"\"\"\n",
    "    ds            : Training dataset (torch DataLoader)\n",
    "    ds_test       : Test dataset (torch DataLoader)\n",
    "    save_path     : str, path to save the model at (doesn't save if '')\n",
    "    num_epochs    : int\n",
    "    num_neurons   : hidden layer size, int\n",
    "    beta          : regularization scalar on the norms of the weight matrices, float\n",
    "    learning rate : float\n",
    "    batch_size    : int\n",
    "    rho           : float, coefficient to penalize the violated constraints\n",
    "    u_vectors     : Comes from sign patterns, tbd\n",
    "    solver_type   : any in ['sgd','adam','adagrad','adadelta','LBFGS']\n",
    "    schedule      : int in (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param   : (int,int) history size and max iterations for solver_type='LBFGS'\n",
    "    verbose       : bool\n",
    "    num_classes   : int\n",
    "    dim_in        : int, input dimension (32*32*3) for cifar10\n",
    "    n             : int, number of iterations? \n",
    "    test_len      : int, number of test iterations\n",
    "    device        : str, 'cuda' or 'cpu' \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model = custom_cvx_layer(num_neurons=num_neurons, num_classes=num_classes, input_dim=dim_in).to(device)\n",
    "    \n",
    "    optimizer = get_optimizer(model,solver_type,learning_rate,LBFGS_param)\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(n / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=verbose, factor=0.5, eps=1e-12)\n",
    "    \n",
    "    model.eval() # Evaluation mode\n",
    "    # Compute loss on entire test set\n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) \n",
    "    \n",
    "    iter_no = 0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        model.train() # Training mode\n",
    "        for ix, (_x, _y, _z) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _z = Variable(_z).to(device)\n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x, _z).float()\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, beta, rho, device)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y) # accuracy\n",
    "            #=======backward pass=====================================\n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)/len(_y)\n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device)\n",
    "        \n",
    "        if i % 5 == 0 or i == num_epochs - 1:\n",
    "            print(\"Epoch [{:>2}/{:}], noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}, TEST noncvx_loss: {:.3f} loss: {:.3f} test acc: {:.3f}\".format(i, num_epochs,\n",
    "                    noncvx_losses[iter_no-1], losses[iter_no-1], accs[iter_no-1], noncvx_losses_test[i+1]/test_len, \n",
    "                    losses_test[i+1]/test_len, accs_test[i+1]/test_len))\n",
    "        scheduler.step(losses[iter_no-1])\n",
    "        \n",
    "    if save_path != '':\n",
    "        save_model(model,save_path)\n",
    "    return {\"losses\":losses, \"accs\":accs, \"noncvx_losses\":noncvx_losses, \"losses_test\":losses_test/test_len,\n",
    "            \"accs_test\":accs_test/test_len, \"noncvx_losses_test\":noncvx_losses_test/test_len, \"times\":np.diff(times), \"model\":model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e84e",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cd19a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter         : Value\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 100\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n",
      "cvx_test_len      : 10000\n",
      "cvx_n             : 50000\n"
     ]
    }
   ],
   "source": [
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5d8eb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n"
     ]
    }
   ],
   "source": [
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=P[\"batch_size\"], shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4d3f99e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1e423b4564b8db91df170a4ba0778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.413 loss: 0.373 acc: 0.447, TEST noncvx_loss: 0.411 loss: 0.366 test acc: 0.470\n",
      "Epoch [ 1/100], noncvx_loss: 0.419 loss: 0.346 acc: 0.522, TEST noncvx_loss: 0.422 loss: 0.355 test acc: 0.491\n",
      "Epoch [ 2/100], noncvx_loss: 0.427 loss: 0.335 acc: 0.540, TEST noncvx_loss: 0.428 loss: 0.351 test acc: 0.503\n",
      "Epoch [ 3/100], noncvx_loss: 0.423 loss: 0.314 acc: 0.616, TEST noncvx_loss: 0.431 loss: 0.347 test acc: 0.512\n",
      "Epoch [ 4/100], noncvx_loss: 0.431 loss: 0.313 acc: 0.602, TEST noncvx_loss: 0.433 loss: 0.344 test acc: 0.518\n",
      "Epoch [ 5/100], noncvx_loss: 0.430 loss: 0.300 acc: 0.649, TEST noncvx_loss: 0.439 loss: 0.345 test acc: 0.510\n",
      "Epoch [ 6/100], noncvx_loss: 0.433 loss: 0.291 acc: 0.666, TEST noncvx_loss: 0.437 loss: 0.341 test acc: 0.523\n",
      "Epoch [ 7/100], noncvx_loss: 0.434 loss: 0.285 acc: 0.686, TEST noncvx_loss: 0.441 loss: 0.340 test acc: 0.523\n",
      "Epoch [ 8/100], noncvx_loss: 0.433 loss: 0.274 acc: 0.715, TEST noncvx_loss: 0.442 loss: 0.340 test acc: 0.528\n",
      "Epoch [ 9/100], noncvx_loss: 0.435 loss: 0.276 acc: 0.720, TEST noncvx_loss: 0.441 loss: 0.338 test acc: 0.526\n",
      "Epoch [10/100], noncvx_loss: 0.432 loss: 0.257 acc: 0.779, TEST noncvx_loss: 0.442 loss: 0.338 test acc: 0.532\n",
      "Epoch [11/100], noncvx_loss: 0.435 loss: 0.257 acc: 0.757, TEST noncvx_loss: 0.443 loss: 0.337 test acc: 0.533\n",
      "Epoch [12/100], noncvx_loss: 0.434 loss: 0.244 acc: 0.792, TEST noncvx_loss: 0.443 loss: 0.337 test acc: 0.535\n",
      "Epoch [13/100], noncvx_loss: 0.433 loss: 0.245 acc: 0.793, TEST noncvx_loss: 0.444 loss: 0.337 test acc: 0.535\n",
      "Epoch [14/100], noncvx_loss: 0.435 loss: 0.240 acc: 0.817, TEST noncvx_loss: 0.445 loss: 0.337 test acc: 0.536\n",
      "Epoch [15/100], noncvx_loss: 0.434 loss: 0.238 acc: 0.811, TEST noncvx_loss: 0.446 loss: 0.338 test acc: 0.535\n",
      "Epoch [16/100], noncvx_loss: 0.433 loss: 0.231 acc: 0.822, TEST noncvx_loss: 0.446 loss: 0.337 test acc: 0.538\n",
      "Epoch [17/100], noncvx_loss: 0.440 loss: 0.233 acc: 0.818, TEST noncvx_loss: 0.447 loss: 0.338 test acc: 0.537\n",
      "Epoch [18/100], noncvx_loss: 0.435 loss: 0.224 acc: 0.848, TEST noncvx_loss: 0.447 loss: 0.337 test acc: 0.540\n",
      "Epoch [19/100], noncvx_loss: 0.437 loss: 0.219 acc: 0.859, TEST noncvx_loss: 0.447 loss: 0.338 test acc: 0.536\n",
      "Epoch [20/100], noncvx_loss: 0.435 loss: 0.213 acc: 0.882, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.539\n",
      "Epoch [21/100], noncvx_loss: 0.433 loss: 0.211 acc: 0.864, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.543\n",
      "Epoch [22/100], noncvx_loss: 0.438 loss: 0.211 acc: 0.870, TEST noncvx_loss: 0.448 loss: 0.338 test acc: 0.540\n",
      "Epoch [23/100], noncvx_loss: 0.438 loss: 0.203 acc: 0.896, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.539\n",
      "Epoch [24/100], noncvx_loss: 0.434 loss: 0.201 acc: 0.889, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.536\n",
      "Epoch [25/100], noncvx_loss: 0.438 loss: 0.194 acc: 0.904, TEST noncvx_loss: 0.450 loss: 0.337 test acc: 0.543\n",
      "Epoch [26/100], noncvx_loss: 0.437 loss: 0.193 acc: 0.909, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.540\n",
      "Epoch [27/100], noncvx_loss: 0.439 loss: 0.194 acc: 0.901, TEST noncvx_loss: 0.450 loss: 0.338 test acc: 0.541\n",
      "Epoch [28/100], noncvx_loss: 0.440 loss: 0.195 acc: 0.917, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.541\n",
      "Epoch [29/100], noncvx_loss: 0.439 loss: 0.187 acc: 0.920, TEST noncvx_loss: 0.451 loss: 0.338 test acc: 0.539\n",
      "Epoch [30/100], noncvx_loss: 0.439 loss: 0.185 acc: 0.923, TEST noncvx_loss: 0.449 loss: 0.339 test acc: 0.543\n",
      "Epoch [31/100], noncvx_loss: 0.439 loss: 0.179 acc: 0.932, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.541\n",
      "Epoch [32/100], noncvx_loss: 0.433 loss: 0.172 acc: 0.939, TEST noncvx_loss: 0.449 loss: 0.339 test acc: 0.542\n",
      "Epoch [33/100], noncvx_loss: 0.437 loss: 0.172 acc: 0.937, TEST noncvx_loss: 0.450 loss: 0.339 test acc: 0.543\n",
      "Epoch [34/100], noncvx_loss: 0.439 loss: 0.176 acc: 0.921, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.543\n",
      "Epoch [35/100], noncvx_loss: 0.437 loss: 0.169 acc: 0.955, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.540\n",
      "Epoch [36/100], noncvx_loss: 0.434 loss: 0.166 acc: 0.959, TEST noncvx_loss: 0.452 loss: 0.340 test acc: 0.541\n",
      "Epoch [37/100], noncvx_loss: 0.438 loss: 0.168 acc: 0.950, TEST noncvx_loss: 0.450 loss: 0.340 test acc: 0.544\n",
      "Epoch [38/100], noncvx_loss: 0.440 loss: 0.162 acc: 0.954, TEST noncvx_loss: 0.452 loss: 0.341 test acc: 0.540\n",
      "Epoch [39/100], noncvx_loss: 0.444 loss: 0.162 acc: 0.967, TEST noncvx_loss: 0.451 loss: 0.341 test acc: 0.541\n",
      "Epoch [40/100], noncvx_loss: 0.437 loss: 0.156 acc: 0.952, TEST noncvx_loss: 0.451 loss: 0.341 test acc: 0.540\n",
      "Epoch [41/100], noncvx_loss: 0.442 loss: 0.160 acc: 0.955, TEST noncvx_loss: 0.452 loss: 0.341 test acc: 0.543\n",
      "Epoch [42/100], noncvx_loss: 0.444 loss: 0.161 acc: 0.944, TEST noncvx_loss: 0.453 loss: 0.342 test acc: 0.542\n",
      "Epoch [43/100], noncvx_loss: 0.442 loss: 0.154 acc: 0.965, TEST noncvx_loss: 0.452 loss: 0.342 test acc: 0.543\n",
      "Epoch [44/100], noncvx_loss: 0.436 loss: 0.151 acc: 0.971, TEST noncvx_loss: 0.451 loss: 0.342 test acc: 0.542\n",
      "Epoch [45/100], noncvx_loss: 0.438 loss: 0.145 acc: 0.970, TEST noncvx_loss: 0.451 loss: 0.342 test acc: 0.540\n",
      "Epoch [46/100], noncvx_loss: 0.437 loss: 0.146 acc: 0.973, TEST noncvx_loss: 0.453 loss: 0.343 test acc: 0.539\n",
      "Epoch [47/100], noncvx_loss: 0.436 loss: 0.142 acc: 0.975, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.539\n",
      "Epoch [48/100], noncvx_loss: 0.440 loss: 0.146 acc: 0.970, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.541\n",
      "Epoch [49/100], noncvx_loss: 0.440 loss: 0.141 acc: 0.971, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.542\n",
      "Epoch [50/100], noncvx_loss: 0.431 loss: 0.139 acc: 0.969, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.542\n",
      "Epoch [51/100], noncvx_loss: 0.439 loss: 0.134 acc: 0.968, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.540\n",
      "Epoch [52/100], noncvx_loss: 0.435 loss: 0.132 acc: 0.980, TEST noncvx_loss: 0.452 loss: 0.344 test acc: 0.545\n",
      "Epoch [53/100], noncvx_loss: 0.440 loss: 0.139 acc: 0.967, TEST noncvx_loss: 0.452 loss: 0.344 test acc: 0.543\n",
      "Epoch [54/100], noncvx_loss: 0.443 loss: 0.137 acc: 0.973, TEST noncvx_loss: 0.452 loss: 0.345 test acc: 0.540\n",
      "Epoch [55/100], noncvx_loss: 0.436 loss: 0.133 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.544\n",
      "Epoch [56/100], noncvx_loss: 0.443 loss: 0.132 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.345 test acc: 0.539\n",
      "Epoch [57/100], noncvx_loss: 0.443 loss: 0.134 acc: 0.969, TEST noncvx_loss: 0.453 loss: 0.345 test acc: 0.542\n",
      "Epoch [58/100], noncvx_loss: 0.437 loss: 0.129 acc: 0.984, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.541\n",
      "Epoch [59/100], noncvx_loss: 0.441 loss: 0.127 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.540\n",
      "Epoch [60/100], noncvx_loss: 0.436 loss: 0.126 acc: 0.979, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.541\n",
      "Epoch [61/100], noncvx_loss: 0.438 loss: 0.126 acc: 0.979, TEST noncvx_loss: 0.452 loss: 0.347 test acc: 0.541\n",
      "Epoch [62/100], noncvx_loss: 0.441 loss: 0.125 acc: 0.978, TEST noncvx_loss: 0.454 loss: 0.347 test acc: 0.540\n",
      "Epoch [63/100], noncvx_loss: 0.435 loss: 0.118 acc: 0.982, TEST noncvx_loss: 0.453 loss: 0.347 test acc: 0.539\n",
      "Epoch [64/100], noncvx_loss: 0.444 loss: 0.120 acc: 0.984, TEST noncvx_loss: 0.454 loss: 0.348 test acc: 0.538\n",
      "Epoch [65/100], noncvx_loss: 0.436 loss: 0.113 acc: 0.990, TEST noncvx_loss: 0.454 loss: 0.348 test acc: 0.539\n",
      "Epoch [66/100], noncvx_loss: 0.441 loss: 0.114 acc: 0.987, TEST noncvx_loss: 0.455 loss: 0.349 test acc: 0.539\n",
      "Epoch [67/100], noncvx_loss: 0.440 loss: 0.114 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.349 test acc: 0.537\n",
      "Epoch [68/100], noncvx_loss: 0.443 loss: 0.112 acc: 0.987, TEST noncvx_loss: 0.454 loss: 0.349 test acc: 0.539\n",
      "Epoch [69/100], noncvx_loss: 0.436 loss: 0.116 acc: 0.981, TEST noncvx_loss: 0.453 loss: 0.349 test acc: 0.540\n",
      "Epoch [70/100], noncvx_loss: 0.439 loss: 0.115 acc: 0.987, TEST noncvx_loss: 0.455 loss: 0.350 test acc: 0.540\n",
      "Epoch [71/100], noncvx_loss: 0.440 loss: 0.110 acc: 0.985, TEST noncvx_loss: 0.454 loss: 0.350 test acc: 0.537\n",
      "Epoch [72/100], noncvx_loss: 0.440 loss: 0.112 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.536\n",
      "Epoch [73/100], noncvx_loss: 0.439 loss: 0.111 acc: 0.985, TEST noncvx_loss: 0.455 loss: 0.351 test acc: 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], noncvx_loss: 0.436 loss: 0.105 acc: 0.988, TEST noncvx_loss: 0.454 loss: 0.350 test acc: 0.538\n",
      "Epoch [75/100], noncvx_loss: 0.436 loss: 0.107 acc: 0.988, TEST noncvx_loss: 0.453 loss: 0.350 test acc: 0.539\n",
      "Epoch [76/100], noncvx_loss: 0.438 loss: 0.109 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.539\n",
      "Epoch [77/100], noncvx_loss: 0.440 loss: 0.104 acc: 0.988, TEST noncvx_loss: 0.456 loss: 0.352 test acc: 0.538\n",
      "Epoch [78/100], noncvx_loss: 0.442 loss: 0.106 acc: 0.992, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.535\n",
      "Epoch [79/100], noncvx_loss: 0.442 loss: 0.104 acc: 0.989, TEST noncvx_loss: 0.455 loss: 0.352 test acc: 0.539\n",
      "Epoch [80/100], noncvx_loss: 0.442 loss: 0.100 acc: 0.991, TEST noncvx_loss: 0.455 loss: 0.352 test acc: 0.540\n",
      "Epoch [81/100], noncvx_loss: 0.442 loss: 0.103 acc: 0.987, TEST noncvx_loss: 0.454 loss: 0.352 test acc: 0.541\n",
      "Epoch [82/100], noncvx_loss: 0.443 loss: 0.100 acc: 0.992, TEST noncvx_loss: 0.455 loss: 0.353 test acc: 0.536\n",
      "Epoch [83/100], noncvx_loss: 0.440 loss: 0.098 acc: 0.991, TEST noncvx_loss: 0.454 loss: 0.353 test acc: 0.538\n",
      "Epoch [84/100], noncvx_loss: 0.442 loss: 0.098 acc: 0.992, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.536\n",
      "Epoch [85/100], noncvx_loss: 0.437 loss: 0.094 acc: 0.997, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.538\n",
      "Epoch [86/100], noncvx_loss: 0.433 loss: 0.096 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.539\n",
      "Epoch [87/100], noncvx_loss: 0.439 loss: 0.093 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.538\n",
      "Epoch [88/100], noncvx_loss: 0.437 loss: 0.092 acc: 0.996, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.536\n",
      "Epoch [89/100], noncvx_loss: 0.437 loss: 0.095 acc: 0.997, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.539\n",
      "Epoch [90/100], noncvx_loss: 0.441 loss: 0.094 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.535\n",
      "Epoch [91/100], noncvx_loss: 0.440 loss: 0.092 acc: 0.990, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.538\n",
      "Epoch [92/100], noncvx_loss: 0.439 loss: 0.094 acc: 0.996, TEST noncvx_loss: 0.454 loss: 0.355 test acc: 0.537\n",
      "Epoch [93/100], noncvx_loss: 0.443 loss: 0.092 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.356 test acc: 0.536\n",
      "Epoch [94/100], noncvx_loss: 0.443 loss: 0.089 acc: 0.988, TEST noncvx_loss: 0.456 loss: 0.356 test acc: 0.536\n",
      "Epoch [95/100], noncvx_loss: 0.441 loss: 0.093 acc: 0.990, TEST noncvx_loss: 0.455 loss: 0.356 test acc: 0.534\n",
      "Epoch [96/100], noncvx_loss: 0.439 loss: 0.087 acc: 0.993, TEST noncvx_loss: 0.456 loss: 0.357 test acc: 0.538\n",
      "Epoch [97/100], noncvx_loss: 0.436 loss: 0.087 acc: 0.989, TEST noncvx_loss: 0.456 loss: 0.357 test acc: 0.534\n",
      "Epoch [98/100], noncvx_loss: 0.440 loss: 0.085 acc: 0.996, TEST noncvx_loss: 0.455 loss: 0.357 test acc: 0.535\n",
      "Epoch [99/100], noncvx_loss: 0.439 loss: 0.082 acc: 0.998, TEST noncvx_loss: 0.456 loss: 0.358 test acc: 0.536\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "\n",
    "results_cvx = cvxproblem(ds_train, test_loader, save_path=cvx_save_loc,\n",
    "                         num_epochs = P[\"cvx_num_epochs\"],\n",
    "                         num_neurons = P[\"num_neurons\"], \n",
    "                         beta = P[\"beta\"],\n",
    "                         learning_rate = P[\"cvx_learning_rate\"],\n",
    "                         batch_size = P[\"batch_size\"],\n",
    "                         rho = P[\"cvx_rho\"],\n",
    "                         u_vectors = u_vectors,\n",
    "                         solver_type = P[\"cvx_solver\"],\n",
    "                         LBFGS_param = P[\"cvx_LBFGS_param\"],\n",
    "                         verbose = P[\"verbose\"],\n",
    "                         num_classes = P[\"num_classes\"],\n",
    "                         dim_in = P[\"dim_in\"],\n",
    "                         n = n,\n",
    "                         test_len = P['cvx_test_len'],\n",
    "                         device = P[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99558371",
   "metadata": {},
   "source": [
    "# Save Parameters (USE SAME FILENAME STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_save_loc = 'models/nn{:}_solver{:}_lr_PARAMS.json'.format(P['num_neurons'],P['cvx_solver'])\n",
    "save_params(P,param_save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8415a",
   "metadata": {},
   "source": [
    "# Save Training Data (Uses same filename style as model files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "feadbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NonConvex output to DataFrame\n",
    "ncvx_model = results_ncvx[\"model\"]\n",
    "del results_ncvx[\"model\"]\n",
    "\n",
    "ncvx_iters = int(np.ceil(P[\"ncvx_train_len\"] / P[\"batch_size\"]))\n",
    "results_ncvx[\"losses_test\"] = np.repeat(results_ncvx[\"losses_test\"][1:],ncvx_iters)\n",
    "results_ncvx[\"accs_test\"] = np.repeat(results_ncvx[\"accs_test\"][1:],ncvx_iters)\n",
    "results_ncvx[\"epoch\"] = np.repeat(np.arange(P[\"ncvx_num_epochs\"]),ncvx_iters)\n",
    "\n",
    "df_ncvx = pd.DataFrame.from_dict(results_ncvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncvx.to_csv(ncvx_save_loc+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex output to DataFrame\n",
    "cvx_model = results_cvx[\"model\"]\n",
    "del results_cvx[\"model\"]\n",
    "\n",
    "cvx_iters = int(np.ceil(P[\"cvx_n\"] / P[\"batch_size\"]))\n",
    "results_cvx[\"losses_test\"] = np.repeat(results_cvx[\"losses_test\"][1:],cvx_iters)\n",
    "results_cvx[\"accs_test\"] = np.repeat(results_cvx[\"accs_test\"][1:],cvx_iters)\n",
    "results_cvx[\"noncvx_losses_test\"] = np.repeat(results_cvx[\"noncvx_losses_test\"][1:],cvx_iters)\n",
    "results_cvx[\"epoch\"] = np.repeat(np.arange(P[\"cvx_num_epochs\"]),cvx_iters)\n",
    "\n",
    "df_cvx = pd.DataFrame.from_dict(results_cvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba1335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cvx.to_csv(cvx_save_loc+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228eb9f",
   "metadata": {},
   "source": [
    "# Lottery Ticket \n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705fa1c3",
   "metadata": {},
   "source": [
    "# TODO: \n",
    " * Better plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
