{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8930e19",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` with Lottery\n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88316c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c927fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helperfunctions\n",
    "reload(helperfunctions)\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822c9da",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5ef8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13303fdd110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P[\"print_freq\"] = 5\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 100\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "P[\"ncvx_prune_epochs\"] = 100 # 25\n",
    "P[\"ncvx_prune_rounds\"] = 1   # 5\n",
    "P[\"ncvx_prune_perc\"] = 80    # 20\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 100\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "P['cvx_test_len'] = 10000\n",
    "\n",
    "P[\"cvx_prune_epochs\"] = 100 # 25\n",
    "P[\"cvx_prune_rounds\"] = 1   # 5\n",
    "P[\"cvx_prune_perc\"] = 80    # 20\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f680d0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bca5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "P[\"cvx_n\"] = n\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386aca",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(path,P):\n",
    "    model = FCNetwork(P[\"num_neurons\"],P[\"num_classes\"],P[\"dim_in\"])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct.item()\n",
    "\n",
    "def ncvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-6\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "        _y = Variable(_y).to(device) # shape 1000\n",
    "        yhat = model(_x).float()\n",
    "        \n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, P[\"beta\"])/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "def ncvx_train(model, ds, ds_test, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"ncvx_prune_epochs\"] if prune else P[\"ncvx_num_epochs\"]\n",
    "    rounds = P[\"ncvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "    scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"losses_test\":[],\"accs_test\":[], \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"ncvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "            scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "            \n",
    "            print(\"\\nPruning Round [{:>2}/{:}]\".format(p,rounds))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = ncvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at = validation_primal(model, ds_test, P[\"beta\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], loss: {:.3f} acc: {:.3f}, TEST loss: {:.3f} test acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"losses\"][-1],d_out[\"accs\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "\n",
    "            if P[\"ncvx_schedule\"] > 0:\n",
    "                scheduler.step(d_out[\"losses\"][-1])\n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22a4de",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba2192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "==========================\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "print_freq         : 5\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 100\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n",
      "ncvx_prune_epochs  : 100\n",
      "ncvx_prune_rounds  : 1\n",
      "ncvx_prune_perc    : 80\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=P[\"batch_size\"], shuffle=True, pin_memory=True, sampler=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05730f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac587fb30947969107cabe5ea75224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.785 acc: 0.220, TEST loss: 0.779 test acc: 0.237\n",
      "Epoch [ 5/100], loss: 0.496 acc: 0.398, TEST loss: 0.549 test acc: 0.346\n",
      "Epoch [10/100], loss: 0.441 acc: 0.460, TEST loss: 0.502 test acc: 0.376\n",
      "Epoch [15/100], loss: 0.412 acc: 0.483, TEST loss: 0.475 test acc: 0.405\n",
      "Epoch [20/100], loss: 0.369 acc: 0.571, TEST loss: 0.459 test acc: 0.414\n",
      "Epoch [25/100], loss: 0.355 acc: 0.563, TEST loss: 0.447 test acc: 0.424\n",
      "Epoch [30/100], loss: 0.340 acc: 0.633, TEST loss: 0.438 test acc: 0.430\n",
      "Epoch [35/100], loss: 0.321 acc: 0.644, TEST loss: 0.432 test acc: 0.439\n",
      "Epoch [40/100], loss: 0.317 acc: 0.657, TEST loss: 0.425 test acc: 0.446\n",
      "Epoch [45/100], loss: 0.307 acc: 0.685, TEST loss: 0.422 test acc: 0.451\n",
      "Epoch [50/100], loss: 0.290 acc: 0.694, TEST loss: 0.418 test acc: 0.455\n",
      "Epoch [55/100], loss: 0.287 acc: 0.720, TEST loss: 0.415 test acc: 0.459\n",
      "Epoch [60/100], loss: 0.284 acc: 0.725, TEST loss: 0.413 test acc: 0.461\n",
      "Epoch [65/100], loss: 0.280 acc: 0.732, TEST loss: 0.413 test acc: 0.463\n",
      "Epoch [70/100], loss: 0.278 acc: 0.714, TEST loss: 0.410 test acc: 0.467\n",
      "Epoch [75/100], loss: 0.266 acc: 0.756, TEST loss: 0.409 test acc: 0.470\n",
      "Epoch [80/100], loss: 0.262 acc: 0.784, TEST loss: 0.408 test acc: 0.471\n",
      "Epoch [85/100], loss: 0.255 acc: 0.784, TEST loss: 0.406 test acc: 0.470\n",
      "Epoch [90/100], loss: 0.255 acc: 0.789, TEST loss: 0.407 test acc: 0.476\n",
      "Epoch [95/100], loss: 0.247 acc: 0.806, TEST loss: 0.406 test acc: 0.477\n",
      "Epoch [99/100], loss: 0.242 acc: 0.822, TEST loss: 0.406 test acc: 0.477\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"models/ncvx_nn{:}_solver{:}_l1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "model.apply(weight_init)\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,ncvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_ncvx = ncvx_train(model, train_loader, test_loader, P, prune=False)\n",
    "results_ncvx.to_csv(ncvx_save_loc+\"_EPOCHS{:}_Results.csv\".format(P[\"ncvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19af287",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c2d7e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruning Round [ 0/1]\n",
      "layer1.0.weight      | nonzeros = 2516583 / 12582912 ( 20.00%) | total_pruned = 10066329 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =    8192 /   40960 ( 20.00%) | total_pruned =   32768 | shape = (10, 4096)\n",
      "alive: 2524775, pruned : 10099097, total: 12623872, Compression rate :       5.00x  ( 80.00% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9d192ed0fd48b68ac15afba8f060c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.589 acc: 0.289, TEST loss: 0.614 test acc: 0.251\n",
      "Epoch [ 5/100], loss: 0.478 acc: 0.379, TEST loss: 0.510 test acc: 0.331\n",
      "Epoch [10/100], loss: 0.450 acc: 0.424, TEST loss: 0.485 test acc: 0.356\n",
      "Epoch [15/100], loss: 0.426 acc: 0.456, TEST loss: 0.471 test acc: 0.372\n",
      "Epoch [20/100], loss: 0.415 acc: 0.441, TEST loss: 0.462 test acc: 0.379\n",
      "Epoch [25/100], loss: 0.409 acc: 0.484, TEST loss: 0.455 test acc: 0.390\n",
      "Epoch [30/100], loss: 0.403 acc: 0.483, TEST loss: 0.449 test acc: 0.397\n",
      "Epoch [35/100], loss: 0.394 acc: 0.498, TEST loss: 0.445 test acc: 0.402\n",
      "Epoch [40/100], loss: 0.388 acc: 0.493, TEST loss: 0.441 test acc: 0.406\n",
      "Epoch [45/100], loss: 0.385 acc: 0.529, TEST loss: 0.438 test acc: 0.409\n",
      "Epoch [50/100], loss: 0.388 acc: 0.488, TEST loss: 0.435 test acc: 0.411\n",
      "Epoch [55/100], loss: 0.384 acc: 0.509, TEST loss: 0.433 test acc: 0.417\n",
      "Epoch [60/100], loss: 0.375 acc: 0.527, TEST loss: 0.431 test acc: 0.421\n",
      "Epoch [65/100], loss: 0.361 acc: 0.552, TEST loss: 0.429 test acc: 0.424\n",
      "Epoch [70/100], loss: 0.368 acc: 0.552, TEST loss: 0.427 test acc: 0.426\n",
      "Epoch [75/100], loss: 0.362 acc: 0.555, TEST loss: 0.426 test acc: 0.427\n",
      "Epoch [80/100], loss: 0.365 acc: 0.555, TEST loss: 0.424 test acc: 0.429\n",
      "Epoch [85/100], loss: 0.354 acc: 0.575, TEST loss: 0.423 test acc: 0.431\n",
      "Epoch [90/100], loss: 0.357 acc: 0.553, TEST loss: 0.421 test acc: 0.433\n",
      "Epoch [95/100], loss: 0.354 acc: 0.560, TEST loss: 0.420 test acc: 0.433\n",
      "Epoch [99/100], loss: 0.360 acc: 0.549, TEST loss: 0.419 test acc: 0.434\n"
     ]
    }
   ],
   "source": [
    "# Load from file just in case\n",
    "model_init = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(ncvx_save_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "prune_results_ncvx = ncvx_train(model, train_loader, test_loader, P, \n",
    "                                prune=True, re_init=False, init_state_dict=initial_state_dict, mask=mask)\n",
    "prune_results_ncvx.to_csv(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_Results.csv\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2c786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}.pth\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))\n",
    "\n",
    "with open(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_MASK.pkl\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e11a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ecd7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db299f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52024754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938316cd",
   "metadata": {},
   "source": [
    "# Convex Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ebe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.weight_v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.weight_w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.weight_v - self.weight_w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.weight_v)\n",
    "    Xw = torch.matmul(_x, model.weight_w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.weight_v, dim=1)**2) + torch.sum(torch.norm(model.weight_w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.weight_v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.weight_w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct.item(), test_noncvx_cost.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb58930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-12\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y, _z) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device)\n",
    "        _y = Variable(_y).to(device)\n",
    "        _z = Variable(_z).to(device)\n",
    "        yhat = model(_x, _z).float()\n",
    "        \n",
    "        loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, P[\"beta\"], P[\"cvx_rho\"], device)/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        \n",
    "        noncvx_loss = get_nonconvex_cost(one_hot(_y).to(device), model, _x, P[\"beta\"], device)/len(_y)\n",
    "        d_out[\"noncvx_losses\"].append(noncvx_loss.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "\n",
    "def cvx_train(model, ds, ds_test, u_vectors, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"cvx_prune_epochs\"] if prune else P[\"cvx_num_epochs\"]\n",
    "    rounds = P[\"cvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"noncvx_losses\": [], \"losses_test\":[],\"accs_test\":[], \"noncvx_losses_test\":[], \n",
    "             \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"cvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "            \n",
    "            print(\"Pruning Round [{:>2}/{:}]\".format(p,rounds))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = cvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at,nlt = validation_cvxproblem(model, ds_test, u_vectors, P[\"beta\"], P[\"cvx_rho\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"noncvx_losses_test\"] += [nlt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            \n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}, TEST noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"noncvx_losses\"][-1],d_out[\"losses\"][-1],d_out[\"accs\"][-1],\n",
    "                       d_out[\"noncvx_losses_test\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "            scheduler.step(d_out[\"losses\"][-1])\n",
    "            \n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e84e",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edf3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n",
      "Parameter         : Value\n",
      "=========================\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "print_freq        : 5\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 100\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n",
      "cvx_test_len      : 10000\n",
      "cvx_prune_epochs  : 25\n",
      "cvx_prune_rounds  : 5\n",
      "cvx_prune_perc    : 20\n"
     ]
    }
   ],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3f99e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1452a9befe541b6a942d2d7f0dada6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.408 loss: 0.366 acc: 0.477, TEST noncvx_loss: 0.411 loss: 0.366 acc: 0.467\n",
      "Epoch [ 5/100], noncvx_loss: 0.431 loss: 0.296 acc: 0.636, TEST noncvx_loss: 0.436 loss: 0.342 acc: 0.519\n",
      "Epoch [10/100], noncvx_loss: 0.436 loss: 0.258 acc: 0.765, TEST noncvx_loss: 0.443 loss: 0.338 acc: 0.529\n",
      "Epoch [15/100], noncvx_loss: 0.435 loss: 0.232 acc: 0.835, TEST noncvx_loss: 0.445 loss: 0.335 acc: 0.539\n",
      "Epoch [20/100], noncvx_loss: 0.438 loss: 0.215 acc: 0.867, TEST noncvx_loss: 0.450 loss: 0.338 acc: 0.544\n",
      "Epoch [25/100], noncvx_loss: 0.440 loss: 0.200 acc: 0.886, TEST noncvx_loss: 0.451 loss: 0.337 acc: 0.543\n",
      "Epoch [30/100], noncvx_loss: 0.439 loss: 0.184 acc: 0.934, TEST noncvx_loss: 0.452 loss: 0.339 acc: 0.541\n",
      "Epoch [35/100], noncvx_loss: 0.437 loss: 0.169 acc: 0.938, TEST noncvx_loss: 0.452 loss: 0.340 acc: 0.541\n",
      "Epoch [40/100], noncvx_loss: 0.440 loss: 0.159 acc: 0.952, TEST noncvx_loss: 0.452 loss: 0.341 acc: 0.541\n",
      "Epoch [45/100], noncvx_loss: 0.443 loss: 0.154 acc: 0.970, TEST noncvx_loss: 0.452 loss: 0.342 acc: 0.539\n",
      "Epoch [50/100], noncvx_loss: 0.443 loss: 0.143 acc: 0.967, TEST noncvx_loss: 0.454 loss: 0.344 acc: 0.541\n",
      "Epoch [55/100], noncvx_loss: 0.436 loss: 0.133 acc: 0.974, TEST noncvx_loss: 0.453 loss: 0.344 acc: 0.541\n",
      "Epoch [60/100], noncvx_loss: 0.438 loss: 0.128 acc: 0.976, TEST noncvx_loss: 0.454 loss: 0.347 acc: 0.540\n",
      "Epoch [65/100], noncvx_loss: 0.441 loss: 0.116 acc: 0.986, TEST noncvx_loss: 0.455 loss: 0.348 acc: 0.540\n",
      "Epoch [70/100], noncvx_loss: 0.433 loss: 0.114 acc: 0.984, TEST noncvx_loss: 0.454 loss: 0.349 acc: 0.536\n",
      "Epoch [75/100], noncvx_loss: 0.440 loss: 0.107 acc: 0.991, TEST noncvx_loss: 0.454 loss: 0.351 acc: 0.539\n",
      "Epoch [80/100], noncvx_loss: 0.443 loss: 0.101 acc: 0.991, TEST noncvx_loss: 0.455 loss: 0.353 acc: 0.534\n",
      "Epoch [85/100], noncvx_loss: 0.435 loss: 0.095 acc: 0.990, TEST noncvx_loss: 0.454 loss: 0.353 acc: 0.536\n",
      "Epoch [90/100], noncvx_loss: 0.443 loss: 0.091 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.355 acc: 0.539\n",
      "Epoch [95/100], noncvx_loss: 0.439 loss: 0.089 acc: 0.998, TEST noncvx_loss: 0.456 loss: 0.356 acc: 0.537\n",
      "Epoch [99/100], noncvx_loss: 0.436 loss: 0.085 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.357 acc: 0.533\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,cvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, prune=False)\n",
    "results_cvx.to_csv(cvx_save_loc+\"_EPOCHS{:}_Results.csv\".format(P[\"cvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,cvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34657c66",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1293d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Round [ 0/25]\n",
      "weight_v             | nonzeros = 100663299 / 125829120 ( 80.00%) | total_pruned = 25165821 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 100663298 / 125829120 ( 80.00%) | total_pruned = 25165822 | shape = (4096, 3072, 10)\n",
      "alive: 201326597, pruned : 50331643, total: 251658240, Compression rate :       1.25x  (  2.93% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\\helperfunctions.py:96: RuntimeWarning: overflow encountered in long_scalars\n",
      "  print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9fb8a07de54ca5b73c64a59b62a5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], noncvx_loss: 0.437 loss: 0.083 acc: 0.993, TEST noncvx_loss: 0.454 loss: 0.356 acc: 0.537\n",
      "Epoch [ 5/25], noncvx_loss: 0.438 loss: 0.081 acc: 0.995, TEST noncvx_loss: 0.453 loss: 0.356 acc: 0.537\n",
      "Epoch [10/25], noncvx_loss: 0.439 loss: 0.082 acc: 0.997, TEST noncvx_loss: 0.453 loss: 0.356 acc: 0.538\n",
      "Epoch [15/25], noncvx_loss: 0.440 loss: 0.080 acc: 0.993, TEST noncvx_loss: 0.453 loss: 0.356 acc: 0.537\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [20/25], noncvx_loss: 0.439 loss: 0.082 acc: 0.993, TEST noncvx_loss: 0.452 loss: 0.356 acc: 0.537\n",
      "Epoch [24/25], noncvx_loss: 0.433 loss: 0.079 acc: 0.995, TEST noncvx_loss: 0.452 loss: 0.355 acc: 0.538\n",
      "Pruning Round [ 1/25]\n",
      "weight_v             | nonzeros = 80530642 / 125829120 ( 64.00%) | total_pruned = 45298478 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 80530642 / 125829120 ( 64.00%) | total_pruned = 45298478 | shape = (4096, 3072, 10)\n",
      "alive: 161061284, pruned : 90596956, total: 251658240, Compression rate :       1.56x  (  1.87% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71a37e4f8d44fa2a997f86895effbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], noncvx_loss: 0.441 loss: 0.091 acc: 0.989, TEST noncvx_loss: 0.451 loss: 0.355 acc: 0.538\n",
      "Epoch [ 5/25], noncvx_loss: 0.436 loss: 0.085 acc: 0.994, TEST noncvx_loss: 0.451 loss: 0.354 acc: 0.537\n",
      "Epoch [10/25], noncvx_loss: 0.440 loss: 0.083 acc: 0.995, TEST noncvx_loss: 0.451 loss: 0.354 acc: 0.538\n",
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [15/25], noncvx_loss: 0.438 loss: 0.085 acc: 0.992, TEST noncvx_loss: 0.450 loss: 0.354 acc: 0.537\n",
      "Epoch [20/25], noncvx_loss: 0.435 loss: 0.084 acc: 0.994, TEST noncvx_loss: 0.450 loss: 0.354 acc: 0.540\n",
      "Epoch [24/25], noncvx_loss: 0.436 loss: 0.083 acc: 0.995, TEST noncvx_loss: 0.451 loss: 0.354 acc: 0.538\n",
      "Pruning Round [ 2/25]\n",
      "weight_v             | nonzeros = 64424516 / 125829120 ( 51.20%) | total_pruned = 61404604 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 64424516 / 125829120 ( 51.20%) | total_pruned = 61404604 | shape = (4096, 3072, 10)\n",
      "alive: 128849032, pruned : 122809208, total: 251658240, Compression rate :       1.95x  ( -2.40% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49fc6ddd62e4850b838cefa1549ec52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], noncvx_loss: 0.436 loss: 0.089 acc: 0.996, TEST noncvx_loss: 0.450 loss: 0.352 acc: 0.538\n",
      "Epoch [ 5/25], noncvx_loss: 0.438 loss: 0.089 acc: 0.998, TEST noncvx_loss: 0.449 loss: 0.351 acc: 0.538\n",
      "Epoch [10/25], noncvx_loss: 0.433 loss: 0.085 acc: 0.996, TEST noncvx_loss: 0.449 loss: 0.351 acc: 0.539\n",
      "Epoch [15/25], noncvx_loss: 0.432 loss: 0.093 acc: 0.988, TEST noncvx_loss: 0.448 loss: 0.351 acc: 0.540\n",
      "Epoch [20/25], noncvx_loss: 0.434 loss: 0.087 acc: 0.996, TEST noncvx_loss: 0.448 loss: 0.351 acc: 0.538\n",
      "Epoch    22: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [24/25], noncvx_loss: 0.431 loss: 0.089 acc: 0.991, TEST noncvx_loss: 0.448 loss: 0.351 acc: 0.538\n",
      "Pruning Round [ 3/25]\n",
      "weight_v             | nonzeros = 51539613 / 125829120 ( 40.96%) | total_pruned = 74289507 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 51539615 / 125829120 ( 40.96%) | total_pruned = 74289505 | shape = (4096, 3072, 10)\n",
      "alive: 103079228, pruned : 148579012, total: 251658240, Compression rate :       2.44x  (  7.84% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177a01cdc55f4678bd3efabafaeee98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], noncvx_loss: 0.438 loss: 0.100 acc: 0.994, TEST noncvx_loss: 0.447 loss: 0.349 acc: 0.540\n",
      "Epoch [ 5/25], noncvx_loss: 0.433 loss: 0.099 acc: 0.996, TEST noncvx_loss: 0.446 loss: 0.348 acc: 0.540\n",
      "Epoch [10/25], noncvx_loss: 0.433 loss: 0.092 acc: 0.997, TEST noncvx_loss: 0.446 loss: 0.348 acc: 0.539\n",
      "Epoch [15/25], noncvx_loss: 0.431 loss: 0.097 acc: 0.990, TEST noncvx_loss: 0.445 loss: 0.348 acc: 0.540\n",
      "Epoch [20/25], noncvx_loss: 0.427 loss: 0.096 acc: 0.992, TEST noncvx_loss: 0.444 loss: 0.348 acc: 0.541\n",
      "Epoch    22: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [24/25], noncvx_loss: 0.426 loss: 0.095 acc: 0.993, TEST noncvx_loss: 0.444 loss: 0.348 acc: 0.539\n",
      "Pruning Round [ 4/25]\n",
      "weight_v             | nonzeros = 41231691 / 125829120 ( 32.77%) | total_pruned = 84597429 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 41231695 / 125829120 ( 32.77%) | total_pruned = 84597425 | shape = (4096, 3072, 10)\n",
      "alive: 82463386, pruned : 169194854, total: 251658240, Compression rate :       3.05x  ( -1.03% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7babc2cbd52344e093566153582a4cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], noncvx_loss: 0.432 loss: 0.113 acc: 0.993, TEST noncvx_loss: 0.447 loss: 0.346 acc: 0.540\n",
      "Epoch [ 5/25], noncvx_loss: 0.429 loss: 0.107 acc: 0.998, TEST noncvx_loss: 0.445 loss: 0.345 acc: 0.541\n",
      "Epoch [10/25], noncvx_loss: 0.433 loss: 0.108 acc: 0.993, TEST noncvx_loss: 0.444 loss: 0.345 acc: 0.543\n",
      "Epoch [15/25], noncvx_loss: 0.430 loss: 0.110 acc: 0.991, TEST noncvx_loss: 0.443 loss: 0.345 acc: 0.543\n",
      "Epoch [20/25], noncvx_loss: 0.430 loss: 0.107 acc: 0.993, TEST noncvx_loss: 0.443 loss: 0.344 acc: 0.545\n",
      "Epoch    21: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [24/25], noncvx_loss: 0.429 loss: 0.106 acc: 0.990, TEST noncvx_loss: 0.442 loss: 0.344 acc: 0.544\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "# Load from file just in case\n",
    "model_init = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(cvx_save_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(cvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "prune_results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, \n",
    "                              prune=True, re_init=False, init_state_dict=initial_state_dict_post, mask=mask)\n",
    "prune_results_cvx.to_csv(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_Results.csv\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01311385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}.pth\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))\n",
    "\n",
    "with open(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_MASK.pkl\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86b11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2f7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99558371",
   "metadata": {},
   "source": [
    "# Save Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_save_loc = 'models/nn{:}_solver{:}_lr_PARAMS.json'.format(P['num_neurons'],P['cvx_solver'])\n",
    "save_params(P,param_save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228eb9f",
   "metadata": {},
   "source": [
    "# Lottery Ticket \n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helperfunctions\n",
    "reload(helperfunctions)\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fa1c3",
   "metadata": {},
   "source": [
    "# TODO: \n",
    " * Better plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
