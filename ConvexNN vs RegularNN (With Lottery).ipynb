{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8930e19",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` with Lottery\n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88316c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helperfunctions\n",
    "reload(helperfunctions)\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822c9da",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5ef8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2190711e110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P[\"print_freq\"] = 5\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 100\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "P[\"ncvx_prune_epochs\"] = 25\n",
    "P[\"ncvx_prune_rounds\"] = 5\n",
    "P[\"ncvx_prune_perc\"] = 20\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 100\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "P['cvx_test_len'] = 10000\n",
    "\n",
    "P[\"cvx_prune_epochs\"] = 25\n",
    "P[\"cvx_prune_rounds\"] = 5\n",
    "P[\"cvx_prune_perc\"] = 20\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f680d0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bca5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "P[\"cvx_n\"] = n\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386aca",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(path,P):\n",
    "    model = FCNetwork(P[\"num_neurons\"],P[\"num_classes\"],P[\"dim_in\"])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct.item()\n",
    "\n",
    "def ncvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-6\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "        _y = Variable(_y).to(device) # shape 1000\n",
    "        yhat = model(_x).float()\n",
    "        \n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, P[\"beta\"])/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "def ncvx_train(model, ds, ds_test, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"ncvx_prune_epochs\"] if prune else P[\"ncvx_num_epochs\"]\n",
    "    rounds = P[\"ncvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "    scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"losses_test\":[],\"accs_test\":[], \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"ncvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "            scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "            \n",
    "            print(\"\\nPruning Round [{:>2}/{:}]\".format(p,rounds))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = ncvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at = validation_primal(model, ds_test, P[\"beta\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], loss: {:.3f} acc: {:.3f}, TEST loss: {:.3f} test acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"losses\"][-1],d_out[\"accs\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "\n",
    "            if P[\"ncvx_schedule\"] > 0:\n",
    "                scheduler.step(d_out[\"losses\"][-1])\n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22a4de",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba2192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "==========================\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "print_freq         : 5\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 100\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n",
      "ncvx_prune_epochs  : 25\n",
      "ncvx_prune_rounds  : 5\n",
      "ncvx_prune_perc    : 20\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=P[\"batch_size\"], shuffle=True, pin_memory=True, sampler=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f05730f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7351fe76d44bf9a8aed65eebd486b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.770 acc: 0.240, TEST loss: 0.787 test acc: 0.239\n",
      "Epoch [ 5/100], loss: 0.503 acc: 0.397, TEST loss: 0.553 test acc: 0.344\n",
      "Epoch [10/100], loss: 0.440 acc: 0.455, TEST loss: 0.504 test acc: 0.378\n",
      "Epoch [15/100], loss: 0.398 acc: 0.507, TEST loss: 0.477 test acc: 0.398\n",
      "Epoch [20/100], loss: 0.379 acc: 0.531, TEST loss: 0.460 test acc: 0.415\n",
      "Epoch [25/100], loss: 0.342 acc: 0.629, TEST loss: 0.449 test acc: 0.425\n",
      "Epoch [30/100], loss: 0.336 acc: 0.628, TEST loss: 0.439 test acc: 0.433\n",
      "Epoch [35/100], loss: 0.322 acc: 0.660, TEST loss: 0.432 test acc: 0.438\n",
      "Epoch [40/100], loss: 0.315 acc: 0.646, TEST loss: 0.427 test acc: 0.442\n",
      "Epoch [45/100], loss: 0.308 acc: 0.678, TEST loss: 0.423 test acc: 0.449\n",
      "Epoch [50/100], loss: 0.302 acc: 0.689, TEST loss: 0.420 test acc: 0.456\n",
      "Epoch [55/100], loss: 0.296 acc: 0.694, TEST loss: 0.417 test acc: 0.454\n",
      "Epoch [60/100], loss: 0.286 acc: 0.738, TEST loss: 0.414 test acc: 0.463\n",
      "Epoch [65/100], loss: 0.273 acc: 0.749, TEST loss: 0.412 test acc: 0.459\n",
      "Epoch [70/100], loss: 0.267 acc: 0.760, TEST loss: 0.411 test acc: 0.465\n",
      "Epoch [75/100], loss: 0.269 acc: 0.752, TEST loss: 0.410 test acc: 0.463\n",
      "Epoch [80/100], loss: 0.262 acc: 0.771, TEST loss: 0.409 test acc: 0.468\n",
      "Epoch [85/100], loss: 0.263 acc: 0.771, TEST loss: 0.408 test acc: 0.469\n",
      "Epoch [90/100], loss: 0.263 acc: 0.776, TEST loss: 0.408 test acc: 0.470\n",
      "Epoch [95/100], loss: 0.245 acc: 0.813, TEST loss: 0.407 test acc: 0.473\n",
      "Epoch [99/100], loss: 0.251 acc: 0.796, TEST loss: 0.407 test acc: 0.470\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"models/ncvx_nn{:}_solver{:}_l1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "model.apply(weight_init)\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,ncvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_ncvx = ncvx_train(model, train_loader, test_loader, P, prune=False)\n",
    "results_ncvx.to_csv(ncvx_save_loc+\"_EPOCHS{:}_Results\".format(P[\"ncvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeff83a",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c2d7e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Round [ 0/5]\n",
      "layer1.0.weight      | nonzeros = 10066329 / 12582912 ( 80.00%) | total_pruned = 2516583 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =   32768 /   40960 ( 80.00%) | total_pruned =    8192 | shape = (10, 4096)\n",
      "alive: 10099097, pruned : 2524775, total: 12623872, Compression rate :       1.25x  ( 20.00% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605d227b3c2242a2b2ffbea195b36abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], loss: 0.695 acc: 0.288, TEST loss: 0.774 test acc: 0.257\n",
      "Epoch [ 5/25], loss: 0.526 acc: 0.398, TEST loss: 0.599 test acc: 0.327\n",
      "Epoch [10/25], loss: 0.470 acc: 0.461, TEST loss: 0.559 test acc: 0.357\n",
      "Epoch [15/25], loss: 0.447 acc: 0.475, TEST loss: 0.538 test acc: 0.373\n",
      "Epoch [20/25], loss: 0.423 acc: 0.499, TEST loss: 0.524 test acc: 0.385\n",
      "Epoch [24/25], loss: 0.403 acc: 0.551, TEST loss: 0.516 test acc: 0.387\n",
      "Pruning Round [ 1/5]\n",
      "layer1.0.weight      | nonzeros = 8053063 / 12582912 ( 64.00%) | total_pruned = 4529849 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =   26214 /   40960 ( 64.00%) | total_pruned =   14746 | shape = (10, 4096)\n",
      "alive: 8079277, pruned : 4544595, total: 12623872, Compression rate :       1.56x  ( 36.00% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb166995fe4f76adc0c909aed8903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], loss: 0.712 acc: 0.262, TEST loss: 0.730 test acc: 0.256\n",
      "Epoch [ 5/25], loss: 0.548 acc: 0.357, TEST loss: 0.598 test acc: 0.319\n",
      "Epoch [10/25], loss: 0.503 acc: 0.435, TEST loss: 0.564 test acc: 0.348\n",
      "Epoch [15/25], loss: 0.472 acc: 0.427, TEST loss: 0.543 test acc: 0.358\n",
      "Epoch [20/25], loss: 0.448 acc: 0.479, TEST loss: 0.530 test acc: 0.366\n",
      "Epoch [24/25], loss: 0.439 acc: 0.484, TEST loss: 0.522 test acc: 0.374\n",
      "Pruning Round [ 2/5]\n",
      "layer1.0.weight      | nonzeros = 6442450 / 12582912 ( 51.20%) | total_pruned = 6140462 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =   20971 /   40960 ( 51.20%) | total_pruned =   19989 | shape = (10, 4096)\n",
      "alive: 6463421, pruned : 6160451, total: 12623872, Compression rate :       1.95x  ( 48.80% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a050643928465188654b09caf527c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], loss: 0.633 acc: 0.318, TEST loss: 0.686 test acc: 0.275\n",
      "Epoch [ 5/25], loss: 0.547 acc: 0.367, TEST loss: 0.580 test acc: 0.326\n",
      "Epoch [10/25], loss: 0.489 acc: 0.430, TEST loss: 0.551 test acc: 0.347\n",
      "Epoch [15/25], loss: 0.470 acc: 0.429, TEST loss: 0.535 test acc: 0.360\n",
      "Epoch [20/25], loss: 0.442 acc: 0.465, TEST loss: 0.524 test acc: 0.362\n",
      "Epoch [24/25], loss: 0.431 acc: 0.477, TEST loss: 0.516 test acc: 0.375\n",
      "Pruning Round [ 3/5]\n",
      "layer1.0.weight      | nonzeros = 5153960 / 12582912 ( 40.96%) | total_pruned = 7428952 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =   16777 /   40960 ( 40.96%) | total_pruned =   24183 | shape = (10, 4096)\n",
      "alive: 5170737, pruned : 7453135, total: 12623872, Compression rate :       2.44x  ( 59.04% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55db7e0a567e414fad34cfe718ce85d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], loss: 0.601 acc: 0.323, TEST loss: 0.653 test acc: 0.272\n",
      "Epoch [ 5/25], loss: 0.507 acc: 0.374, TEST loss: 0.564 test acc: 0.328\n",
      "Epoch [10/25], loss: 0.481 acc: 0.414, TEST loss: 0.539 test acc: 0.342\n",
      "Epoch [15/25], loss: 0.464 acc: 0.434, TEST loss: 0.524 test acc: 0.359\n",
      "Epoch [20/25], loss: 0.456 acc: 0.433, TEST loss: 0.514 test acc: 0.365\n",
      "Epoch [24/25], loss: 0.444 acc: 0.442, TEST loss: 0.508 test acc: 0.371\n",
      "Pruning Round [ 4/5]\n",
      "layer1.0.weight      | nonzeros = 4123169 / 12582912 ( 32.77%) | total_pruned = 8459743 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =   13421 /   40960 ( 32.77%) | total_pruned =   27539 | shape = (10, 4096)\n",
      "alive: 4136590, pruned : 8487282, total: 12623872, Compression rate :       3.05x  ( 67.23% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb93f7281ec40b1815849d07728d0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/25], loss: 0.596 acc: 0.319, TEST loss: 0.630 test acc: 0.290\n",
      "Epoch [ 5/25], loss: 0.511 acc: 0.370, TEST loss: 0.548 test acc: 0.337\n",
      "Epoch [10/25], loss: 0.465 acc: 0.418, TEST loss: 0.525 test acc: 0.350\n",
      "Epoch [15/25], loss: 0.469 acc: 0.403, TEST loss: 0.512 test acc: 0.363\n",
      "Epoch [20/25], loss: 0.452 acc: 0.427, TEST loss: 0.503 test acc: 0.369\n",
      "Epoch [24/25], loss: 0.432 acc: 0.464, TEST loss: 0.497 test acc: 0.378\n"
     ]
    }
   ],
   "source": [
    "# Load from file just in case\n",
    "model_init = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(ncvx_save_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "prune_results_ncvx = ncvx_train(model, train_loader, test_loader, P, \n",
    "                                prune=True, re_init=False, init_state_dict=initial_state_dict, mask=mask)\n",
    "prune_results_ncvx.to_csv(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_Results.csv\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2c786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}.pth\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))\n",
    "\n",
    "with open(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_MASK.pkl\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690edc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a166359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729d3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1448c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938316cd",
   "metadata": {},
   "source": [
    "# Convex Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ebe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.weight_v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.weight_w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.weight_v - self.weight_w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.weight_v)\n",
    "    Xw = torch.matmul(_x, model.weight_w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.weight_v, dim=1)**2) + torch.sum(torch.norm(model.weight_w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.weight_v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.weight_w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct.item(), test_noncvx_cost.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba574a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-12\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y, _z) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device)\n",
    "        _y = Variable(_y).to(device)\n",
    "        _z = Variable(_z).to(device)\n",
    "        yhat = model(_x, _z).float()\n",
    "        \n",
    "        loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, P[\"beta\"], P[\"cvx_rho\"], device)/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        \n",
    "        noncvx_loss = get_nonconvex_cost(one_hot(_y).to(device), model, _x, P[\"beta\"], device)/len(_y)\n",
    "        d_out[\"noncvx_losses\"].append(noncvx_loss.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "\n",
    "def cvx_train(model, ds, ds_test, u_vectors, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"cvx_prune_epochs\"] if prune else P[\"cvx_num_epochs\"]\n",
    "    rounds = P[\"cvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"noncvx_losses\": [], \"losses_test\":[],\"accs_test\":[], \"noncvx_losses_test\":[], \n",
    "             \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"cvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "            \n",
    "            print(\"Pruning Round [{:>2}/{:}]\".format(p,num_epochs))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = cvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at,nlt = validation_cvxproblem(model, ds_test, u_vectors, P[\"beta\"], P[\"cvx_rho\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"noncvx_losses_test\"] += [nlt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            \n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}, TEST noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"noncvx_losses\"][-1],d_out[\"losses\"][-1],d_out[\"accs\"][-1],\n",
    "                       d_out[\"noncvx_losses_test\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "            scheduler.step(d_out[\"losses\"][-1])\n",
    "            \n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e84e",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edf3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n",
      "Parameter         : Value\n",
      "=========================\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "print_freq        : 5\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 100\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n",
      "cvx_test_len      : 10000\n",
      "cvx_prune_epochs  : 25\n",
      "cvx_prune_rounds  : 5\n",
      "cvx_prune_perc    : 20\n"
     ]
    }
   ],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f99e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1452a9befe541b6a942d2d7f0dada6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.408 loss: 0.366 acc: 0.477, TEST noncvx_loss: 0.411 loss: 0.366 acc: 0.467\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,cvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, prune=False)\n",
    "results_cvx.to_csv(cvx_save_loc+\"_EPOCHS{:}_Results\".format(P[\"cvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,cvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d9795",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912cd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "# Load from file just in case\n",
    "model_init = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(cvx_save_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(cvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "prune_results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, \n",
    "                              prune=True, re_init=False, init_state_dict=initial_state_dict_post, mask=mask)\n",
    "prune_results_cvx.to_csv(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_Results.csv\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}.pth\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))\n",
    "\n",
    "with open(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_MASK.pkl\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df02c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df405ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c44ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd04f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99558371",
   "metadata": {},
   "source": [
    "# Save Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_save_loc = 'models/nn{:}_solver{:}_lr_PARAMS.json'.format(P['num_neurons'],P['cvx_solver'])\n",
    "save_params(P,param_save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228eb9f",
   "metadata": {},
   "source": [
    "# Lottery Ticket \n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helperfunctions\n",
    "reload(helperfunctions)\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fa1c3",
   "metadata": {},
   "source": [
    "# TODO: \n",
    " * Better plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
