{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8930e19",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` with Lottery\n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88316c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c927fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helperfunctions\n",
    "reload(helperfunctions)\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822c9da",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5ef8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16b57e4c5f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P[\"print_freq\"] = 5\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 100\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "P[\"ncvx_prune_epochs\"] = 100 # 25\n",
    "P[\"ncvx_prune_rounds\"] = 5   # 5\n",
    "P[\"ncvx_prune_perc\"] = 80    # 20\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 100\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "P['cvx_test_len'] = 10000\n",
    "\n",
    "P[\"cvx_prune_epochs\"] = 100 # 25\n",
    "P[\"cvx_prune_rounds\"] = 5   # 5\n",
    "P[\"cvx_prune_perc\"] = 80    # 20\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f680d0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bca5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "P[\"cvx_n\"] = n\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386aca",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(path,P):\n",
    "    model = FCNetwork(P[\"num_neurons\"],P[\"num_classes\"],P[\"dim_in\"])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct.item()\n",
    "\n",
    "def ncvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-6\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "        _y = Variable(_y).to(device) # shape 1000\n",
    "        yhat = model(_x).float()\n",
    "        \n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, P[\"beta\"])/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "def ncvx_train(model, ds, ds_test, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"ncvx_prune_epochs\"] if prune else P[\"ncvx_num_epochs\"]\n",
    "    rounds = P[\"ncvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "    scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"losses_test\":[],\"accs_test\":[], \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"ncvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"ncvx_solver\"],P[\"ncvx_learning_rate\"],P[\"ncvx_LBFGS_param\"])\n",
    "            scheduler = get_scheduler(P[\"ncvx_schedule\"],optimizer,P[\"verbose\"])\n",
    "            \n",
    "            print(\"\\nPruning Round [{:>2}/{:}]\".format(p,rounds))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = ncvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at = validation_primal(model, ds_test, P[\"beta\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"ncvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], loss: {:.3f} acc: {:.3f}, TEST loss: {:.3f} test acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"losses\"][-1],d_out[\"accs\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "\n",
    "            if P[\"ncvx_schedule\"] > 0:\n",
    "                scheduler.step(d_out[\"losses\"][-1])\n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22a4de",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba2192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "==========================\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\Repos\\spring22\\convex_nn\n",
      "print_freq         : 5\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 100\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n",
      "ncvx_prune_epochs  : 100\n",
      "ncvx_prune_rounds  : 5\n",
      "ncvx_prune_perc    : 80\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=P[\"batch_size\"], shuffle=True, pin_memory=True, sampler=None)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05730f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac587fb30947969107cabe5ea75224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.785 acc: 0.220, TEST loss: 0.779 test acc: 0.237\n",
      "Epoch [ 5/100], loss: 0.496 acc: 0.398, TEST loss: 0.549 test acc: 0.346\n",
      "Epoch [10/100], loss: 0.441 acc: 0.460, TEST loss: 0.502 test acc: 0.376\n",
      "Epoch [15/100], loss: 0.412 acc: 0.483, TEST loss: 0.475 test acc: 0.405\n",
      "Epoch [20/100], loss: 0.369 acc: 0.571, TEST loss: 0.459 test acc: 0.414\n",
      "Epoch [25/100], loss: 0.355 acc: 0.563, TEST loss: 0.447 test acc: 0.424\n",
      "Epoch [30/100], loss: 0.340 acc: 0.633, TEST loss: 0.438 test acc: 0.430\n",
      "Epoch [35/100], loss: 0.321 acc: 0.644, TEST loss: 0.432 test acc: 0.439\n",
      "Epoch [40/100], loss: 0.317 acc: 0.657, TEST loss: 0.425 test acc: 0.446\n",
      "Epoch [45/100], loss: 0.307 acc: 0.685, TEST loss: 0.422 test acc: 0.451\n",
      "Epoch [50/100], loss: 0.290 acc: 0.694, TEST loss: 0.418 test acc: 0.455\n",
      "Epoch [55/100], loss: 0.287 acc: 0.720, TEST loss: 0.415 test acc: 0.459\n",
      "Epoch [60/100], loss: 0.284 acc: 0.725, TEST loss: 0.413 test acc: 0.461\n",
      "Epoch [65/100], loss: 0.280 acc: 0.732, TEST loss: 0.413 test acc: 0.463\n",
      "Epoch [70/100], loss: 0.278 acc: 0.714, TEST loss: 0.410 test acc: 0.467\n",
      "Epoch [75/100], loss: 0.266 acc: 0.756, TEST loss: 0.409 test acc: 0.470\n",
      "Epoch [80/100], loss: 0.262 acc: 0.784, TEST loss: 0.408 test acc: 0.471\n",
      "Epoch [85/100], loss: 0.255 acc: 0.784, TEST loss: 0.406 test acc: 0.470\n",
      "Epoch [90/100], loss: 0.255 acc: 0.789, TEST loss: 0.407 test acc: 0.476\n",
      "Epoch [95/100], loss: 0.247 acc: 0.806, TEST loss: 0.406 test acc: 0.477\n",
      "Epoch [99/100], loss: 0.242 acc: 0.822, TEST loss: 0.406 test acc: 0.477\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"models/ncvx_nn{:}_solver{:}_l1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "model.apply(weight_init)\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,ncvx_save_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_ncvx = ncvx_train(model, train_loader, test_loader, P, prune=False)\n",
    "results_ncvx.to_csv(ncvx_save_loc+\"_EPOCHS{:}_Results.csv\".format(P[\"ncvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,ncvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19af287",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c2d7e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruning Round [ 0/5]\n",
      "layer1.0.weight      | nonzeros = 2516583 / 12582912 ( 20.00%) | total_pruned = 10066329 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =    8192 /   40960 ( 20.00%) | total_pruned =   32768 | shape = (10, 4096)\n",
      "alive: 2524775, pruned : 10099097, total: 12623872, Compression rate :       5.00x  ( 80.00% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4050619105a4ce69b0e2025dfa81ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.471 acc: 0.371, TEST loss: 0.518 test acc: 0.292\n",
      "Epoch [ 5/100], loss: 0.421 acc: 0.429, TEST loss: 0.453 test acc: 0.365\n",
      "Epoch [10/100], loss: 0.396 acc: 0.448, TEST loss: 0.437 test acc: 0.388\n",
      "Epoch [15/100], loss: 0.386 acc: 0.511, TEST loss: 0.429 test acc: 0.399\n",
      "Epoch [20/100], loss: 0.378 acc: 0.516, TEST loss: 0.423 test acc: 0.406\n",
      "Epoch [25/100], loss: 0.378 acc: 0.505, TEST loss: 0.418 test acc: 0.414\n",
      "Epoch [30/100], loss: 0.374 acc: 0.527, TEST loss: 0.415 test acc: 0.421\n",
      "Epoch [35/100], loss: 0.362 acc: 0.536, TEST loss: 0.412 test acc: 0.426\n",
      "Epoch [40/100], loss: 0.367 acc: 0.530, TEST loss: 0.410 test acc: 0.432\n",
      "Epoch [45/100], loss: 0.361 acc: 0.524, TEST loss: 0.408 test acc: 0.435\n",
      "Epoch [50/100], loss: 0.350 acc: 0.558, TEST loss: 0.405 test acc: 0.439\n",
      "Epoch [55/100], loss: 0.353 acc: 0.546, TEST loss: 0.404 test acc: 0.440\n",
      "Epoch [60/100], loss: 0.353 acc: 0.548, TEST loss: 0.403 test acc: 0.442\n",
      "Epoch [65/100], loss: 0.352 acc: 0.558, TEST loss: 0.401 test acc: 0.445\n",
      "Epoch [70/100], loss: 0.345 acc: 0.563, TEST loss: 0.400 test acc: 0.447\n",
      "Epoch [75/100], loss: 0.340 acc: 0.573, TEST loss: 0.399 test acc: 0.447\n",
      "Epoch [80/100], loss: 0.344 acc: 0.550, TEST loss: 0.398 test acc: 0.449\n",
      "Epoch [85/100], loss: 0.337 acc: 0.596, TEST loss: 0.397 test acc: 0.451\n",
      "Epoch [90/100], loss: 0.342 acc: 0.560, TEST loss: 0.396 test acc: 0.452\n",
      "Epoch [95/100], loss: 0.334 acc: 0.585, TEST loss: 0.395 test acc: 0.454\n",
      "Epoch [99/100], loss: 0.335 acc: 0.602, TEST loss: 0.394 test acc: 0.455\n",
      "\n",
      "Pruning Round [ 1/5]\n",
      "layer1.0.weight      | nonzeros =  503317 / 12582912 (  4.00%) | total_pruned = 12079595 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =    1639 /   40960 (  4.00%) | total_pruned =   39321 | shape = (10, 4096)\n",
      "alive: 504956, pruned : 12118916, total: 12623872, Compression rate :      25.00x  ( 96.00% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcb0e39a27e418e9c648a5bfd2eb120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.443 acc: 0.240, TEST loss: 0.452 test acc: 0.226\n",
      "Epoch [ 5/100], loss: 0.415 acc: 0.328, TEST loss: 0.418 test acc: 0.325\n",
      "Epoch [10/100], loss: 0.404 acc: 0.359, TEST loss: 0.410 test acc: 0.347\n",
      "Epoch [15/100], loss: 0.401 acc: 0.368, TEST loss: 0.405 test acc: 0.361\n",
      "Epoch [20/100], loss: 0.400 acc: 0.382, TEST loss: 0.402 test acc: 0.370\n",
      "Epoch [25/100], loss: 0.392 acc: 0.385, TEST loss: 0.399 test acc: 0.380\n",
      "Epoch [30/100], loss: 0.396 acc: 0.374, TEST loss: 0.397 test acc: 0.386\n",
      "Epoch [35/100], loss: 0.392 acc: 0.406, TEST loss: 0.396 test acc: 0.389\n",
      "Epoch [40/100], loss: 0.392 acc: 0.387, TEST loss: 0.394 test acc: 0.393\n",
      "Epoch [45/100], loss: 0.394 acc: 0.388, TEST loss: 0.393 test acc: 0.398\n",
      "Epoch [50/100], loss: 0.389 acc: 0.397, TEST loss: 0.392 test acc: 0.399\n",
      "Epoch [55/100], loss: 0.391 acc: 0.399, TEST loss: 0.391 test acc: 0.401\n",
      "Epoch [60/100], loss: 0.391 acc: 0.402, TEST loss: 0.390 test acc: 0.403\n",
      "Epoch [65/100], loss: 0.380 acc: 0.439, TEST loss: 0.389 test acc: 0.406\n",
      "Epoch [70/100], loss: 0.388 acc: 0.415, TEST loss: 0.388 test acc: 0.407\n",
      "Epoch [75/100], loss: 0.382 acc: 0.425, TEST loss: 0.387 test acc: 0.408\n",
      "Epoch [80/100], loss: 0.385 acc: 0.421, TEST loss: 0.387 test acc: 0.409\n",
      "Epoch [85/100], loss: 0.377 acc: 0.442, TEST loss: 0.386 test acc: 0.411\n",
      "Epoch [90/100], loss: 0.383 acc: 0.432, TEST loss: 0.385 test acc: 0.412\n",
      "Epoch [95/100], loss: 0.383 acc: 0.407, TEST loss: 0.385 test acc: 0.413\n",
      "Epoch [99/100], loss: 0.377 acc: 0.432, TEST loss: 0.384 test acc: 0.414\n",
      "\n",
      "Pruning Round [ 2/5]\n",
      "layer1.0.weight      | nonzeros =  100664 / 12582912 (  0.80%) | total_pruned = 12482248 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =     328 /   40960 (  0.80%) | total_pruned =   40632 | shape = (10, 4096)\n",
      "alive: 100992, pruned : 12522880, total: 12623872, Compression rate :     125.00x  ( 99.20% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fef4e3d7de4907a919717026b8cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.445 acc: 0.284, TEST loss: 0.445 test acc: 0.282\n",
      "Epoch [ 5/100], loss: 0.426 acc: 0.313, TEST loss: 0.426 test acc: 0.312\n",
      "Epoch [10/100], loss: 0.419 acc: 0.312, TEST loss: 0.418 test acc: 0.324\n",
      "Epoch [15/100], loss: 0.416 acc: 0.334, TEST loss: 0.414 test acc: 0.335\n",
      "Epoch [20/100], loss: 0.406 acc: 0.370, TEST loss: 0.411 test acc: 0.343\n",
      "Epoch [25/100], loss: 0.407 acc: 0.338, TEST loss: 0.409 test acc: 0.345\n",
      "Epoch [30/100], loss: 0.409 acc: 0.339, TEST loss: 0.407 test acc: 0.348\n",
      "Epoch [35/100], loss: 0.402 acc: 0.367, TEST loss: 0.405 test acc: 0.351\n",
      "Epoch [40/100], loss: 0.407 acc: 0.335, TEST loss: 0.404 test acc: 0.354\n",
      "Epoch [45/100], loss: 0.407 acc: 0.341, TEST loss: 0.403 test acc: 0.356\n",
      "Epoch [50/100], loss: 0.397 acc: 0.366, TEST loss: 0.402 test acc: 0.357\n",
      "Epoch [55/100], loss: 0.398 acc: 0.375, TEST loss: 0.402 test acc: 0.358\n",
      "Epoch [60/100], loss: 0.402 acc: 0.343, TEST loss: 0.401 test acc: 0.360\n",
      "Epoch [65/100], loss: 0.406 acc: 0.364, TEST loss: 0.400 test acc: 0.361\n",
      "Epoch [70/100], loss: 0.401 acc: 0.360, TEST loss: 0.400 test acc: 0.362\n",
      "Epoch [75/100], loss: 0.393 acc: 0.390, TEST loss: 0.399 test acc: 0.363\n",
      "Epoch [80/100], loss: 0.394 acc: 0.380, TEST loss: 0.399 test acc: 0.365\n",
      "Epoch [85/100], loss: 0.398 acc: 0.350, TEST loss: 0.398 test acc: 0.367\n",
      "Epoch [90/100], loss: 0.396 acc: 0.365, TEST loss: 0.398 test acc: 0.368\n",
      "Epoch [95/100], loss: 0.392 acc: 0.383, TEST loss: 0.397 test acc: 0.368\n",
      "Epoch [99/100], loss: 0.397 acc: 0.364, TEST loss: 0.397 test acc: 0.369\n",
      "\n",
      "Pruning Round [ 3/5]\n",
      "layer1.0.weight      | nonzeros =   20133 / 12582912 (  0.16%) | total_pruned = 12562779 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =      66 /   40960 (  0.16%) | total_pruned =   40894 | shape = (10, 4096)\n",
      "alive: 20199, pruned : 12603673, total: 12623872, Compression rate :     624.98x  ( 99.84% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d9cc46b32347e4878fed8f8dade62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.483 acc: 0.252, TEST loss: 0.483 test acc: 0.226\n",
      "Epoch [ 5/100], loss: 0.473 acc: 0.263, TEST loss: 0.473 test acc: 0.240\n",
      "Epoch [10/100], loss: 0.461 acc: 0.256, TEST loss: 0.462 test acc: 0.230\n",
      "Epoch [15/100], loss: 0.453 acc: 0.243, TEST loss: 0.454 test acc: 0.228\n",
      "Epoch [20/100], loss: 0.444 acc: 0.261, TEST loss: 0.448 test acc: 0.234\n",
      "Epoch [25/100], loss: 0.442 acc: 0.245, TEST loss: 0.444 test acc: 0.241\n",
      "Epoch [30/100], loss: 0.437 acc: 0.281, TEST loss: 0.441 test acc: 0.249\n",
      "Epoch [35/100], loss: 0.435 acc: 0.271, TEST loss: 0.438 test acc: 0.255\n",
      "Epoch [40/100], loss: 0.436 acc: 0.269, TEST loss: 0.436 test acc: 0.263\n",
      "Epoch [45/100], loss: 0.433 acc: 0.280, TEST loss: 0.435 test acc: 0.269\n",
      "Epoch [50/100], loss: 0.433 acc: 0.274, TEST loss: 0.433 test acc: 0.274\n",
      "Epoch [55/100], loss: 0.429 acc: 0.275, TEST loss: 0.432 test acc: 0.277\n",
      "Epoch [60/100], loss: 0.430 acc: 0.264, TEST loss: 0.431 test acc: 0.279\n",
      "Epoch [65/100], loss: 0.429 acc: 0.308, TEST loss: 0.430 test acc: 0.284\n",
      "Epoch [70/100], loss: 0.432 acc: 0.286, TEST loss: 0.429 test acc: 0.287\n",
      "Epoch [75/100], loss: 0.427 acc: 0.295, TEST loss: 0.429 test acc: 0.287\n",
      "Epoch [80/100], loss: 0.426 acc: 0.301, TEST loss: 0.428 test acc: 0.289\n",
      "Epoch [85/100], loss: 0.430 acc: 0.275, TEST loss: 0.427 test acc: 0.290\n",
      "Epoch [90/100], loss: 0.429 acc: 0.274, TEST loss: 0.427 test acc: 0.290\n",
      "Epoch [95/100], loss: 0.426 acc: 0.320, TEST loss: 0.426 test acc: 0.291\n",
      "Epoch [99/100], loss: 0.425 acc: 0.290, TEST loss: 0.426 test acc: 0.292\n",
      "\n",
      "Pruning Round [ 4/5]\n",
      "layer1.0.weight      | nonzeros =    4027 / 12582912 (  0.03%) | total_pruned = 12578885 | shape = (4096, 3072)\n",
      "layer2.weight        | nonzeros =      14 /   40960 (  0.03%) | total_pruned =   40946 | shape = (10, 4096)\n",
      "alive: 4041, pruned : 12619831, total: 12623872, Compression rate :    3123.95x  ( 99.97% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ba2ac07cec44e5be38ea3a424186e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], loss: 0.497 acc: 0.191, TEST loss: 0.496 test acc: 0.202\n",
      "Epoch [ 5/100], loss: 0.493 acc: 0.211, TEST loss: 0.493 test acc: 0.204\n",
      "Epoch [10/100], loss: 0.489 acc: 0.183, TEST loss: 0.488 test acc: 0.202\n",
      "Epoch [15/100], loss: 0.483 acc: 0.206, TEST loss: 0.482 test acc: 0.197\n",
      "Epoch [20/100], loss: 0.474 acc: 0.204, TEST loss: 0.476 test acc: 0.194\n",
      "Epoch [25/100], loss: 0.473 acc: 0.195, TEST loss: 0.472 test acc: 0.199\n",
      "Epoch [30/100], loss: 0.470 acc: 0.199, TEST loss: 0.469 test acc: 0.202\n",
      "Epoch [35/100], loss: 0.470 acc: 0.199, TEST loss: 0.467 test acc: 0.206\n",
      "Epoch [40/100], loss: 0.464 acc: 0.213, TEST loss: 0.466 test acc: 0.210\n",
      "Epoch [45/100], loss: 0.466 acc: 0.201, TEST loss: 0.464 test acc: 0.214\n",
      "Epoch [50/100], loss: 0.461 acc: 0.232, TEST loss: 0.463 test acc: 0.217\n",
      "Epoch [55/100], loss: 0.463 acc: 0.217, TEST loss: 0.462 test acc: 0.218\n",
      "Epoch [60/100], loss: 0.463 acc: 0.217, TEST loss: 0.462 test acc: 0.221\n",
      "Epoch [65/100], loss: 0.462 acc: 0.239, TEST loss: 0.461 test acc: 0.223\n",
      "Epoch [70/100], loss: 0.461 acc: 0.225, TEST loss: 0.460 test acc: 0.224\n",
      "Epoch [75/100], loss: 0.461 acc: 0.208, TEST loss: 0.460 test acc: 0.224\n",
      "Epoch [80/100], loss: 0.457 acc: 0.233, TEST loss: 0.460 test acc: 0.226\n",
      "Epoch [85/100], loss: 0.461 acc: 0.218, TEST loss: 0.459 test acc: 0.226\n",
      "Epoch [90/100], loss: 0.455 acc: 0.238, TEST loss: 0.459 test acc: 0.226\n",
      "Epoch [95/100], loss: 0.456 acc: 0.237, TEST loss: 0.459 test acc: 0.227\n",
      "Epoch [99/100], loss: 0.461 acc: 0.212, TEST loss: 0.459 test acc: 0.228\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"models/ncvx_5at80_solver{:}_l1e-3\".format(P['cvx_solver'])\n",
    "ncvx_load_loc = \"models/ncvx_nn{:}_solver{:}_l1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "# Load from file just in case\n",
    "model_init = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(ncvx_load_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(ncvx_load_loc+\"_EPOCHS{:}.pth\".format(P[\"ncvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "# Use init_state_dict=initial_state_dict_post for iterative, initial_state_dict for lottery - iterative is more fair comp\n",
    "prune_results_ncvx = ncvx_train(model, train_loader, test_loader, P, \n",
    "                                prune=True, re_init=False, init_state_dict=initial_state_dict_post, mask=mask)\n",
    "# TODO - add pruning percentage to name here\n",
    "prune_results_ncvx.to_csv(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_POST_Results.csv\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2c786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_POST.pth\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]))\n",
    "\n",
    "with open(ncvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_POST_MASK.pkl\".format(P[\"ncvx_prune_rounds\"],P[\"ncvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e11a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ecd7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db299f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52024754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938316cd",
   "metadata": {},
   "source": [
    "# Convex Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ebe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.weight_v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.weight_w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.weight_v - self.weight_w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.weight_v)\n",
    "    Xw = torch.matmul(_x, model.weight_w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.weight_v, dim=1)**2) + torch.sum(torch.norm(model.weight_w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.weight_w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.weight_v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.weight_w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct.item(), test_noncvx_cost.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb58930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_train_step(model, ds, optimizer, P, d_out, freeze=True):\n",
    "    EPS = 1e-12\n",
    "    device = P[\"device\"]\n",
    "    for ix, (_x, _y, _z) in enumerate(ds):\n",
    "        optimizer.zero_grad()\n",
    "        # Make input differentiable\n",
    "        _x = Variable(_x).to(device)\n",
    "        _y = Variable(_y).to(device)\n",
    "        _z = Variable(_z).to(device)\n",
    "        yhat = model(_x, _z).float()\n",
    "        \n",
    "        loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, P[\"beta\"], P[\"cvx_rho\"], device)/len(_y)\n",
    "        correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Freezing Pruned weights by making their gradients Zero (if zero stay zero)\n",
    "        if freeze:\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tensor = p.data.cpu().numpy()\n",
    "                    grad_tensor = p.grad.data.cpu().numpy()\n",
    "                    grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                    p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "        d_out[\"losses\"].append(loss.item())\n",
    "        d_out[\"accs\"].append(correct.item())\n",
    "        \n",
    "        noncvx_loss = get_nonconvex_cost(one_hot(_y).to(device), model, _x, P[\"beta\"], device)/len(_y)\n",
    "        d_out[\"noncvx_losses\"].append(noncvx_loss.item())\n",
    "        d_out[\"times\"].append(time.time())\n",
    "    return ix\n",
    "\n",
    "\n",
    "def cvx_train(model, ds, ds_test, u_vectors, P, prune=True, re_init=False, init_state_dict=None, mask=None):\n",
    "    # Runs training loop\n",
    "    num_epochs = P[\"cvx_prune_epochs\"] if prune else P[\"cvx_num_epochs\"]\n",
    "    rounds = P[\"cvx_prune_rounds\"] if prune else 1\n",
    "\n",
    "    device = torch.device(P[\"device\"])\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "    \n",
    "    d_out = {\"losses\":[], \"accs\":[], \"noncvx_losses\": [], \"losses_test\":[],\"accs_test\":[], \"noncvx_losses_test\":[], \n",
    "             \"times\":[time.time()], \"epoch\": [], \"round\": []}\n",
    "    if prune:\n",
    "        d_out[\"nonzero_perc\"] = []\n",
    "\n",
    "    for p in range(rounds):\n",
    "        if prune:\n",
    "            prune_by_percentile(model,mask,P[\"cvx_prune_perc\"])\n",
    "            _ = re_init(model,mask) if re_init else og_init(model,mask,init_state_dict)\n",
    "            optimizer = get_optimizer(model,P[\"cvx_solver\"],P[\"cvx_learning_rate\"],P[\"cvx_LBFGS_param\"])\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=P[\"verbose\"], factor=0.5, eps=1e-12)\n",
    "            \n",
    "            print(\"Pruning Round [{:>2}/{:}]\".format(p,rounds))\n",
    "            nonzero_pc = print_nonzeros(model)\n",
    "            \n",
    "        iter_no = 0\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            train_iters = cvx_train_step(model, ds, optimizer, P, d_out, freeze=prune)\n",
    "\n",
    "            model.eval()\n",
    "            lt,at,nlt = validation_cvxproblem(model, ds_test, u_vectors, P[\"beta\"], P[\"cvx_rho\"], device)\n",
    "            d_out[\"losses_test\"] += [lt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"accs_test\"] += [at/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"noncvx_losses_test\"] += [nlt/P[\"cvx_test_len\"]]*(train_iters + 1)\n",
    "            d_out[\"epoch\"] += [i]*(train_iters + 1)\n",
    "            d_out[\"round\"] += [p]*(train_iters + 1)\n",
    "            \n",
    "            if prune:\n",
    "                d_out[\"nonzero_perc\"] += [nonzero_pc]*(train_iters + 1)\n",
    "            iter_no += train_iters + 1\n",
    "\n",
    "            if i % P[\"print_freq\"] == 0 or i == num_epochs - 1:\n",
    "                print(\"Epoch [{:>2}/{:}], noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}, TEST noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}\".format(\n",
    "                       i,num_epochs,d_out[\"noncvx_losses\"][-1],d_out[\"losses\"][-1],d_out[\"accs\"][-1],\n",
    "                       d_out[\"noncvx_losses_test\"][-1],d_out[\"losses_test\"][-1],d_out[\"accs_test\"][-1]))\n",
    "            scheduler.step(d_out[\"losses\"][-1])\n",
    "            \n",
    "    d_out[\"times\"] = np.diff(d_out[\"times\"])\n",
    "    return pd.DataFrame.from_dict(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e84e",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5edf3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n",
      "Parameter         : Value\n",
      "=========================\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\Repos\\spring22\\convex_nn\n",
      "print_freq        : 5\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 100\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n",
      "cvx_test_len      : 10000\n",
      "cvx_prune_epochs  : 100\n",
      "cvx_prune_rounds  : 5\n",
      "cvx_prune_perc    : 80\n",
      "cvx_n             : 50000\n"
     ]
    }
   ],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=P[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3f99e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1452a9befe541b6a942d2d7f0dada6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.408 loss: 0.366 acc: 0.477, TEST noncvx_loss: 0.411 loss: 0.366 acc: 0.467\n",
      "Epoch [ 5/100], noncvx_loss: 0.431 loss: 0.296 acc: 0.636, TEST noncvx_loss: 0.436 loss: 0.342 acc: 0.519\n",
      "Epoch [10/100], noncvx_loss: 0.436 loss: 0.258 acc: 0.765, TEST noncvx_loss: 0.443 loss: 0.338 acc: 0.529\n",
      "Epoch [15/100], noncvx_loss: 0.435 loss: 0.232 acc: 0.835, TEST noncvx_loss: 0.445 loss: 0.335 acc: 0.539\n",
      "Epoch [20/100], noncvx_loss: 0.438 loss: 0.215 acc: 0.867, TEST noncvx_loss: 0.450 loss: 0.338 acc: 0.544\n",
      "Epoch [25/100], noncvx_loss: 0.440 loss: 0.200 acc: 0.886, TEST noncvx_loss: 0.451 loss: 0.337 acc: 0.543\n",
      "Epoch [30/100], noncvx_loss: 0.439 loss: 0.184 acc: 0.934, TEST noncvx_loss: 0.452 loss: 0.339 acc: 0.541\n",
      "Epoch [35/100], noncvx_loss: 0.437 loss: 0.169 acc: 0.938, TEST noncvx_loss: 0.452 loss: 0.340 acc: 0.541\n",
      "Epoch [40/100], noncvx_loss: 0.440 loss: 0.159 acc: 0.952, TEST noncvx_loss: 0.452 loss: 0.341 acc: 0.541\n",
      "Epoch [45/100], noncvx_loss: 0.443 loss: 0.154 acc: 0.970, TEST noncvx_loss: 0.452 loss: 0.342 acc: 0.539\n",
      "Epoch [50/100], noncvx_loss: 0.443 loss: 0.143 acc: 0.967, TEST noncvx_loss: 0.454 loss: 0.344 acc: 0.541\n",
      "Epoch [55/100], noncvx_loss: 0.436 loss: 0.133 acc: 0.974, TEST noncvx_loss: 0.453 loss: 0.344 acc: 0.541\n",
      "Epoch [60/100], noncvx_loss: 0.438 loss: 0.128 acc: 0.976, TEST noncvx_loss: 0.454 loss: 0.347 acc: 0.540\n",
      "Epoch [65/100], noncvx_loss: 0.441 loss: 0.116 acc: 0.986, TEST noncvx_loss: 0.455 loss: 0.348 acc: 0.540\n",
      "Epoch [70/100], noncvx_loss: 0.433 loss: 0.114 acc: 0.984, TEST noncvx_loss: 0.454 loss: 0.349 acc: 0.536\n",
      "Epoch [75/100], noncvx_loss: 0.440 loss: 0.107 acc: 0.991, TEST noncvx_loss: 0.454 loss: 0.351 acc: 0.539\n",
      "Epoch [80/100], noncvx_loss: 0.443 loss: 0.101 acc: 0.991, TEST noncvx_loss: 0.455 loss: 0.353 acc: 0.534\n",
      "Epoch [85/100], noncvx_loss: 0.435 loss: 0.095 acc: 0.990, TEST noncvx_loss: 0.454 loss: 0.353 acc: 0.536\n",
      "Epoch [90/100], noncvx_loss: 0.443 loss: 0.091 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.355 acc: 0.539\n",
      "Epoch [95/100], noncvx_loss: 0.439 loss: 0.089 acc: 0.998, TEST noncvx_loss: 0.456 loss: 0.356 acc: 0.537\n",
      "Epoch [99/100], noncvx_loss: 0.436 loss: 0.085 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.357 acc: 0.533\n"
     ]
    }
   ],
   "source": [
    "cvx_load_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "cvx_save_loc = \"models/cvx_5at80_solver{:}_lr5e-7\".format(P['cvx_solver'])\n",
    "\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict,cvx_load_loc+\"_INITIAL.pth\")\n",
    "\n",
    "# Initial training,\n",
    "results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, prune=False)\n",
    "results_cvx.to_csv(cvx_load_loc+\"_EPOCHS{:}_Results.csv\".format(P[\"cvx_num_epochs\"]))\n",
    "\n",
    "# Save model after 100 epochs\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_post,cvx_save_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34657c66",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1293d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Round [ 0/5]\n",
      "weight_v             | nonzeros = 25165824 / 125829120 ( 20.00%) | total_pruned = 100663296 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 25165829 / 125829120 ( 20.00%) | total_pruned = 100663291 | shape = (4096, 3072, 10)\n",
      "alive: 50331653, pruned : 201326587, total: 251658240, Compression rate :       5.00x  ( -5.33% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trevo\\Documents\\Repos\\spring22\\convex_nn\\helperfunctions.py:96: RuntimeWarning: overflow encountered in long_scalars\n",
      "  print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b743ba44ee641b79cc7b1bcbe413cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.441 loss: 0.162 acc: 0.992, TEST noncvx_loss: 0.452 loss: 0.357 acc: 0.537\n",
      "Epoch [ 5/100], noncvx_loss: 0.438 loss: 0.146 acc: 0.984, TEST noncvx_loss: 0.449 loss: 0.345 acc: 0.545\n",
      "Epoch [10/100], noncvx_loss: 0.434 loss: 0.143 acc: 0.985, TEST noncvx_loss: 0.447 loss: 0.343 acc: 0.547\n",
      "Epoch [15/100], noncvx_loss: 0.438 loss: 0.146 acc: 0.987, TEST noncvx_loss: 0.446 loss: 0.342 acc: 0.545\n",
      "Epoch [20/100], noncvx_loss: 0.433 loss: 0.142 acc: 0.987, TEST noncvx_loss: 0.445 loss: 0.341 acc: 0.545\n",
      "Epoch [25/100], noncvx_loss: 0.435 loss: 0.143 acc: 0.987, TEST noncvx_loss: 0.445 loss: 0.341 acc: 0.546\n",
      "Epoch [30/100], noncvx_loss: 0.434 loss: 0.141 acc: 0.986, TEST noncvx_loss: 0.443 loss: 0.340 acc: 0.546\n",
      "Epoch [35/100], noncvx_loss: 0.433 loss: 0.138 acc: 0.984, TEST noncvx_loss: 0.443 loss: 0.340 acc: 0.547\n",
      "Epoch [40/100], noncvx_loss: 0.430 loss: 0.138 acc: 0.982, TEST noncvx_loss: 0.443 loss: 0.340 acc: 0.548\n",
      "Epoch [45/100], noncvx_loss: 0.430 loss: 0.140 acc: 0.986, TEST noncvx_loss: 0.442 loss: 0.340 acc: 0.548\n",
      "Epoch [50/100], noncvx_loss: 0.427 loss: 0.140 acc: 0.986, TEST noncvx_loss: 0.441 loss: 0.340 acc: 0.547\n",
      "Epoch    53: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [55/100], noncvx_loss: 0.424 loss: 0.136 acc: 0.985, TEST noncvx_loss: 0.441 loss: 0.340 acc: 0.547\n",
      "Epoch [60/100], noncvx_loss: 0.424 loss: 0.134 acc: 0.992, TEST noncvx_loss: 0.441 loss: 0.340 acc: 0.547\n",
      "Epoch [65/100], noncvx_loss: 0.432 loss: 0.138 acc: 0.992, TEST noncvx_loss: 0.441 loss: 0.340 acc: 0.547\n",
      "Epoch [70/100], noncvx_loss: 0.428 loss: 0.141 acc: 0.983, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.548\n",
      "Epoch [75/100], noncvx_loss: 0.429 loss: 0.138 acc: 0.984, TEST noncvx_loss: 0.441 loss: 0.340 acc: 0.547\n",
      "Epoch    76: reducing learning rate of group 0 to 1.2500e-07.\n",
      "Epoch [80/100], noncvx_loss: 0.425 loss: 0.137 acc: 0.988, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.548\n",
      "Epoch [85/100], noncvx_loss: 0.425 loss: 0.133 acc: 0.984, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.547\n",
      "Epoch    87: reducing learning rate of group 0 to 6.2500e-08.\n",
      "Epoch [90/100], noncvx_loss: 0.432 loss: 0.136 acc: 0.983, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.548\n",
      "Epoch [95/100], noncvx_loss: 0.422 loss: 0.132 acc: 0.990, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.547\n",
      "Epoch    98: reducing learning rate of group 0 to 3.1250e-08.\n",
      "Epoch [99/100], noncvx_loss: 0.425 loss: 0.134 acc: 0.986, TEST noncvx_loss: 0.440 loss: 0.340 acc: 0.548\n",
      "Pruning Round [ 1/5]\n",
      "weight_v             | nonzeros = 5033167 / 125829120 (  4.00%) | total_pruned = 120795953 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 5033167 / 125829120 (  4.00%) | total_pruned = 120795953 | shape = (4096, 3072, 10)\n",
      "alive: 10066334, pruned : 241591906, total: 251658240, Compression rate :      25.00x  ( -6.40% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80abae19374d49a89b5b1d52ed8d7955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.445 loss: 0.328 acc: 0.864, TEST noncvx_loss: 0.446 loss: 0.400 acc: 0.517\n",
      "Epoch [ 5/100], noncvx_loss: 0.433 loss: 0.301 acc: 0.826, TEST noncvx_loss: 0.439 loss: 0.378 acc: 0.518\n",
      "Epoch [10/100], noncvx_loss: 0.429 loss: 0.297 acc: 0.801, TEST noncvx_loss: 0.435 loss: 0.368 acc: 0.518\n",
      "Epoch [15/100], noncvx_loss: 0.426 loss: 0.291 acc: 0.798, TEST noncvx_loss: 0.432 loss: 0.364 acc: 0.520\n",
      "Epoch [20/100], noncvx_loss: 0.423 loss: 0.290 acc: 0.774, TEST noncvx_loss: 0.430 loss: 0.361 acc: 0.521\n",
      "Epoch [25/100], noncvx_loss: 0.422 loss: 0.289 acc: 0.787, TEST noncvx_loss: 0.429 loss: 0.360 acc: 0.521\n",
      "Epoch [30/100], noncvx_loss: 0.420 loss: 0.289 acc: 0.791, TEST noncvx_loss: 0.428 loss: 0.359 acc: 0.523\n",
      "Epoch [35/100], noncvx_loss: 0.421 loss: 0.282 acc: 0.802, TEST noncvx_loss: 0.427 loss: 0.358 acc: 0.524\n",
      "Epoch [40/100], noncvx_loss: 0.418 loss: 0.284 acc: 0.793, TEST noncvx_loss: 0.426 loss: 0.357 acc: 0.525\n",
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [45/100], noncvx_loss: 0.418 loss: 0.282 acc: 0.796, TEST noncvx_loss: 0.426 loss: 0.357 acc: 0.526\n",
      "Epoch [50/100], noncvx_loss: 0.420 loss: 0.285 acc: 0.786, TEST noncvx_loss: 0.426 loss: 0.357 acc: 0.526\n",
      "Epoch [55/100], noncvx_loss: 0.419 loss: 0.283 acc: 0.799, TEST noncvx_loss: 0.425 loss: 0.357 acc: 0.526\n",
      "Epoch    58: reducing learning rate of group 0 to 1.2500e-07.\n",
      "Epoch [60/100], noncvx_loss: 0.416 loss: 0.281 acc: 0.812, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch [65/100], noncvx_loss: 0.418 loss: 0.284 acc: 0.779, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch    69: reducing learning rate of group 0 to 6.2500e-08.\n",
      "Epoch [70/100], noncvx_loss: 0.419 loss: 0.284 acc: 0.789, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch [75/100], noncvx_loss: 0.415 loss: 0.279 acc: 0.810, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch    80: reducing learning rate of group 0 to 3.1250e-08.\n",
      "Epoch [80/100], noncvx_loss: 0.419 loss: 0.281 acc: 0.793, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch [85/100], noncvx_loss: 0.411 loss: 0.274 acc: 0.808, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch [90/100], noncvx_loss: 0.423 loss: 0.288 acc: 0.778, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch [95/100], noncvx_loss: 0.418 loss: 0.283 acc: 0.793, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Epoch    97: reducing learning rate of group 0 to 1.5625e-08.\n",
      "Epoch [99/100], noncvx_loss: 0.417 loss: 0.280 acc: 0.792, TEST noncvx_loss: 0.425 loss: 0.356 acc: 0.527\n",
      "Pruning Round [ 2/5]\n",
      "weight_v             | nonzeros = 1006634 / 125829120 (  0.80%) | total_pruned = 124822486 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros = 1006634 / 125829120 (  0.80%) | total_pruned = 124822486 | shape = (4096, 3072, 10)\n",
      "alive: 2013268, pruned : 249644972, total: 251658240, Compression rate :     125.00x  ( -3.20% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c116f5bc3b7041a79dbedcbe15e105db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.449 loss: 0.386 acc: 0.614, TEST noncvx_loss: 0.450 loss: 0.405 acc: 0.463\n",
      "Epoch [ 5/100], noncvx_loss: 0.451 loss: 0.368 acc: 0.568, TEST noncvx_loss: 0.451 loss: 0.387 acc: 0.467\n",
      "Epoch [10/100], noncvx_loss: 0.449 loss: 0.359 acc: 0.575, TEST noncvx_loss: 0.450 loss: 0.383 acc: 0.476\n",
      "Epoch [15/100], noncvx_loss: 0.447 loss: 0.351 acc: 0.617, TEST noncvx_loss: 0.449 loss: 0.381 acc: 0.480\n",
      "Epoch [20/100], noncvx_loss: 0.442 loss: 0.350 acc: 0.606, TEST noncvx_loss: 0.449 loss: 0.380 acc: 0.484\n",
      "Epoch [25/100], noncvx_loss: 0.443 loss: 0.349 acc: 0.579, TEST noncvx_loss: 0.448 loss: 0.379 acc: 0.488\n",
      "Epoch [30/100], noncvx_loss: 0.446 loss: 0.348 acc: 0.602, TEST noncvx_loss: 0.447 loss: 0.378 acc: 0.489\n",
      "Epoch [35/100], noncvx_loss: 0.440 loss: 0.339 acc: 0.634, TEST noncvx_loss: 0.447 loss: 0.378 acc: 0.492\n",
      "Epoch [40/100], noncvx_loss: 0.438 loss: 0.343 acc: 0.605, TEST noncvx_loss: 0.446 loss: 0.378 acc: 0.494\n",
      "Epoch [45/100], noncvx_loss: 0.440 loss: 0.345 acc: 0.612, TEST noncvx_loss: 0.445 loss: 0.377 acc: 0.495\n",
      "Epoch    47: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [50/100], noncvx_loss: 0.439 loss: 0.342 acc: 0.625, TEST noncvx_loss: 0.445 loss: 0.377 acc: 0.496\n",
      "Epoch [55/100], noncvx_loss: 0.441 loss: 0.343 acc: 0.620, TEST noncvx_loss: 0.445 loss: 0.377 acc: 0.496\n",
      "Epoch [60/100], noncvx_loss: 0.436 loss: 0.333 acc: 0.652, TEST noncvx_loss: 0.445 loss: 0.377 acc: 0.497\n",
      "Epoch [65/100], noncvx_loss: 0.440 loss: 0.339 acc: 0.624, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.497\n",
      "Epoch [70/100], noncvx_loss: 0.444 loss: 0.347 acc: 0.586, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch    72: reducing learning rate of group 0 to 1.2500e-07.\n",
      "Epoch [75/100], noncvx_loss: 0.438 loss: 0.335 acc: 0.640, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch [80/100], noncvx_loss: 0.434 loss: 0.335 acc: 0.633, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch    83: reducing learning rate of group 0 to 6.2500e-08.\n",
      "Epoch [85/100], noncvx_loss: 0.440 loss: 0.342 acc: 0.636, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch [90/100], noncvx_loss: 0.439 loss: 0.340 acc: 0.633, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch    94: reducing learning rate of group 0 to 3.1250e-08.\n",
      "Epoch [95/100], noncvx_loss: 0.442 loss: 0.341 acc: 0.630, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Epoch [99/100], noncvx_loss: 0.437 loss: 0.342 acc: 0.625, TEST noncvx_loss: 0.444 loss: 0.377 acc: 0.498\n",
      "Pruning Round [ 3/5]\n",
      "weight_v             | nonzeros =  201327 / 125829120 (  0.16%) | total_pruned = 125627793 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros =  201327 / 125829120 (  0.16%) | total_pruned = 125627793 | shape = (4096, 3072, 10)\n",
      "alive: 402654, pruned : 251255586, total: 251658240, Compression rate :     625.00x  ( -2.56% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c61fcc8f3548738f324af3b49f448d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.467 loss: 0.442 acc: 0.458, TEST noncvx_loss: 0.465 loss: 0.445 acc: 0.417\n",
      "Epoch [ 5/100], noncvx_loss: 0.453 loss: 0.408 acc: 0.473, TEST noncvx_loss: 0.453 loss: 0.413 acc: 0.414\n",
      "Epoch [10/100], noncvx_loss: 0.453 loss: 0.393 acc: 0.477, TEST noncvx_loss: 0.453 loss: 0.402 acc: 0.426\n",
      "Epoch [15/100], noncvx_loss: 0.447 loss: 0.380 acc: 0.498, TEST noncvx_loss: 0.454 loss: 0.396 acc: 0.436\n",
      "Epoch [20/100], noncvx_loss: 0.453 loss: 0.386 acc: 0.468, TEST noncvx_loss: 0.454 loss: 0.392 acc: 0.444\n",
      "Epoch [25/100], noncvx_loss: 0.452 loss: 0.372 acc: 0.522, TEST noncvx_loss: 0.454 loss: 0.389 acc: 0.449\n",
      "Epoch [30/100], noncvx_loss: 0.457 loss: 0.376 acc: 0.509, TEST noncvx_loss: 0.455 loss: 0.388 acc: 0.451\n",
      "Epoch [35/100], noncvx_loss: 0.450 loss: 0.373 acc: 0.480, TEST noncvx_loss: 0.454 loss: 0.386 acc: 0.457\n",
      "Epoch [40/100], noncvx_loss: 0.454 loss: 0.374 acc: 0.498, TEST noncvx_loss: 0.454 loss: 0.385 acc: 0.459\n",
      "Epoch [45/100], noncvx_loss: 0.455 loss: 0.371 acc: 0.524, TEST noncvx_loss: 0.454 loss: 0.385 acc: 0.462\n",
      "Epoch [50/100], noncvx_loss: 0.448 loss: 0.367 acc: 0.532, TEST noncvx_loss: 0.453 loss: 0.384 acc: 0.463\n",
      "Epoch [55/100], noncvx_loss: 0.454 loss: 0.372 acc: 0.525, TEST noncvx_loss: 0.453 loss: 0.383 acc: 0.464\n",
      "Epoch    58: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [60/100], noncvx_loss: 0.448 loss: 0.364 acc: 0.540, TEST noncvx_loss: 0.452 loss: 0.383 acc: 0.464\n",
      "Epoch [65/100], noncvx_loss: 0.443 loss: 0.361 acc: 0.531, TEST noncvx_loss: 0.452 loss: 0.383 acc: 0.465\n",
      "Epoch [70/100], noncvx_loss: 0.451 loss: 0.365 acc: 0.517, TEST noncvx_loss: 0.452 loss: 0.383 acc: 0.466\n",
      "Epoch [75/100], noncvx_loss: 0.457 loss: 0.371 acc: 0.496, TEST noncvx_loss: 0.452 loss: 0.382 acc: 0.467\n",
      "Epoch [80/100], noncvx_loss: 0.447 loss: 0.364 acc: 0.552, TEST noncvx_loss: 0.451 loss: 0.382 acc: 0.467\n",
      "Epoch [85/100], noncvx_loss: 0.454 loss: 0.368 acc: 0.515, TEST noncvx_loss: 0.451 loss: 0.382 acc: 0.469\n",
      "Epoch    88: reducing learning rate of group 0 to 1.2500e-07.\n",
      "Epoch [90/100], noncvx_loss: 0.455 loss: 0.367 acc: 0.522, TEST noncvx_loss: 0.451 loss: 0.382 acc: 0.469\n",
      "Epoch [95/100], noncvx_loss: 0.450 loss: 0.367 acc: 0.540, TEST noncvx_loss: 0.451 loss: 0.382 acc: 0.469\n",
      "Epoch    99: reducing learning rate of group 0 to 6.2500e-08.\n",
      "Epoch [99/100], noncvx_loss: 0.454 loss: 0.372 acc: 0.524, TEST noncvx_loss: 0.451 loss: 0.382 acc: 0.469\n",
      "Pruning Round [ 4/5]\n",
      "weight_v             | nonzeros =   40266 / 125829120 (  0.03%) | total_pruned = 125788854 | shape = (4096, 3072, 10)\n",
      "weight_w             | nonzeros =   40266 / 125829120 (  0.03%) | total_pruned = 125788854 | shape = (4096, 3072, 10)\n",
      "alive: 80532, pruned : 251577708, total: 251658240, Compression rate :    3124.95x  ( -2.43% pruned)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90aacc1a17548baabd0f8b9b2255564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.488 loss: 0.478 acc: 0.408, TEST noncvx_loss: 0.488 loss: 0.479 acc: 0.377\n",
      "Epoch [ 5/100], noncvx_loss: 0.467 loss: 0.448 acc: 0.373, TEST noncvx_loss: 0.466 loss: 0.449 acc: 0.355\n",
      "Epoch [10/100], noncvx_loss: 0.457 loss: 0.429 acc: 0.379, TEST noncvx_loss: 0.458 loss: 0.434 acc: 0.361\n",
      "Epoch [15/100], noncvx_loss: 0.455 loss: 0.419 acc: 0.398, TEST noncvx_loss: 0.455 loss: 0.425 acc: 0.369\n",
      "Epoch [20/100], noncvx_loss: 0.455 loss: 0.416 acc: 0.392, TEST noncvx_loss: 0.454 loss: 0.419 acc: 0.377\n",
      "Epoch [25/100], noncvx_loss: 0.455 loss: 0.414 acc: 0.404, TEST noncvx_loss: 0.453 loss: 0.415 acc: 0.385\n",
      "Epoch [30/100], noncvx_loss: 0.456 loss: 0.411 acc: 0.395, TEST noncvx_loss: 0.453 loss: 0.411 acc: 0.389\n",
      "Epoch [35/100], noncvx_loss: 0.451 loss: 0.403 acc: 0.399, TEST noncvx_loss: 0.453 loss: 0.409 acc: 0.393\n",
      "Epoch [40/100], noncvx_loss: 0.454 loss: 0.399 acc: 0.430, TEST noncvx_loss: 0.453 loss: 0.407 acc: 0.396\n",
      "Epoch [45/100], noncvx_loss: 0.454 loss: 0.398 acc: 0.430, TEST noncvx_loss: 0.453 loss: 0.405 acc: 0.400\n",
      "Epoch [50/100], noncvx_loss: 0.454 loss: 0.400 acc: 0.397, TEST noncvx_loss: 0.454 loss: 0.403 acc: 0.403\n",
      "Epoch [55/100], noncvx_loss: 0.450 loss: 0.387 acc: 0.463, TEST noncvx_loss: 0.454 loss: 0.402 acc: 0.406\n",
      "Epoch [60/100], noncvx_loss: 0.454 loss: 0.393 acc: 0.442, TEST noncvx_loss: 0.454 loss: 0.401 acc: 0.408\n",
      "Epoch [65/100], noncvx_loss: 0.457 loss: 0.395 acc: 0.425, TEST noncvx_loss: 0.454 loss: 0.400 acc: 0.409\n",
      "Epoch    67: reducing learning rate of group 0 to 2.5000e-07.\n",
      "Epoch [70/100], noncvx_loss: 0.458 loss: 0.399 acc: 0.407, TEST noncvx_loss: 0.455 loss: 0.399 acc: 0.410\n",
      "Epoch [75/100], noncvx_loss: 0.449 loss: 0.384 acc: 0.474, TEST noncvx_loss: 0.455 loss: 0.399 acc: 0.411\n",
      "Epoch [80/100], noncvx_loss: 0.450 loss: 0.387 acc: 0.460, TEST noncvx_loss: 0.455 loss: 0.399 acc: 0.412\n",
      "Epoch [85/100], noncvx_loss: 0.460 loss: 0.395 acc: 0.419, TEST noncvx_loss: 0.455 loss: 0.398 acc: 0.413\n",
      "Epoch    87: reducing learning rate of group 0 to 1.2500e-07.\n",
      "Epoch [90/100], noncvx_loss: 0.455 loss: 0.391 acc: 0.457, TEST noncvx_loss: 0.455 loss: 0.398 acc: 0.414\n",
      "Epoch [95/100], noncvx_loss: 0.454 loss: 0.390 acc: 0.446, TEST noncvx_loss: 0.455 loss: 0.398 acc: 0.414\n",
      "Epoch    98: reducing learning rate of group 0 to 6.2500e-08.\n",
      "Epoch [99/100], noncvx_loss: 0.454 loss: 0.388 acc: 0.449, TEST noncvx_loss: 0.455 loss: 0.398 acc: 0.415\n"
     ]
    }
   ],
   "source": [
    "cvx_load_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "cvx_save_loc = \"models/cvx_5at80_solver{:}_lr5e-7\".format(P['cvx_solver'])\n",
    "\n",
    "# Load from file just in case\n",
    "model_init = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model_init.load_state_dict(torch.load(cvx_load_loc+\"_INITIAL.pth\"))\n",
    "initial_state_dict = copy.deepcopy(model_init.state_dict())\n",
    "\n",
    "model = custom_cvx_layer(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "model.load_state_dict(torch.load(cvx_load_loc+\"_EPOCHS{:}.pth\".format(P[\"cvx_num_epochs\"])))\n",
    "initial_state_dict_post = copy.deepcopy(model.state_dict())\n",
    "                           \n",
    "mask = make_mask(model)\n",
    "prune_results_cvx = cvx_train(model, ds_train, test_loader, u_vectors, P, \n",
    "                              prune=True, re_init=False, init_state_dict=initial_state_dict_post, mask=mask)\n",
    "prune_results_cvx.to_csv(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_Results.csv\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01311385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model/mask after Lottery epochs\n",
    "initial_state_dict_lotto = copy.deepcopy(model.state_dict())\n",
    "torch.save(initial_state_dict_lotto,cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}.pth\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]))\n",
    "\n",
    "with open(cvx_save_loc+\"_ROUNDS{:}_EPOCHS{:}_MASK.pkl\".format(P[\"cvx_prune_rounds\"],P[\"cvx_prune_epochs\"]),'wb') as f:\n",
    "    pickle.dump(mask,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86b11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99558371",
   "metadata": {},
   "source": [
    "# Save Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bc3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_save_loc = 'models/5at80_100EPOCHS_solver{:}.json'.format(P['num_neurons'],P['cvx_solver'])\n",
    "save_params(P,param_save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aad66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
