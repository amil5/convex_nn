{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1844ce06",
   "metadata": {},
   "source": [
    "# From `convexnn_pytorch_stepsize_fig.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeef79aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdill\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "import time\n",
    "import scipy\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import torch\n",
    "import sklearn.linear_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda55e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFCNetwork\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, H, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3072\u001b[39m):\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def parse_args(inp=[\"--GD\", \"0\", \"--CVX\", \"0\", \"--n_epochs\",\"100\", \"100\", \"--solver_cvx\", \"sgd\"]):\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--GD', nargs=1, type=int, required=True)\n",
    "    parser.add_argument('--CVX', nargs=1, type=int, required=True)\n",
    "    parser.add_argument('--n_epochs', nargs=2, type=int, required=True)\n",
    "    parser.add_argument('--solver_cvx', type=str, nargs=1, default=\"adam\")\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    args = parser.parse_args(inp)\n",
    "    random.seed(a=args.seed)\n",
    "    np.random.seed(seed=args.seed)\n",
    "    torch.manual_seed(seed=args.seed)\n",
    "    return args\n",
    "\n",
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, H, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, H, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(H, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "# functions for generating sign patterns\n",
    "def check_if_already_exists(element_list, element):\n",
    "    # check if element exists in element_list\n",
    "    # where element is a numpy array\n",
    "    for i in range(len(element_list)):\n",
    "        if np.array_equal(element_list[i], element):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "            \n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class PrepareData3D(Dataset):\n",
    "    def __init__(self, X, y, z):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "            \n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "        \n",
    "        if not torch.is_tensor(z):\n",
    "            self.z = torch.from_numpy(z)\n",
    "        else:\n",
    "            self.z = z\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.z[idx]\n",
    "\n",
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "def one_hot(labels, num_classes=10):\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels.long()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c685c2fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ARGS\u001b[38;5;241m=\u001b[39m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mparse_args\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(inp\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--GD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--CVX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--n_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--solver_cvx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Parse arguments\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser()\n\u001b[0;32m      4\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--GD\u001b[39m\u001b[38;5;124m'\u001b[39m, nargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--CVX\u001b[39m\u001b[38;5;124m'\u001b[39m, nargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "ARGS=parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9d82d",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4421bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    \n",
    "    ## l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "\n",
    "        output = model.forward(_x)\n",
    "        yhat = model(_x).float()\n",
    "\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "\n",
    "    return test_loss, test_correct\n",
    "\n",
    "# solves nonconvex problem\n",
    "def sgd_solver_pytorch_v2(ds, ds_test, num_epochs, num_neurons, beta, \n",
    "                         learning_rate, batch_size, solver_type, schedule, \n",
    "                          LBFGS_param, verbose=False, \n",
    "                        num_classes=10, D_in=3*1024, test_len=10000, \n",
    "                          train_len=50000, device='cuda'):\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    # D_in is input dimension, H is hidden dimension, D_out is output dimension.\n",
    "    H, D_out = num_neurons, num_classes\n",
    "    # create the model\n",
    "    model = FCNetwork(H, D_out, D_in).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])#,\n",
    "        \n",
    "    # arrays for saving the loss and accuracy    \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(train_len / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    \n",
    "    losses_test[0], accs_test[0] = validation_primal(model, ds_test, beta, device) # loss on the entire test set\n",
    "    \n",
    "    if schedule==1:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           verbose=verbose,\n",
    "                                                           factor=0.5,\n",
    "                                                           eps=1e-12)\n",
    "    elif schedule==2:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
    "        \n",
    "    iter_no = 0\n",
    "    for i in range(num_epochs):\n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            \n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x).float()\n",
    "            \n",
    "            loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "            \n",
    "           \n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1] = validation_primal(model, ds_test, beta, device) # loss on the entire test set\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{}/{}], loss: {} acc: {}, test loss: {} test acc: {}\".format(i, num_epochs,\n",
    "                    np.round(losses[iter_no-1], 3), np.round(accs[iter_no-1], 3), \n",
    "                    np.round(losses_test[i+1], 3)/test_len, np.round(accs_test[i+1]/test_len, 3)))\n",
    "        if schedule>0:\n",
    "            scheduler.step(losses[iter_no-1])\n",
    "            \n",
    "    return losses, accs, losses_test/test_len, accs_test/test_len, times, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47de073",
   "metadata": {},
   "source": [
    "# Convex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d7131d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcustom_cvx_layer\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d, num_neurons, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m        In the constructor we instantiate two nn.Linear modules and assign them as\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        member variables.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, d, num_neurons, num_classes=10):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # P x d x C\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(num_neurons, d, num_classes), requires_grad=True)\n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(num_neurons, d, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.v - self.w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.v)\n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    \n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.v, dim=1)**2) + torch.sum(torch.norm(model.w, p=1, dim=1)**2))\n",
    "    \n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    \n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.w, dim=1))\n",
    "    \n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "\n",
    "def sgd_solver_cvxproblem(ds, ds_test, num_epochs, num_neurons, beta, \n",
    "                       learning_rate, batch_size, rho, u_vectors, \n",
    "                          solver_type, LBFGS_param, verbose=False,\n",
    "                         n=60000, d=3072, num_classes=10, device='cpu'):\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # create the model\n",
    "    model = custom_cvx_layer(d, num_neurons, num_classes).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])#,\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(n / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    \n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           verbose=verbose,\n",
    "                                                           factor=0.5,\n",
    "                                                           eps=1e-12)\n",
    "    \n",
    "    model.eval()\n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) # loss on the entire test set\n",
    "    \n",
    "    iter_no = 0\n",
    "    print('starting training')\n",
    "    for i in range(num_epochs):\n",
    "        model.train()\n",
    "        for ix, (_x, _y, _z) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _z = Variable(_z).to(device)\n",
    "            \n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x, _z).float()\n",
    "            \n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, beta, rho, device)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y) # accuracy\n",
    "            #=======backward pass=====================================\n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)/len(_y)\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) # loss on the entire test set\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{}/{}], TRAIN: noncvx/cvx loss: {}, {} acc: {}. TEST: noncvx/cvx loss: {}, {} acc: {}\".format(i, num_epochs,\n",
    "                    np.round(noncvx_losses[iter_no-1], 3), np.round(losses[iter_no-1], 3), np.round(accs[iter_no-1], 3), \n",
    "                    np.round(noncvx_losses_test[i+1], 3)/10000, np.round(losses_test[i+1], 3)/10000, np.round(accs_test[i+1]/10000, 3)))\n",
    "        \n",
    "        scheduler.step(losses[iter_no-1])\n",
    "        \n",
    "    return noncvx_losses, accs, noncvx_losses_test/10000, accs_test/10000, times, losses, losses_test/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830547ab",
   "metadata": {},
   "source": [
    "# Cifar 10 Download\n",
    "\n",
    "cifar-10 -- using the version downloaded from http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "NOTE: This won't download the dataset if it already has been downloaded (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feba594f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    directory, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    directory, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04c0b6",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44064bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting the data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m dummy_loader\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      3\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m A, y \u001b[38;5;129;01min\u001b[39;00m dummy_loader:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print('Extracting the data')\n",
    "dummy_loader= torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=50000, shuffle=False,\n",
    "    pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone()\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,d=A.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880ee17",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e9a134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ARGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m      2\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m GD_only\u001b[38;5;241m=\u001b[39m\u001b[43mARGS\u001b[49m\u001b[38;5;241m.\u001b[39mGD[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m CVX_only\u001b[38;5;241m=\u001b[39mARGS\u001b[38;5;241m.\u001b[39mCVX[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m \u001b[38;5;66;03m# regularization parameter\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ARGS' is not defined"
     ]
    }
   ],
   "source": [
    "P = 4096\n",
    "verbose = True \n",
    "GD_only=ARGS.GD[0]\n",
    "CVX_only=ARGS.CVX[0]\n",
    "beta = 1e-3 # regularization parameter\n",
    "num_epochs1, batch_size =  ARGS.n_epochs[0], 1000 #\n",
    "num_neurons = P # number of neurons is equal to number of hyperplane arrangements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4611c2",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44010236",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      2\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      6\u001b[0m     test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    pin_memory=True, sampler=None)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72429c",
   "metadata": {},
   "source": [
    "# SGD Solver for Nonconvex Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d80bb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CVX_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mCVX_only\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m     solver_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     schedule\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CVX_only' is not defined"
     ]
    }
   ],
   "source": [
    "if CVX_only==0:\n",
    "\n",
    "    solver_type = \"sgd\" # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "    schedule=0 # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param = [10, 4] # these parameters are for the LBFGS solver\n",
    "    learning_rate = 1e-2\n",
    "    \n",
    "    ## SGD1 constant    \n",
    "    print('SGD1-training-mu={}'.format(learning_rate))\n",
    "    results_noncvx_sgd1 = sgd_solver_pytorch_v2(train_loader, test_loader, num_epochs1, num_neurons, beta, \n",
    "                             learning_rate, batch_size, solver_type, schedule, \n",
    "                              LBFGS_param, verbose=True, \n",
    "                            num_classes=10, D_in=d, train_len=n )\n",
    "    \n",
    "\n",
    "    ## SGD2 constant    \n",
    "    learning_rate = 5e-3\n",
    "    print('SGD2-training-mu={}'.format(learning_rate))\n",
    "    results_noncvx_sgd2 = sgd_solver_pytorch_v2(train_loader, test_loader, num_epochs1, num_neurons, beta, \n",
    "                             learning_rate, batch_size, solver_type, schedule,\n",
    "                              LBFGS_param, verbose=True, \n",
    "                            num_classes=10, D_in=d, train_len=n )\n",
    "  \n",
    "    ## SGD3 constant\n",
    "    learning_rate = 1e-3\n",
    "    print('SGD3-training-mu={}'.format(learning_rate))\n",
    "    results_noncvx_sgd3 = sgd_solver_pytorch_v2(train_loader, test_loader, num_epochs1, num_neurons, beta, \n",
    "                             learning_rate, batch_size, solver_type, schedule,\n",
    "                              LBFGS_param, verbose=True, \n",
    "                            num_classes=10, D_in=d, train_len=n )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abfb27",
   "metadata": {},
   "source": [
    "# Solver for Convex Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84c6fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GD_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mGD_only\u001b[49m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m     rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-2\u001b[39m \u001b[38;5;66;03m# coefficient to penalize the violated constraints\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     solver_type \u001b[38;5;241m=\u001b[39m ARGS\u001b[38;5;241m.\u001b[39msolver_cvx[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GD_only' is not defined"
     ]
    }
   ],
   "source": [
    "if GD_only ==0:\n",
    "\n",
    "    rho = 1e-2 # coefficient to penalize the violated constraints\n",
    "    solver_type = ARGS.solver_cvx[0] # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "    LBFGS_param = [10, 4] \n",
    "    batch_size = 1000\n",
    "    num_epochs2, batch_size = ARGS.n_epochs[1], 1000 \n",
    " \n",
    "    \n",
    "    \n",
    "    #  Convex\n",
    "    print('Generating sign patterns')\n",
    "    num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P, verbose)\n",
    "    sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "    u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "    \n",
    "    ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "    ds_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=1000, shuffle=False,\n",
    "        pin_memory=True)\n",
    "\n",
    "\n",
    "    #  Convex1\n",
    "    learning_rate = 1e-6 # 1e-6 for sgd    \n",
    "    print('Convex Random1-mu={}'.format(learning_rate))\n",
    "    results_cvx1 = sgd_solver_cvxproblem(ds_train, test_loader, num_epochs2, num_neurons, beta, \n",
    "                            learning_rate, batch_size, rho, u_vectors, solver_type, LBFGS_param, verbose=True, \n",
    "                                             n=n, device='cuda')\n",
    "\n",
    "    #  Convex2\n",
    "    learning_rate = 5e-7 # 1e-6 for sgd    \n",
    "    print('Convex Random2-mu={}'.format(learning_rate))\n",
    "    results_cvx2 = sgd_solver_cvxproblem(ds_train, test_loader, num_epochs2, num_neurons, beta, \n",
    "                            learning_rate, batch_size, rho, u_vectors, solver_type, LBFGS_param, verbose=True, \n",
    "                                             n=n, device='cuda')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #  Convex with convolutional patterns\n",
    "    print('Generating conv sign patterns')\n",
    "    num_neurons,sign_pattern_list, u_vector_list = generate_conv_sign_patterns(Apatch, P, verbose)\n",
    "    sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "    u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "    \n",
    "    ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "    ds_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #  Convex Conv1\n",
    "    learning_rate = 1e-6       \n",
    "    print('Convex Conv1-mu={}'.format(learning_rate))\n",
    "    results_cvx_conv1 = sgd_solver_cvxproblem(ds_train, test_loader, num_epochs2, num_neurons, beta, \n",
    "                            learning_rate, batch_size, rho, u_vectors, solver_type, LBFGS_param, verbose=True, \n",
    "                                             n=n, device='cuda')\n",
    "\n",
    "    #  Convex Conv2 \n",
    "    learning_rate = 5e-7       \n",
    "    print('Convex Conv2-mu={}'.format(learning_rate))\n",
    "    results_cvx_conv2 = sgd_solver_cvxproblem(ds_train, test_loader, num_epochs2, num_neurons, beta, \n",
    "                            learning_rate, batch_size, rho, u_vectors, solver_type, LBFGS_param, verbose=True, \n",
    "                                             n=n, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbd269",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e6b231",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m now \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow() \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GD_only\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m CVX_only\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m     results_noncvx_sgd1v2\u001b[38;5;241m=\u001b[39mresults_noncvx_sgd1[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "now = datetime.now() \n",
    "if GD_only==1 and CVX_only==0:\n",
    "\n",
    "\n",
    "    \n",
    "    results_noncvx_sgd1v2=results_noncvx_sgd1[:5]\n",
    "    results_noncvx_sgd2v2=results_noncvx_sgd2[:5]\n",
    "    results_noncvx_sgd3v2=results_noncvx_sgd3[:5]\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    print('Saving the objects')\n",
    "    torch.save([num_epochs1,results_noncvx_sgd1v2, results_noncvx_sgd2v2, results_noncvx_sgd3v2\n",
    "                    ],'results_fig_gdonly_stepsize_cifar10_'+now.strftime(\"%d-%m-%Y_%H-%M-%S\")+'.pt')\n",
    "    \n",
    "\n",
    "    \n",
    "elif GD_only==0 and CVX_only==1:\n",
    "\n",
    "    print('Saving the objects')\n",
    "    torch.save([num_epochs2, results_cvx1,results_cvx2, \n",
    "                    results_cvx_conv1,results_cvx_conv2],'results_fig_cvxonly_stepsize_cifar10_'+now.strftime(\"%d-%m-%Y_%H-%M-%S\")+'.pt')\n",
    "    \n",
    "else:\n",
    "\n",
    "    results_noncvx_sgd1v2=results_noncvx_sgd1[:5]\n",
    "    results_noncvx_sgd2v2=results_noncvx_sgd2[:5]\n",
    "    results_noncvx_sgd3v2=results_noncvx_sgd3[:5]\n",
    "    print('Saving the objects')\n",
    "    torch.save([num_epochs1,num_epochs2,results_noncvx_sgd1v2, results_noncvx_sgd2v2, results_noncvx_sgd3v2, results_cvx1,results_cvx2, \n",
    "                    results_cvx_conv1,results_cvx_conv2],'results_fig_all_stepsize_cifar10_'+now.strftime(\"%d-%m-%Y_%H-%M-%S\")+'.pt')\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    skip=1#int(num_epochs1/num_epochs2)\n",
    "    mark_sgd=10\n",
    "    mark_cvx=30\n",
    "    \n",
    "    marker_size_sgd=10\n",
    "    marker_size_cvx=12\n",
    "    \n",
    "\n",
    "    plt.gcf().set_facecolor(\"white\")\n",
    "    #fig,ax = plt.subplots()\n",
    "    \n",
    "    # plot\n",
    "    fsize=24\n",
    "    fsize_legend=15\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "    plt.xlabel('Time(s)',fontsize=fsize);  plt.grid()\n",
    "    \n",
    "    plot_no = 1 # select --> 0: cost, 1: accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_all_iters1 = results_noncvx_sgd1v2[4].shape[0] - 1\n",
    "    num_all_iters2 = results_cvx1[4].shape[0] - 1\n",
    "    \n",
    "    iters_per_epoch1 = num_all_iters1 // num_epochs1\n",
    "    iters_per_epoch2 = num_all_iters2 // num_epochs2\n",
    "    \n",
    "    epoch_times_noncvx1 = results_noncvx_sgd1v2[4][0:num_all_iters1+1:iters_per_epoch1]-results_noncvx_sgd1v2[4][0]\n",
    "    epoch_times_noncvx2 = results_noncvx_sgd2v2[4][0:num_all_iters1+1:iters_per_epoch1]-results_noncvx_sgd2v2[4][0]\n",
    "    epoch_times_noncvx3 = results_noncvx_sgd3v2[4][0:num_all_iters1+1:iters_per_epoch1]-results_noncvx_sgd3v2[4][0]\n",
    "    \n",
    "    \n",
    "    epoch_times_cvx1 = results_cvx1[4][0:num_all_iters2+1:iters_per_epoch2]-results_cvx1[4][0]\n",
    "    epoch_times_cvx2 = results_cvx2[4][0:num_all_iters2+1:iters_per_epoch2]-results_cvx2[4][0]\n",
    "    \n",
    "    epoch_times_cvx_conv1= results_cvx_conv1[4][0:num_all_iters2+1:iters_per_epoch2]-results_cvx_conv1[4][0]\n",
    "    epoch_times_cvx_conv2= results_cvx_conv2[4][0:num_all_iters2+1:iters_per_epoch2]-results_cvx_conv2[4][0]\n",
    "    \n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    # To plot results in the validation set\n",
    "    plt.plot( epoch_times_noncvx1[::skip],results_noncvx_sgd1v2[plot_no+2][::skip],'--', color='darkred', markevery=mark_sgd,linewidth=3.0, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-2$\")\n",
    "    plt.plot( epoch_times_noncvx2[::skip],results_noncvx_sgd2v2[plot_no+2][::skip],'--', color='red', markevery=mark_sgd,linewidth=3.0, markersize=marker_size_sgd,label=\"SGD-$\\mu=5e-3$\")\n",
    "    plt.plot( epoch_times_noncvx3[::skip],results_noncvx_sgd3v2[plot_no+2][::skip],'--', color='lightcoral', markevery=mark_sgd,linewidth=3.0, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-3$\")\n",
    "    \n",
    "    \n",
    "    plt.plot( epoch_times_cvx1,results_cvx1[plot_no+2],  'o--', color='g', markevery=mark_cvx,linewidth=3.0, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=1e-6$\")\n",
    "    plt.plot( epoch_times_cvx2,results_cvx2[plot_no+2],  'o--', color='lime', markevery=mark_cvx,linewidth=3.0, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=5e-7$\")\n",
    "    \n",
    "    plt.plot( epoch_times_cvx_conv1,results_cvx_conv1[plot_no+2],  'o--', color='b', markevery=mark_cvx,linewidth=3.0, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=1e-6$\")\n",
    "    plt.plot( epoch_times_cvx_conv2,results_cvx_conv1[plot_no+2],  'o--', color='lightblue', markevery=mark_cvx,linewidth=3.0, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=5e-7$\")\n",
    "    \n",
    "\n",
    "    plt.legend(prop={'size': fsize_legend})\n",
    "    plt.ylabel(\"Test Accuracy\",fontsize=fsize)\n",
    "    plt.ylim(0.3, 0.6)\n",
    "    plt.xlim(0, 4500)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.savefig('cifar_multiclass_stepsize_testacc.png', format='png', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    # To plot training  acc\n",
    "    \n",
    "    plt.xlabel('Time(s)',fontsize=fsize)  \n",
    "    plt.grid()\n",
    "\n",
    "    p11=results_noncvx_sgd1v2[1].reshape(-1,1)\n",
    "    p12=results_noncvx_sgd2v2[1].reshape(-1,1)\n",
    "    p13=results_noncvx_sgd3v2[1].reshape(-1,1)\n",
    "    \n",
    "    p21=results_cvx1[1].reshape(-1,1)\n",
    "    p22=results_cvx2[1].reshape(-1,1)\n",
    "    \n",
    "    p31=results_cvx_conv1[1].reshape(-1,1)\n",
    "    p32=results_cvx_conv2[1].reshape(-1,1)\n",
    "    \n",
    "\n",
    "    \n",
    "    n=50000\n",
    "    batch_size1=1000\n",
    "    batch_size2=1000\n",
    "    \n",
    "    plt.plot(epoch_times_noncvx1[:-1][::skip],p11[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='darkred', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-2$\")\n",
    "    plt.plot(epoch_times_noncvx2[:-1][::skip],p12[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='red', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=5e-2$\")\n",
    "    plt.plot(epoch_times_noncvx3[:-1][::skip],p13[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='lightcoral', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-3$\")\n",
    "    \n",
    "    plt.plot( epoch_times_cvx1[:-1],p21[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-',color='g', markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=1e-6$\")\n",
    "    plt.plot( epoch_times_cvx2[:-1],p22[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-',color='lime', markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=5e-7$\")\n",
    "    \n",
    "    plt.plot( epoch_times_cvx_conv1[:-1],p31[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-', color='b',markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=1e-6$\")\n",
    "    plt.plot( epoch_times_cvx_conv2[:-1],p32[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-', color='lightblue',markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=5e-7$\")\n",
    "    \n",
    "\n",
    "    plt.xlim(0, 4500)\n",
    "    \n",
    "    plt.ylabel(\"Training Accuracy\",fontsize=fsize)\n",
    "    plt.grid()\n",
    "    matplotlib.pyplot.grid(True, which=\"both\")\n",
    "    plt.savefig('cifar_multiclass_stepsize_tracc.png', format='png', bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    # To plot training loss\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.xlabel('Time(s)',fontsize=fsize)  \n",
    "    plt.grid()\n",
    "    p11=results_noncvx_sgd1v2[0].reshape(-1,1)\n",
    "    p12=results_noncvx_sgd2v2[0].reshape(-1,1)\n",
    "    p13=results_noncvx_sgd3v2[0].reshape(-1,1)\n",
    "    \n",
    "    p21=results_cvx1[5].reshape(-1,1)\n",
    "    p22=results_cvx2[5].reshape(-1,1)\n",
    "    \n",
    "    p31=results_cvx_conv1[5].reshape(-1,1)\n",
    "    p32=results_cvx_conv2[5].reshape(-1,1)\n",
    "    \n",
    "\n",
    "    \n",
    "    n=50000\n",
    "    batch_size1=1000\n",
    "    batch_size2=1000\n",
    "    \n",
    "    plt.semilogy(epoch_times_noncvx1[:-1][::skip],p11[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='darkred', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-2$\")\n",
    "    plt.semilogy(epoch_times_noncvx2[:-1][::skip],p12[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='red', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=5e-2$\")\n",
    "    plt.semilogy(epoch_times_noncvx3[:-1][::skip],p13[np.arange(num_epochs1)*int(n/batch_size1)][::skip],'-',color='lightcoral', markevery=mark_sgd,linewidth=3, markersize=marker_size_sgd,label=\"SGD-$\\mu=1e-3$\")\n",
    "    \n",
    "    plt.semilogy( epoch_times_cvx1[:-1],p21[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-',color='g', markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=1e-6$\")\n",
    "    plt.semilogy( epoch_times_cvx2[:-1],p22[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-',color='lime', markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Random-$\\mu=5e-7$\")\n",
    "    \n",
    "    plt.semilogy( epoch_times_cvx_conv1[:-1],p31[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-', color='b',markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=1e-6$\")\n",
    "    plt.semilogy( epoch_times_cvx_conv2[:-1],p32[np.arange(num_epochs2)*int(n/batch_size2)] ,'o-', color='lightblue',markevery=mark_cvx,linewidth=3, markersize=marker_size_cvx,label=\"Convex-Conv-$\\mu=5e-7$\")\n",
    "    \n",
    "\n",
    "    plt.xlim(0, 4500)\n",
    "    \n",
    "    plt.ylabel(\"Objective Value\",fontsize=fsize)\n",
    "    plt.grid()\n",
    "    matplotlib.pyplot.grid(True, which=\"both\")\n",
    "    plt.savefig('cifar_multiclass_stepsize_obj.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e5ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
