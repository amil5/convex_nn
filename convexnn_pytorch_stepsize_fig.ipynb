{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1844ce06",
   "metadata": {},
   "source": [
    "# From `convexnn_pytorch_stepsize_fig.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeef79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "import time\n",
    "import scipy\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import torch\n",
    "import sklearn.linear_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda55e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(inp=[\"--GD\", \"0\", \"--CVX\", \"0\", \"--n_epochs\",\"100\", \"100\", \"--solver_cvx\", \"sgd\"]):\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--GD', nargs=1, type=int, required=True)\n",
    "    parser.add_argument('--CVX', nargs=1, type=int, required=True)\n",
    "    parser.add_argument('--n_epochs', nargs=2, type=int, required=True)\n",
    "    parser.add_argument('--solver_cvx', type=str, nargs=1, default=\"adam\")\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    args = parser.parse_args(inp)\n",
    "    random.seed(a=args.seed)\n",
    "    np.random.seed(seed=args.seed)\n",
    "    torch.manual_seed(seed=args.seed)\n",
    "    return args\n",
    "\n",
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, H, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, H, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(H, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "# functions for generating sign patterns\n",
    "def check_if_already_exists(element_list, element):\n",
    "    # check if element exists in element_list\n",
    "    # where element is a numpy array\n",
    "    for i in range(len(element_list)):\n",
    "        if np.array_equal(element_list[i], element):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "            \n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class PrepareData3D(Dataset):\n",
    "    def __init__(self, X, y, z):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "            \n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "        \n",
    "        if not torch.is_tensor(z):\n",
    "            self.z = torch.from_numpy(z)\n",
    "        else:\n",
    "            self.z = z\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.z[idx]\n",
    "\n",
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "def one_hot(labels, num_classes=10):\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels.long()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c685c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS=parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9d82d",
   "metadata": {},
   "source": [
    "# Standard Non-Convex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4421bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    \n",
    "    ## l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "\n",
    "        output = model.forward(_x)\n",
    "        yhat = model(_x).float()\n",
    "\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "\n",
    "    return test_loss, test_correct\n",
    "\n",
    "# solves nonconvex problem\n",
    "def sgd_solver_pytorch_v2(ds, ds_test, num_epochs, num_neurons, beta, \n",
    "                         learning_rate, batch_size, solver_type, schedule, \n",
    "                          LBFGS_param, verbose=False, \n",
    "                        num_classes=10, D_in=3*1024, test_len=10000, \n",
    "                          train_len=50000, device='cuda'):\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    # D_in is input dimension, H is hidden dimension, D_out is output dimension.\n",
    "    H, D_out = num_neurons, num_classes\n",
    "    # create the model\n",
    "    model = FCNetwork(H, D_out, D_in).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])#,\n",
    "        \n",
    "    # arrays for saving the loss and accuracy    \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(train_len / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    \n",
    "    losses_test[0], accs_test[0] = validation_primal(model, ds_test, beta, device) # loss on the entire test set\n",
    "    \n",
    "    if schedule==1:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           verbose=verbose,\n",
    "                                                           factor=0.5,\n",
    "                                                           eps=1e-12)\n",
    "    elif schedule==2:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
    "        \n",
    "    iter_no = 0\n",
    "    for i in range(num_epochs):\n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            \n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x).float()\n",
    "            \n",
    "            loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "            \n",
    "           \n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1] = validation_primal(model, ds_test, beta, device) # loss on the entire test set\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{}/{}], loss: {} acc: {}, test loss: {} test acc: {}\".format(i, num_epochs,\n",
    "                    np.round(losses[iter_no-1], 3), np.round(accs[iter_no-1], 3), \n",
    "                    np.round(losses_test[i+1], 3)/test_len, np.round(accs_test[i+1]/test_len, 3)))\n",
    "        if schedule>0:\n",
    "            scheduler.step(losses[iter_no-1])\n",
    "            \n",
    "    return losses, accs, losses_test/test_len, accs_test/test_len, times, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47de073",
   "metadata": {},
   "source": [
    "# Convex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d7131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, d, num_neurons, num_classes=10):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # P x d x C\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(num_neurons, d, num_classes), requires_grad=True)\n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(num_neurons, d, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.v - self.w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.v)\n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    \n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.v, dim=1)**2) + torch.sum(torch.norm(model.w, p=1, dim=1)**2))\n",
    "    \n",
    "    return prediction_cost + regularization_cost\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    \n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.w, dim=1))\n",
    "    \n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "def sgd_solver_cvxproblem(ds, ds_test, num_epochs, num_neurons, beta, \n",
    "                       learning_rate, batch_size, rho, u_vectors, \n",
    "                          solver_type, LBFGS_param, verbose=False,\n",
    "                         n=60000, d=3072, num_classes=10, device='cpu'):\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # create the model\n",
    "    model = custom_cvx_layer(d, num_neurons, num_classes).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#,\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])#,\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(n / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    \n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           verbose=verbose,\n",
    "                                                           factor=0.5,\n",
    "                                                           eps=1e-12)\n",
    "    \n",
    "    model.eval()\n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) # loss on the entire test set\n",
    "    \n",
    "    iter_no = 0\n",
    "    print('starting training')\n",
    "    for i in range(num_epochs):\n",
    "        model.train()\n",
    "        for ix, (_x, _y, _z) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _z = Variable(_z).to(device)\n",
    "            \n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x, _z).float()\n",
    "            \n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, beta, rho, device)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y) # accuracy\n",
    "            #=======backward pass=====================================\n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)/len(_y)\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) # loss on the entire test set\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{}/{}], TRAIN: noncvx/cvx loss: {}, {} acc: {}. TEST: noncvx/cvx loss: {}, {} acc: {}\".format(i, num_epochs,\n",
    "                    np.round(noncvx_losses[iter_no-1], 3), np.round(losses[iter_no-1], 3), np.round(accs[iter_no-1], 3), \n",
    "                    np.round(noncvx_losses_test[i+1], 3)/10000, np.round(losses_test[i+1], 3)/10000, np.round(accs_test[i+1]/10000, 3)))\n",
    "        \n",
    "        scheduler.step(losses[iter_no-1])\n",
    "        \n",
    "    return noncvx_losses, accs, noncvx_losses_test/10000, accs_test/10000, times, losses, losses_test/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830547ab",
   "metadata": {},
   "source": [
    "Line 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba594f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
