{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd1cb18",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` to be more sane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a67af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ab2e1",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba40b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d69deace70>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 1 # ADD TWO ZEROS\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 1\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dd14f",
   "metadata": {},
   "source": [
    "# Data Preparation Functions\n",
    "Most have been relegated to `helperfunctions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0ad16",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c370d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(model,path,num_neurons=4096,num_classes=10,dim_in=3072):\n",
    "    model = FCNetwork(num_neurons,num_classes,dim_in)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct\n",
    "\n",
    "\n",
    "def sgd_solver_pytorch_v2(ds, ds_test, save_path='', num_epochs=100, num_neurons=4096, beta=1e-3,\n",
    "                          learning_rate=1e-3, batch_size=1000, solver_type='sgd', schedule=0, \n",
    "                          LBFGS_param=(10,4), verbose=False, num_classes=10, dim_in=3*1024, \n",
    "                          test_len=10000, train_len=50000, device='cuda'):\n",
    "    \"\"\"\n",
    "    ds            : Training dataset (torch DataLoader)\n",
    "    ds_test       : Test dataset (torch DataLoader)\n",
    "    save_path     : str, path to save the model at (doesn't save if '')\n",
    "    num_epochs    : int\n",
    "    num_neurons   : hidden layer size, int\n",
    "    beta          : regularization scalar on the norms of the weight matrices, float\n",
    "    learning rate : float\n",
    "    batch_size    : int\n",
    "    solver_type   : any in ['sgd','adam','adagrad','adadelta','LBFGS']\n",
    "    schedule      : int in (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param   : (int,int) history size and max iterations for solver_type='LBFGS'\n",
    "    verbose       : bool\n",
    "    num_classes   : int\n",
    "    dim_in        : int, input dimension (32*32*3) for cifar10\n",
    "    test_len      : int, number of test images\n",
    "    train_len     : int, number of training images\n",
    "    device        : str, 'cuda' or 'cpu' \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model = FCNetwork(num_neurons, num_classes, dim_in).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])\n",
    "        \n",
    "    # Arrays for saving the loss and accuracy    \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(train_len / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    \n",
    "    # loss on the entire test set\n",
    "    losses_test[0], accs_test[0] = validation_primal(model, ds_test, beta, device) \n",
    "    \n",
    "    if schedule==1:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=verbose, factor=0.5, eps=1e-12)\n",
    "    elif schedule==2:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
    "        \n",
    "    iter_no = 0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "            _y = Variable(_y).to(device) # shape 1000\n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x).float()\n",
    "            \n",
    "            loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        losses_test[i+1], accs_test[i+1] = validation_primal(model, ds_test, beta, device)\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{:2>}/{:}], loss: {:.3f} acc: {:.3f}, test loss: {:.3f} test acc: {:.3f}\".format(i, num_epochs,\n",
    "                   losses[iter_no-1],accs[iter_no-1],losses_test[i+1]/test_len,accs_test[i+1]/test_len))\n",
    "        if schedule>0:\n",
    "            scheduler.step(losses[iter_no-1])\n",
    "            \n",
    "    if save_path != '':\n",
    "        save_model(model,save_path)\n",
    "    return losses, accs, losses_test/test_len, accs_test/test_len, times, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c6232",
   "metadata": {},
   "source": [
    "## Convex Network\n",
    "Haven't cleaned these up as much given that I'm not 100% sure how it works.\n",
    "TODO: Figure out the hyperplane arangement stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4413a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b117da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.v - self.w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.v)\n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.v, dim=1)**2) + torch.sum(torch.norm(model.w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "\n",
    "\n",
    "def sgd_solver_cvxproblem(ds, ds_test, save_path='', num_epochs=100, num_neurons=4096, beta=1e-3, \n",
    "                          learning_rate=1e-2, batch_size=1000, rho=1e-2, u_vectors=None, \n",
    "                          solver_type='sgd', LBFGS_param=(10,4), verbose=False, num_classes=10,\n",
    "                          dim_in=3072, n=60000, device='cuda'):\n",
    "    \"\"\"\n",
    "    ds            : Training dataset (torch DataLoader)\n",
    "    ds_test       : Test dataset (torch DataLoader)\n",
    "    save_path     : str, path to save the model at (doesn't save if '')\n",
    "    num_epochs    : int\n",
    "    num_neurons   : hidden layer size, int\n",
    "    beta          : regularization scalar on the norms of the weight matrices, float\n",
    "    learning rate : float\n",
    "    batch_size    : int\n",
    "    rho           : float, coefficient to penalize the violated constraints\n",
    "    u_vectors     : Comes from sign patterns, tbd\n",
    "    solver_type   : any in ['sgd','adam','adagrad','adadelta','LBFGS']\n",
    "    schedule      : int in (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param   : (int,int) history size and max iterations for solver_type='LBFGS'\n",
    "    verbose       : bool\n",
    "    num_classes   : int\n",
    "    dim_in        : int, input dimension (32*32*3) for cifar10\n",
    "    n             : int, number of iterations? \n",
    "    device        : str, 'cuda' or 'cpu' \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model = custom_cvx_layer(num_neurons=num_neurons, num_classes=num_classes, input_dim=dim_in).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif solver_type == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(n / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=verbose, factor=0.5, eps=1e-12)\n",
    "    \n",
    "    model.eval() # Evaluation mode\n",
    "    # Compute loss on entire test set\n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) \n",
    "    \n",
    "    iter_no = 0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        model.train() # Training mode\n",
    "        for ix, (_x, _y, _z) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _z = Variable(_z).to(device)\n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x, _z).float()\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, beta, rho, device)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y) # accuracy\n",
    "            #=======backward pass=====================================\n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)/len(_y)\n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) # loss on the entire test set\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{:2>}/{:}], loss: {:.3f} acc: {:.3f}, test loss: {:.3f} test acc: {:.3f}\".format(i, num_epochs,\n",
    "                    noncvx_losses[iter_no-1], losses[iter_no-1], accs[iter_no-1], noncvx_losses_test[i+1]/10000, \n",
    "                    losses_test[i+1]/10000, accs_test[i+1]/10000))\n",
    "        scheduler.step(losses[iter_no-1])\n",
    "        \n",
    "    if save_path != '':\n",
    "        print(\"AH\")\n",
    "        save_model(model,save_path)\n",
    "    return noncvx_losses, accs, noncvx_losses_test/10000, accs_test/10000, times, losses, losses_test/10000, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166a684",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c56249eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64459f",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54b2e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=P[\"batch_size\"], shuffle=True,\n",
    "    pin_memory=True, sampler=None)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=P[\"batch_size\"], shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13e24b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 1\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n"
     ]
    }
   ],
   "source": [
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b70f6a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a562605286fb4ffe955ec764d18c9695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], loss: 0.432 acc: 0.321, test loss: 0.422 test acc: 0.343\n"
     ]
    }
   ],
   "source": [
    "ncvx_save_loc = \"ncvx_nn{:}_solver{:}_lr1e-3\".format(P['num_neurons'],P['ncvx_solver'])\n",
    "\n",
    "results_ncvx = sgd_solver_pytorch_v2(train_loader, test_loader, save_path=ncvx_save_loc, \n",
    "                                       num_epochs = P[\"ncvx_num_epochs\"],\n",
    "                                       num_neurons = P[\"num_neurons\"], \n",
    "                                       beta = P[\"beta\"],\n",
    "                                       learning_rate = P[\"ncvx_learning_rate\"],\n",
    "                                       batch_size = P[\"batch_size\"],\n",
    "                                       solver_type = P[\"ncvx_solver\"],\n",
    "                                       schedule = P[\"ncvx_schedule\"],\n",
    "                                       LBFGS_param = P[\"ncvx_LBFGS_param\"],\n",
    "                                       verbose = P[\"verbose\"],\n",
    "                                       num_classes = P[\"num_classes\"],\n",
    "                                       dim_in = P[\"dim_in\"],\n",
    "                                       test_len = P[\"ncvx_test_len\"],\n",
    "                                       train_len = P[\"ncvx_train_len\"],\n",
    "                                       device = P[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7ccc3",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec934f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter         : Value\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 1\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n"
     ]
    }
   ],
   "source": [
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3010a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n"
     ]
    }
   ],
   "source": [
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=P[\"batch_size\"], shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c3f6948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72854fc1f3544deb920df92215c9b4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], loss: 0.405 acc: 0.364, test loss: 0.454 test acc: 0.410\n",
      "AH\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"cvx_nn{:}_solver{:}_lr1e-3\".format(P['num_neurons'],P['cvx_solver'])\n",
    "\n",
    "results_cvx = sgd_solver_cvxproblem(ds_train, test_loader, save_path=cvx_save_loc,\n",
    "                                    num_epochs = P[\"cvx_num_epochs\"],\n",
    "                                    num_neurons = P[\"num_neurons\"], \n",
    "                                    beta = P[\"beta\"],\n",
    "                                    learning_rate = P[\"cvx_learning_rate\"],\n",
    "                                    batch_size = P[\"batch_size\"],\n",
    "                                    rho = P[\"cvx_rho\"],\n",
    "                                    u_vectors = u_vectors,\n",
    "                                    solver_type = P[\"cvx_solver\"],\n",
    "                                    LBFGS_param = P[\"cvx_LBFGS_param\"],\n",
    "                                    verbose = P[\"verbose\"],\n",
    "                                    num_classes = P[\"num_classes\"],\n",
    "                                    dim_in = P[\"dim_in\"],\n",
    "                                    n = n,\n",
    "                                    device = P[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe6295",
   "metadata": {},
   "source": [
    "# TODO: \n",
    " * Reorder output of training functions, also rename them.\n",
    " * Convert output to dataframes to save as csvs\n",
    " * Better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea075a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.3f}\".format(np.array([1,2,3])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44919c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
