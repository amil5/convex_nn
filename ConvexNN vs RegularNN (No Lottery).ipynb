{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49062733",
   "metadata": {},
   "source": [
    "# Rewrite of `convexnn_pytorch_stepsize_fig.py` to be more sane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d4198c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a6a25",
   "metadata": {},
   "source": [
    "# Parameters and Args\n",
    "I'm not using argparse in a notebook, it's gross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ddd0ed2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d69deace70>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dict()\n",
    "P['seed'] = 42        # Well we can tell who read Hitchhiker's Guide to the Galaxy lol\n",
    "P['device'] = 'cuda'  # Or 'cpu'\n",
    "P['verbose'] = True\n",
    "P['P'] = 4096         # Number of hyperplane arrangements and number of neurons\n",
    "P['num_neurons'] = P['P']\n",
    "P[\"num_classes\"] = 10\n",
    "P[\"dim_in\"] = 3*32*32\n",
    "P['batch_size'] = 1000\n",
    "P['beta'] = 1e-3      # Regularization parameter (in loss)\n",
    "P['dir'] = os.path.abspath('')\n",
    "P[\"print_freq\"] = 5\n",
    "P['device'] = 'cuda'\n",
    "\n",
    "# Nonconvex (Regular) Args:\n",
    "P['ncvx_solver'] = 'sgd'       # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['ncvx_schedule'] = 0         # learning rate schedule (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "P['ncvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['ncvx_num_epochs'] = 100\n",
    "P[\"ncvx_learning_rate\"] = 1e-3\n",
    "P[\"ncvx_train_len\"] = 50000\n",
    "P[\"ncvx_test_len\"] = 10000\n",
    "\n",
    "# Convex Args:\n",
    "P['cvx_solver'] = 'sgd'   # pick: \"sgd\", \"adam\", \"adagrad\", \"adadelta\", \"LBFGS\"\n",
    "P['cvx_LBFGS_param'] = (10,4) # params for solver LBFGS\n",
    "P['cvx_num_epochs'] = 100\n",
    "P['cvx_learning_rate'] = 5e-7\n",
    "P['cvx_rho'] = 1e-2\n",
    "P['cvx_test_len'] = 10000\n",
    "\n",
    "# Set seed\n",
    "random.seed(a=P['seed'])\n",
    "np.random.seed(seed=P['seed'])\n",
    "torch.manual_seed(seed=P['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2028f",
   "metadata": {},
   "source": [
    "# Data Preparation Functions\n",
    "Most have been relegated to `helperfunctions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa1877",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Standard Non-Convex Network\n",
    "Consists of typical 2-layer network definition, training and test loss, as well as training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "273e45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_dim, num_neurons, bias=False), nn.ReLU())\n",
    "        self.layer2 = nn.Linear(num_neurons, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out\n",
    "    \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "    \n",
    "def load_fc_model(path,num_neurons=4096,num_classes=10,dim_in=3072):\n",
    "    model = FCNetwork(num_neurons,num_classes,dim_in)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss_func_primal(yhat, y, model, beta):\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # l2 norm on first layer weights, l1 squared norm on second layer\n",
    "    for layer, p in enumerate(model.parameters()):\n",
    "        if layer == 0:\n",
    "            loss += beta/2 * torch.norm(p)**2\n",
    "        else:\n",
    "            loss += beta/2 * sum([torch.norm(p[:, j], 1)**2 for j in range(p.shape[1])])\n",
    "    return loss\n",
    "\n",
    "def validation_primal(model, testloader, beta, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float().to(device)\n",
    "        _y = Variable(_y).float().to(device)\n",
    "        #output = model.forward(_x) # Does this do anything?\n",
    "        yhat = model(_x).float()\n",
    "        loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()\n",
    "    return test_loss, test_correct\n",
    "\n",
    "\n",
    "def ncvxproblem(ds, ds_test, model, save_path='', num_epochs=100, beta=1e-3,\n",
    "                learning_rate=1e-3, batch_size=1000, solver_type='sgd', schedule=0, \n",
    "                LBFGS_param=(10,4), verbose=False, \n",
    "                test_len=10000, train_len=50000, device='cuda'):\n",
    "    \"\"\"\n",
    "    ds            : Training dataset (torch DataLoader)\n",
    "    ds_test       : Test dataset (torch DataLoader)\n",
    "    model         : Torch.nn model\n",
    "    save_path     : str, path to save the model at (doesn't save if '')\n",
    "    num_epochs    : int\n",
    "    beta          : regularization scalar on the norms of the weight matrices, float\n",
    "    learning rate : float\n",
    "    batch_size    : int\n",
    "    solver_type   : any in ['sgd','adam','adagrad','adadelta','LBFGS']\n",
    "    schedule      : int in (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param   : (int,int) history size and max iterations for solver_type='LBFGS'\n",
    "    verbose       : bool\n",
    "    test_len      : int, number of test images\n",
    "    train_len     : int, number of training images\n",
    "    device        : str, 'cuda' or 'cpu' \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = get_optimizer(model,solver_type,learning_rate,LBFGS_param)\n",
    "        \n",
    "    # Arrays for saving the loss and accuracy    \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(train_len / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    \n",
    "    # loss on the entire test set\n",
    "    losses_test[0], accs_test[0] = validation_primal(model, ds_test, beta, device) \n",
    "    \n",
    "    if schedule==1:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=verbose, factor=0.5, eps=1e-12)\n",
    "    elif schedule==2:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
    "        \n",
    "    iter_no = 0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device) # shape 1000,3,32,32\n",
    "            _y = Variable(_y).to(device) # shape 1000\n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x).float()\n",
    "            \n",
    "            loss = loss_func_primal(yhat, one_hot(_y).to(device), model, beta)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), torch.squeeze(_y)).float().sum()/len(_y)\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        losses_test[i+1], accs_test[i+1] = validation_primal(model, ds_test, beta, device)\n",
    "\n",
    "        if i % 5 == 0 or i == num_epochs - 1:\n",
    "            print(\"Epoch [{:>2}/{:}], loss: {:.3f} acc: {:.3f}, TEST loss: {:.3f} test acc: {:.3f}\".format(i, num_epochs,\n",
    "                   losses[iter_no-1],accs[iter_no-1],losses_test[i+1]/test_len,accs_test[i+1]/test_len))\n",
    "        if schedule>0:\n",
    "            scheduler.step(losses[iter_no-1])\n",
    "            \n",
    "    if save_path != '':\n",
    "        save_model(model,save_path)\n",
    "    return {\"losses\":losses, \"accs\":accs, \"losses_test\":losses_test/test_len,\n",
    "            \"accs_test\":accs_test/test_len, \"times\":np.diff(times), \"model\":model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb53e6f",
   "metadata": {},
   "source": [
    "## Convex Network\n",
    "Haven't cleaned these up as much given that I'm not 100% sure how it works.\n",
    "TODO: Figure out the hyperplane arangement stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "86beb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conv_sign_patterns(A2, P, verbose=False): \n",
    "    # generate convolutional sign patterns\n",
    "    n, c, p1, p2 = A2.shape\n",
    "    A = A2.reshape(n,int(c*p1*p2))\n",
    "    fsize=9*c\n",
    "    d=c*p1*p2;\n",
    "    fs=int(np.sqrt(9))\n",
    "    unique_sign_pattern_list = []  \n",
    "    u_vector_list = []             \n",
    "\n",
    "    for i in range(P): \n",
    "        # obtain a sign pattern\n",
    "        ind1=np.random.randint(0,p1-fs+1)\n",
    "        ind2=np.random.randint(0,p2-fs+1)\n",
    "        u1p= np.zeros((c,p1,p2))\n",
    "        u1p[:,ind1:ind1+fs,ind2:ind2+fs]=np.random.normal(0, 1, (fsize,1)).reshape(c,fs,fs)\n",
    "        u1=u1p.reshape(d,1)\n",
    "        sampled_sign_pattern = (np.matmul(A, u1) >= 0)[:,0]\n",
    "        unique_sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of unique sign patterns generated: \" + str(len(unique_sign_pattern_list)))\n",
    "    return len(unique_sign_pattern_list),unique_sign_pattern_list, u_vector_list\n",
    "\n",
    "def generate_sign_patterns(A, P, verbose=False):\n",
    "    # generate sign patterns\n",
    "    n, d = A.shape\n",
    "    sign_pattern_list = []  # sign patterns\n",
    "    u_vector_list = []             # random vectors used to generate the sign paterns\n",
    "    umat = np.random.normal(0, 1, (d,P))\n",
    "    sampled_sign_pattern_mat = (np.matmul(A, umat) >= 0)\n",
    "    for i in range(P):\n",
    "        sampled_sign_pattern = sampled_sign_pattern_mat[:,i]\n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(umat[:,i])\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7fe7f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, num_neurons=4096, num_classes=10, input_dim=3072):\n",
    "        self.num_classes = num_classes\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        # (num_neurons) P x (input_dim) d x (num_classes) C\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(num_neurons, input_dim, num_classes), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, sign_patterns):\n",
    "        sign_patterns = sign_patterns.unsqueeze(2)\n",
    "        x = x.view(x.shape[0], -1) # n x d\n",
    "        \n",
    "        Xv_w = torch.matmul(x, self.v - self.w) # P x N x C\n",
    "        \n",
    "        # for some reason, the permutation is necessary. not sure why\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w.permute(1, 0, 2)) #  N x P x C\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=False) # N x C\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    Xv = torch.matmul(_x, model.v)\n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]).to(device))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]).to(device))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=0, keepdim=False)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2\n",
    "    regularization_cost = beta * (torch.sum(torch.norm(model.v, dim=1)**2) + torch.sum(torch.norm(model.w, p=1, dim=1)**2))\n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, sign_patterns, beta, rho, device):\n",
    "    _x = _x.view(_x.shape[0], -1)\n",
    "    # term 1\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2\n",
    "    # term 2\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.v, dim=1))\n",
    "    loss = loss + beta * torch.sum(torch.norm(model.w, dim=1))\n",
    "    # term 3\n",
    "    sign_patterns = sign_patterns.unsqueeze(2) # N x P x 1\n",
    "    \n",
    "    Xv = torch.matmul(_x, torch.sum(model.v, dim=2, keepdim=True)) # N x d times P x d x 1 -> P x N x 1\n",
    "    DXv = torch.mul(sign_patterns, Xv.permute(1, 0, 2)) # P x N x 1\n",
    "    relu_term_v = torch.max(-2*DXv + Xv.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_v)\n",
    "    \n",
    "    Xw = torch.matmul(_x, torch.sum(model.w, dim=2, keepdim=True))\n",
    "    DXw = torch.mul(sign_patterns, Xw.permute(1, 0, 2))\n",
    "    relu_term_w = torch.max(-2*DXw + Xw.permute(1, 0, 2), torch.Tensor([0]).to(device))\n",
    "    loss = loss + rho * torch.sum(relu_term_w)\n",
    "    return loss\n",
    "\n",
    "def validation_cvxproblem(model, testloader, u_vectors, beta, rho, device):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ix, (_x, _y) in enumerate(testloader):\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _x = _x.view(_x.shape[0], -1)\n",
    "            _z = (torch.matmul(_x, torch.from_numpy(u_vectors).float().to(device)) >= 0)\n",
    "\n",
    "            output = model.forward(_x, _z)\n",
    "            yhat = model(_x, _z).float()\n",
    "\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x, _z, beta, rho, device)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()\n",
    "\n",
    "            test_noncvx_cost += get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "\n",
    "\n",
    "def cvxproblem(ds, ds_test, save_path='', num_epochs=100, num_neurons=4096, beta=1e-3, \n",
    "                          learning_rate=1e-2, batch_size=1000, rho=1e-2, u_vectors=None, \n",
    "                          solver_type='sgd', LBFGS_param=(10,4), verbose=False, num_classes=10,\n",
    "                          dim_in=3072, n=50000, test_len=10000, device='cuda'):\n",
    "    \"\"\"\n",
    "    ds            : Training dataset (torch DataLoader)\n",
    "    ds_test       : Test dataset (torch DataLoader)\n",
    "    save_path     : str, path to save the model at (doesn't save if '')\n",
    "    num_epochs    : int\n",
    "    num_neurons   : hidden layer size, int\n",
    "    beta          : regularization scalar on the norms of the weight matrices, float\n",
    "    learning rate : float\n",
    "    batch_size    : int\n",
    "    rho           : float, coefficient to penalize the violated constraints\n",
    "    u_vectors     : Comes from sign patterns, tbd\n",
    "    solver_type   : any in ['sgd','adam','adagrad','adadelta','LBFGS']\n",
    "    schedule      : int in (0: Nothing, 1: ReduceLROnPlateau, 2: ExponentialLR)\n",
    "    LBFGS_param   : (int,int) history size and max iterations for solver_type='LBFGS'\n",
    "    verbose       : bool\n",
    "    num_classes   : int\n",
    "    dim_in        : int, input dimension (32*32*3) for cifar10\n",
    "    n             : int, number of iterations? \n",
    "    test_len      : int, number of test iterations\n",
    "    device        : str, 'cuda' or 'cpu' \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model = custom_cvx_layer(num_neurons=num_neurons, num_classes=num_classes, input_dim=dim_in).to(device)\n",
    "    \n",
    "    optimizer = get_optimizer(model,solver_type,learning_rate,LBFGS_param)\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(n / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=verbose, factor=0.5, eps=1e-12)\n",
    "    \n",
    "    model.eval() # Evaluation mode\n",
    "    # Compute loss on entire test set\n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device) \n",
    "    \n",
    "    iter_no = 0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        model.train() # Training mode\n",
    "        for ix, (_x, _y, _z) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).to(device)\n",
    "            _y = Variable(_y).to(device)\n",
    "            _z = Variable(_z).to(device)\n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x, _z).float()\n",
    "            loss = loss_func_cvxproblem(yhat, one_hot(_y).to(device), model, _x,_z, beta, rho, device)/len(_y)\n",
    "            correct = torch.eq(torch.argmax(yhat, dim=1), _y).float().sum()/len(_y) # accuracy\n",
    "            #=======backward pass=====================================\n",
    "            optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "            loss.backward() # backpropagate the loss through the model\n",
    "            optimizer.step() # update the gradients w.r.t the loss\n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(one_hot(_y).to(device), model, _x, beta, device)/len(_y)\n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, u_vectors, beta, rho, device)\n",
    "        \n",
    "        if i % 5 == 0 or i == num_epochs - 1:\n",
    "            print(\"Epoch [{:>2}/{:}], noncvx_loss: {:.3f} loss: {:.3f} acc: {:.3f}, TEST noncvx_loss: {:.3f} loss: {:.3f} test acc: {:.3f}\".format(i, num_epochs,\n",
    "                    noncvx_losses[iter_no-1], losses[iter_no-1], accs[iter_no-1], noncvx_losses_test[i+1]/test_len, \n",
    "                    losses_test[i+1]/test_len, accs_test[i+1]/test_len))\n",
    "        scheduler.step(losses[iter_no-1])\n",
    "        \n",
    "    if save_path != '':\n",
    "        save_model(model,save_path)\n",
    "    return {\"losses\":losses, \"accs\":accs, \"noncvx_losses\":noncvx_losses, \"losses_test\":losses_test/test_len,\n",
    "            \"accs_test\":accs_test/test_len, \"noncvx_losses_test\":noncvx_losses_test/test_len, \"times\":np.diff(times), \"model\":model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0176ab9",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Downloads CIFAR10 if not already downloaded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7a1a2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Apatch (Detached A) Shape: torch.Size([50000, 3, 32, 32])\n",
      "A shape: torch.Size([50000, 3072])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(P['dir'], train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(P['dir'], train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), normalize,]))\n",
    "\n",
    "# Extract the data via a dummy loader (dumps entire dataset at once)\n",
    "dummy_loader= torch.utils.data.DataLoader(train_dataset, batch_size=50000, shuffle=False, pin_memory=True, sampler=None)\n",
    "for A, y in dummy_loader:\n",
    "    pass\n",
    "Apatch=A.detach().clone() # Detaches from graph\n",
    "\n",
    "A = A.view(A.shape[0], -1)\n",
    "n,dim_in=A.size()\n",
    "\n",
    "P[\"cvx_n\"] = n\n",
    "\n",
    "print(\"Apatch (Detached A) Shape:\",Apatch.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5413f61",
   "metadata": {},
   "source": [
    "# Nonconvex (Regular) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0313dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=P[\"batch_size\"], shuffle=True,\n",
    "    pin_memory=True, sampler=None)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=P[\"batch_size\"], shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "065f24aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter          : Value\n",
      "seed               : 42\n",
      "device             : cuda\n",
      "verbose            : True\n",
      "P                  : 4096\n",
      "num_neurons        : 4096\n",
      "num_classes        : 10\n",
      "dim_in             : 3072\n",
      "batch_size         : 1000\n",
      "beta               : 0.001\n",
      "dir                : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "ncvx_solver        : sgd\n",
      "ncvx_schedule      : 0\n",
      "ncvx_LBFGS_param   : (10, 4)\n",
      "ncvx_num_epochs    : 100\n",
      "ncvx_learning_rate : 0.001\n",
      "ncvx_train_len     : 50000\n",
      "ncvx_test_len      : 10000\n"
     ]
    }
   ],
   "source": [
    "print_params(P,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a856a12b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc86d574b5be4fd59b7c7389fc875518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], loss: 0.427 acc: 0.330, test loss: 0.424 test acc: 0.349\n",
      "Epoch [1/100], loss: 0.407 acc: 0.391, test loss: 0.405 test acc: 0.386\n",
      "Epoch [2/100], loss: 0.388 acc: 0.433, test loss: 0.396 test acc: 0.406\n",
      "Epoch [3/100], loss: 0.388 acc: 0.406, test loss: 0.390 test acc: 0.418\n",
      "Epoch [4/100], loss: 0.383 acc: 0.399, test loss: 0.387 test acc: 0.420\n",
      "Epoch [5/100], loss: 0.365 acc: 0.495, test loss: 0.382 test acc: 0.432\n",
      "Epoch [6/100], loss: 0.370 acc: 0.451, test loss: 0.379 test acc: 0.435\n",
      "Epoch [7/100], loss: 0.368 acc: 0.459, test loss: 0.377 test acc: 0.441\n",
      "Epoch [8/100], loss: 0.361 acc: 0.496, test loss: 0.374 test acc: 0.446\n",
      "Epoch [9/100], loss: 0.365 acc: 0.482, test loss: 0.372 test acc: 0.450\n",
      "Epoch [10/100], loss: 0.362 acc: 0.492, test loss: 0.371 test acc: 0.451\n",
      "Epoch [11/100], loss: 0.361 acc: 0.494, test loss: 0.370 test acc: 0.458\n",
      "Epoch [12/100], loss: 0.360 acc: 0.486, test loss: 0.368 test acc: 0.459\n",
      "Epoch [13/100], loss: 0.351 acc: 0.523, test loss: 0.367 test acc: 0.460\n",
      "Epoch [14/100], loss: 0.344 acc: 0.507, test loss: 0.367 test acc: 0.460\n",
      "Epoch [15/100], loss: 0.342 acc: 0.515, test loss: 0.366 test acc: 0.466\n",
      "Epoch [16/100], loss: 0.342 acc: 0.540, test loss: 0.365 test acc: 0.467\n",
      "Epoch [17/100], loss: 0.343 acc: 0.537, test loss: 0.364 test acc: 0.469\n",
      "Epoch [18/100], loss: 0.342 acc: 0.536, test loss: 0.363 test acc: 0.471\n",
      "Epoch [19/100], loss: 0.340 acc: 0.535, test loss: 0.363 test acc: 0.473\n",
      "Epoch [20/100], loss: 0.331 acc: 0.576, test loss: 0.362 test acc: 0.475\n",
      "Epoch [21/100], loss: 0.326 acc: 0.567, test loss: 0.361 test acc: 0.477\n",
      "Epoch [22/100], loss: 0.341 acc: 0.545, test loss: 0.360 test acc: 0.477\n",
      "Epoch [23/100], loss: 0.340 acc: 0.533, test loss: 0.360 test acc: 0.478\n",
      "Epoch [24/100], loss: 0.332 acc: 0.548, test loss: 0.359 test acc: 0.480\n",
      "Epoch [25/100], loss: 0.334 acc: 0.553, test loss: 0.359 test acc: 0.480\n",
      "Epoch [26/100], loss: 0.332 acc: 0.543, test loss: 0.358 test acc: 0.483\n",
      "Epoch [27/100], loss: 0.333 acc: 0.561, test loss: 0.358 test acc: 0.481\n",
      "Epoch [28/100], loss: 0.329 acc: 0.560, test loss: 0.358 test acc: 0.480\n",
      "Epoch [29/100], loss: 0.329 acc: 0.548, test loss: 0.357 test acc: 0.484\n",
      "Epoch [30/100], loss: 0.327 acc: 0.565, test loss: 0.357 test acc: 0.487\n",
      "Epoch [31/100], loss: 0.333 acc: 0.544, test loss: 0.356 test acc: 0.488\n",
      "Epoch [32/100], loss: 0.317 acc: 0.594, test loss: 0.356 test acc: 0.488\n",
      "Epoch [33/100], loss: 0.322 acc: 0.592, test loss: 0.356 test acc: 0.489\n",
      "Epoch [34/100], loss: 0.323 acc: 0.567, test loss: 0.355 test acc: 0.490\n",
      "Epoch [35/100], loss: 0.320 acc: 0.591, test loss: 0.355 test acc: 0.491\n",
      "Epoch [36/100], loss: 0.323 acc: 0.592, test loss: 0.355 test acc: 0.493\n",
      "Epoch [37/100], loss: 0.318 acc: 0.596, test loss: 0.355 test acc: 0.492\n",
      "Epoch [38/100], loss: 0.322 acc: 0.584, test loss: 0.355 test acc: 0.495\n",
      "Epoch [39/100], loss: 0.312 acc: 0.630, test loss: 0.354 test acc: 0.493\n",
      "Epoch [40/100], loss: 0.318 acc: 0.601, test loss: 0.354 test acc: 0.495\n",
      "Epoch [41/100], loss: 0.306 acc: 0.615, test loss: 0.353 test acc: 0.497\n",
      "Epoch [42/100], loss: 0.316 acc: 0.600, test loss: 0.353 test acc: 0.496\n",
      "Epoch [43/100], loss: 0.307 acc: 0.612, test loss: 0.353 test acc: 0.495\n",
      "Epoch [44/100], loss: 0.308 acc: 0.612, test loss: 0.354 test acc: 0.497\n",
      "Epoch [45/100], loss: 0.308 acc: 0.630, test loss: 0.353 test acc: 0.498\n",
      "Epoch [46/100], loss: 0.305 acc: 0.638, test loss: 0.353 test acc: 0.498\n",
      "Epoch [47/100], loss: 0.306 acc: 0.636, test loss: 0.353 test acc: 0.501\n",
      "Epoch [48/100], loss: 0.316 acc: 0.577, test loss: 0.352 test acc: 0.501\n",
      "Epoch [49/100], loss: 0.313 acc: 0.603, test loss: 0.352 test acc: 0.497\n",
      "Epoch [50/100], loss: 0.309 acc: 0.602, test loss: 0.352 test acc: 0.500\n",
      "Epoch [51/100], loss: 0.299 acc: 0.623, test loss: 0.352 test acc: 0.500\n",
      "Epoch [52/100], loss: 0.307 acc: 0.612, test loss: 0.352 test acc: 0.502\n",
      "Epoch [53/100], loss: 0.303 acc: 0.647, test loss: 0.351 test acc: 0.499\n",
      "Epoch [54/100], loss: 0.309 acc: 0.607, test loss: 0.351 test acc: 0.502\n",
      "Epoch [55/100], loss: 0.305 acc: 0.629, test loss: 0.352 test acc: 0.501\n",
      "Epoch [56/100], loss: 0.304 acc: 0.622, test loss: 0.351 test acc: 0.503\n",
      "Epoch [57/100], loss: 0.308 acc: 0.617, test loss: 0.351 test acc: 0.505\n",
      "Epoch [58/100], loss: 0.306 acc: 0.620, test loss: 0.351 test acc: 0.501\n",
      "Epoch [59/100], loss: 0.297 acc: 0.656, test loss: 0.351 test acc: 0.505\n",
      "Epoch [60/100], loss: 0.295 acc: 0.656, test loss: 0.351 test acc: 0.507\n",
      "Epoch [61/100], loss: 0.301 acc: 0.628, test loss: 0.350 test acc: 0.506\n",
      "Epoch [62/100], loss: 0.293 acc: 0.665, test loss: 0.351 test acc: 0.506\n",
      "Epoch [63/100], loss: 0.293 acc: 0.657, test loss: 0.351 test acc: 0.505\n",
      "Epoch [64/100], loss: 0.297 acc: 0.646, test loss: 0.350 test acc: 0.503\n",
      "Epoch [65/100], loss: 0.299 acc: 0.628, test loss: 0.350 test acc: 0.508\n",
      "Epoch [66/100], loss: 0.298 acc: 0.636, test loss: 0.350 test acc: 0.508\n",
      "Epoch [67/100], loss: 0.294 acc: 0.649, test loss: 0.350 test acc: 0.508\n",
      "Epoch [68/100], loss: 0.291 acc: 0.655, test loss: 0.350 test acc: 0.509\n",
      "Epoch [69/100], loss: 0.287 acc: 0.681, test loss: 0.350 test acc: 0.511\n",
      "Epoch [70/100], loss: 0.296 acc: 0.678, test loss: 0.350 test acc: 0.508\n",
      "Epoch [71/100], loss: 0.290 acc: 0.666, test loss: 0.350 test acc: 0.508\n",
      "Epoch [72/100], loss: 0.290 acc: 0.670, test loss: 0.350 test acc: 0.511\n",
      "Epoch [73/100], loss: 0.289 acc: 0.666, test loss: 0.349 test acc: 0.508\n",
      "Epoch [74/100], loss: 0.290 acc: 0.669, test loss: 0.350 test acc: 0.512\n",
      "Epoch [75/100], loss: 0.284 acc: 0.680, test loss: 0.349 test acc: 0.511\n",
      "Epoch [76/100], loss: 0.288 acc: 0.667, test loss: 0.350 test acc: 0.509\n",
      "Epoch [77/100], loss: 0.284 acc: 0.683, test loss: 0.350 test acc: 0.512\n",
      "Epoch [78/100], loss: 0.286 acc: 0.676, test loss: 0.349 test acc: 0.512\n",
      "Epoch [79/100], loss: 0.279 acc: 0.685, test loss: 0.350 test acc: 0.508\n",
      "Epoch [80/100], loss: 0.284 acc: 0.691, test loss: 0.349 test acc: 0.513\n",
      "Epoch [81/100], loss: 0.284 acc: 0.702, test loss: 0.350 test acc: 0.512\n",
      "Epoch [82/100], loss: 0.273 acc: 0.725, test loss: 0.349 test acc: 0.514\n",
      "Epoch [83/100], loss: 0.283 acc: 0.681, test loss: 0.349 test acc: 0.512\n",
      "Epoch [84/100], loss: 0.281 acc: 0.684, test loss: 0.349 test acc: 0.512\n",
      "Epoch [85/100], loss: 0.276 acc: 0.692, test loss: 0.349 test acc: 0.515\n",
      "Epoch [86/100], loss: 0.281 acc: 0.688, test loss: 0.349 test acc: 0.514\n",
      "Epoch [87/100], loss: 0.278 acc: 0.686, test loss: 0.349 test acc: 0.511\n",
      "Epoch [88/100], loss: 0.282 acc: 0.695, test loss: 0.349 test acc: 0.512\n",
      "Epoch [89/100], loss: 0.282 acc: 0.693, test loss: 0.349 test acc: 0.517\n",
      "Epoch [90/100], loss: 0.276 acc: 0.715, test loss: 0.349 test acc: 0.514\n",
      "Epoch [91/100], loss: 0.278 acc: 0.703, test loss: 0.350 test acc: 0.517\n",
      "Epoch [92/100], loss: 0.274 acc: 0.716, test loss: 0.349 test acc: 0.517\n",
      "Epoch [93/100], loss: 0.272 acc: 0.723, test loss: 0.349 test acc: 0.517\n",
      "Epoch [94/100], loss: 0.282 acc: 0.691, test loss: 0.349 test acc: 0.516\n",
      "Epoch [95/100], loss: 0.272 acc: 0.712, test loss: 0.349 test acc: 0.519\n",
      "Epoch [96/100], loss: 0.275 acc: 0.711, test loss: 0.349 test acc: 0.518\n",
      "Epoch [97/100], loss: 0.269 acc: 0.715, test loss: 0.350 test acc: 0.518\n",
      "Epoch [98/100], loss: 0.277 acc: 0.703, test loss: 0.349 test acc: 0.517\n",
      "Epoch [99/100], loss: 0.270 acc: 0.727, test loss: 0.349 test acc: 0.517\n"
     ]
    }
   ],
   "source": [
    "model = FCNetwork(P[\"num_neurons\"], P[\"num_classes\"], P[\"dim_in\"])\n",
    "\n",
    "# Save initial model\n",
    "model.apply(lambda m: init.xavier_normal_(m.weight.data))\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(model,ncvx_save_loc+\"_INITIAL\")\n",
    "\n",
    "# Run model (will save after 100 epochs) and report stats, normal training\n",
    "results_ncvx = ncvxproblem(train_loader, test_loader, model, save_path=ncvx_save_loc, \n",
    "                           num_epochs = P[\"ncvx_num_epochs\"],\n",
    "                           beta = P[\"beta\"],\n",
    "                           learning_rate = P[\"ncvx_learning_rate\"],\n",
    "                           batch_size = P[\"batch_size\"],\n",
    "                           solver_type = P[\"ncvx_solver\"],\n",
    "                           schedule = P[\"ncvx_schedule\"],\n",
    "                           LBFGS_param = P[\"ncvx_LBFGS_param\"],\n",
    "                           verbose = P[\"verbose\"],\n",
    "                           test_len = P[\"ncvx_test_len\"],\n",
    "                           train_len = P[\"ncvx_train_len\"],\n",
    "                           device = P[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a175e1",
   "metadata": {},
   "source": [
    "# Convex Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a1d4d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter         : Value\n",
      "seed              : 42\n",
      "device            : cuda\n",
      "verbose           : True\n",
      "P                 : 4096\n",
      "num_neurons       : 4096\n",
      "num_classes       : 10\n",
      "dim_in            : 3072\n",
      "batch_size        : 1000\n",
      "beta              : 0.001\n",
      "dir               : C:\\Users\\trevo\\Documents\\repos\\spring22\\convex_nn\n",
      "cvx_solver        : sgd\n",
      "cvx_LBFGS_param   : (10, 4)\n",
      "cvx_num_epochs    : 100\n",
      "cvx_learning_rate : 5e-07\n",
      "cvx_rho           : 0.01\n",
      "cvx_test_len      : 10000\n",
      "cvx_n             : 50000\n"
     ]
    }
   ],
   "source": [
    "print_params(P,True,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1f3b412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 4096\n"
     ]
    }
   ],
   "source": [
    "# Generate sign patterns for convex network\n",
    "num_neurons,sign_pattern_list, u_vector_list = generate_sign_patterns(A, P[\"P\"], P[\"verbose\"])\n",
    "sign_patterns = np.array([sign_pattern_list[i].int().data.numpy() for i in range(num_neurons)])\n",
    "u_vectors = np.asarray(u_vector_list).reshape((num_neurons, A.shape[1])).T\n",
    "\n",
    "ds_train = PrepareData3D(X=A, y=y, z=sign_patterns.T)\n",
    "ds_train = DataLoader(ds_train, batch_size=P[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=P[\"batch_size\"], shuffle=False,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7b9f8a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1e423b4564b8db91df170a4ba0778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 0/100], noncvx_loss: 0.413 loss: 0.373 acc: 0.447, TEST noncvx_loss: 0.411 loss: 0.366 test acc: 0.470\n",
      "Epoch [ 1/100], noncvx_loss: 0.419 loss: 0.346 acc: 0.522, TEST noncvx_loss: 0.422 loss: 0.355 test acc: 0.491\n",
      "Epoch [ 2/100], noncvx_loss: 0.427 loss: 0.335 acc: 0.540, TEST noncvx_loss: 0.428 loss: 0.351 test acc: 0.503\n",
      "Epoch [ 3/100], noncvx_loss: 0.423 loss: 0.314 acc: 0.616, TEST noncvx_loss: 0.431 loss: 0.347 test acc: 0.512\n",
      "Epoch [ 4/100], noncvx_loss: 0.431 loss: 0.313 acc: 0.602, TEST noncvx_loss: 0.433 loss: 0.344 test acc: 0.518\n",
      "Epoch [ 5/100], noncvx_loss: 0.430 loss: 0.300 acc: 0.649, TEST noncvx_loss: 0.439 loss: 0.345 test acc: 0.510\n",
      "Epoch [ 6/100], noncvx_loss: 0.433 loss: 0.291 acc: 0.666, TEST noncvx_loss: 0.437 loss: 0.341 test acc: 0.523\n",
      "Epoch [ 7/100], noncvx_loss: 0.434 loss: 0.285 acc: 0.686, TEST noncvx_loss: 0.441 loss: 0.340 test acc: 0.523\n",
      "Epoch [ 8/100], noncvx_loss: 0.433 loss: 0.274 acc: 0.715, TEST noncvx_loss: 0.442 loss: 0.340 test acc: 0.528\n",
      "Epoch [ 9/100], noncvx_loss: 0.435 loss: 0.276 acc: 0.720, TEST noncvx_loss: 0.441 loss: 0.338 test acc: 0.526\n",
      "Epoch [10/100], noncvx_loss: 0.432 loss: 0.257 acc: 0.779, TEST noncvx_loss: 0.442 loss: 0.338 test acc: 0.532\n",
      "Epoch [11/100], noncvx_loss: 0.435 loss: 0.257 acc: 0.757, TEST noncvx_loss: 0.443 loss: 0.337 test acc: 0.533\n",
      "Epoch [12/100], noncvx_loss: 0.434 loss: 0.244 acc: 0.792, TEST noncvx_loss: 0.443 loss: 0.337 test acc: 0.535\n",
      "Epoch [13/100], noncvx_loss: 0.433 loss: 0.245 acc: 0.793, TEST noncvx_loss: 0.444 loss: 0.337 test acc: 0.535\n",
      "Epoch [14/100], noncvx_loss: 0.435 loss: 0.240 acc: 0.817, TEST noncvx_loss: 0.445 loss: 0.337 test acc: 0.536\n",
      "Epoch [15/100], noncvx_loss: 0.434 loss: 0.238 acc: 0.811, TEST noncvx_loss: 0.446 loss: 0.338 test acc: 0.535\n",
      "Epoch [16/100], noncvx_loss: 0.433 loss: 0.231 acc: 0.822, TEST noncvx_loss: 0.446 loss: 0.337 test acc: 0.538\n",
      "Epoch [17/100], noncvx_loss: 0.440 loss: 0.233 acc: 0.818, TEST noncvx_loss: 0.447 loss: 0.338 test acc: 0.537\n",
      "Epoch [18/100], noncvx_loss: 0.435 loss: 0.224 acc: 0.848, TEST noncvx_loss: 0.447 loss: 0.337 test acc: 0.540\n",
      "Epoch [19/100], noncvx_loss: 0.437 loss: 0.219 acc: 0.859, TEST noncvx_loss: 0.447 loss: 0.338 test acc: 0.536\n",
      "Epoch [20/100], noncvx_loss: 0.435 loss: 0.213 acc: 0.882, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.539\n",
      "Epoch [21/100], noncvx_loss: 0.433 loss: 0.211 acc: 0.864, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.543\n",
      "Epoch [22/100], noncvx_loss: 0.438 loss: 0.211 acc: 0.870, TEST noncvx_loss: 0.448 loss: 0.338 test acc: 0.540\n",
      "Epoch [23/100], noncvx_loss: 0.438 loss: 0.203 acc: 0.896, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.539\n",
      "Epoch [24/100], noncvx_loss: 0.434 loss: 0.201 acc: 0.889, TEST noncvx_loss: 0.448 loss: 0.337 test acc: 0.536\n",
      "Epoch [25/100], noncvx_loss: 0.438 loss: 0.194 acc: 0.904, TEST noncvx_loss: 0.450 loss: 0.337 test acc: 0.543\n",
      "Epoch [26/100], noncvx_loss: 0.437 loss: 0.193 acc: 0.909, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.540\n",
      "Epoch [27/100], noncvx_loss: 0.439 loss: 0.194 acc: 0.901, TEST noncvx_loss: 0.450 loss: 0.338 test acc: 0.541\n",
      "Epoch [28/100], noncvx_loss: 0.440 loss: 0.195 acc: 0.917, TEST noncvx_loss: 0.449 loss: 0.338 test acc: 0.541\n",
      "Epoch [29/100], noncvx_loss: 0.439 loss: 0.187 acc: 0.920, TEST noncvx_loss: 0.451 loss: 0.338 test acc: 0.539\n",
      "Epoch [30/100], noncvx_loss: 0.439 loss: 0.185 acc: 0.923, TEST noncvx_loss: 0.449 loss: 0.339 test acc: 0.543\n",
      "Epoch [31/100], noncvx_loss: 0.439 loss: 0.179 acc: 0.932, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.541\n",
      "Epoch [32/100], noncvx_loss: 0.433 loss: 0.172 acc: 0.939, TEST noncvx_loss: 0.449 loss: 0.339 test acc: 0.542\n",
      "Epoch [33/100], noncvx_loss: 0.437 loss: 0.172 acc: 0.937, TEST noncvx_loss: 0.450 loss: 0.339 test acc: 0.543\n",
      "Epoch [34/100], noncvx_loss: 0.439 loss: 0.176 acc: 0.921, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.543\n",
      "Epoch [35/100], noncvx_loss: 0.437 loss: 0.169 acc: 0.955, TEST noncvx_loss: 0.451 loss: 0.339 test acc: 0.540\n",
      "Epoch [36/100], noncvx_loss: 0.434 loss: 0.166 acc: 0.959, TEST noncvx_loss: 0.452 loss: 0.340 test acc: 0.541\n",
      "Epoch [37/100], noncvx_loss: 0.438 loss: 0.168 acc: 0.950, TEST noncvx_loss: 0.450 loss: 0.340 test acc: 0.544\n",
      "Epoch [38/100], noncvx_loss: 0.440 loss: 0.162 acc: 0.954, TEST noncvx_loss: 0.452 loss: 0.341 test acc: 0.540\n",
      "Epoch [39/100], noncvx_loss: 0.444 loss: 0.162 acc: 0.967, TEST noncvx_loss: 0.451 loss: 0.341 test acc: 0.541\n",
      "Epoch [40/100], noncvx_loss: 0.437 loss: 0.156 acc: 0.952, TEST noncvx_loss: 0.451 loss: 0.341 test acc: 0.540\n",
      "Epoch [41/100], noncvx_loss: 0.442 loss: 0.160 acc: 0.955, TEST noncvx_loss: 0.452 loss: 0.341 test acc: 0.543\n",
      "Epoch [42/100], noncvx_loss: 0.444 loss: 0.161 acc: 0.944, TEST noncvx_loss: 0.453 loss: 0.342 test acc: 0.542\n",
      "Epoch [43/100], noncvx_loss: 0.442 loss: 0.154 acc: 0.965, TEST noncvx_loss: 0.452 loss: 0.342 test acc: 0.543\n",
      "Epoch [44/100], noncvx_loss: 0.436 loss: 0.151 acc: 0.971, TEST noncvx_loss: 0.451 loss: 0.342 test acc: 0.542\n",
      "Epoch [45/100], noncvx_loss: 0.438 loss: 0.145 acc: 0.970, TEST noncvx_loss: 0.451 loss: 0.342 test acc: 0.540\n",
      "Epoch [46/100], noncvx_loss: 0.437 loss: 0.146 acc: 0.973, TEST noncvx_loss: 0.453 loss: 0.343 test acc: 0.539\n",
      "Epoch [47/100], noncvx_loss: 0.436 loss: 0.142 acc: 0.975, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.539\n",
      "Epoch [48/100], noncvx_loss: 0.440 loss: 0.146 acc: 0.970, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.541\n",
      "Epoch [49/100], noncvx_loss: 0.440 loss: 0.141 acc: 0.971, TEST noncvx_loss: 0.452 loss: 0.343 test acc: 0.542\n",
      "Epoch [50/100], noncvx_loss: 0.431 loss: 0.139 acc: 0.969, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.542\n",
      "Epoch [51/100], noncvx_loss: 0.439 loss: 0.134 acc: 0.968, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.540\n",
      "Epoch [52/100], noncvx_loss: 0.435 loss: 0.132 acc: 0.980, TEST noncvx_loss: 0.452 loss: 0.344 test acc: 0.545\n",
      "Epoch [53/100], noncvx_loss: 0.440 loss: 0.139 acc: 0.967, TEST noncvx_loss: 0.452 loss: 0.344 test acc: 0.543\n",
      "Epoch [54/100], noncvx_loss: 0.443 loss: 0.137 acc: 0.973, TEST noncvx_loss: 0.452 loss: 0.345 test acc: 0.540\n",
      "Epoch [55/100], noncvx_loss: 0.436 loss: 0.133 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.344 test acc: 0.544\n",
      "Epoch [56/100], noncvx_loss: 0.443 loss: 0.132 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.345 test acc: 0.539\n",
      "Epoch [57/100], noncvx_loss: 0.443 loss: 0.134 acc: 0.969, TEST noncvx_loss: 0.453 loss: 0.345 test acc: 0.542\n",
      "Epoch [58/100], noncvx_loss: 0.437 loss: 0.129 acc: 0.984, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.541\n",
      "Epoch [59/100], noncvx_loss: 0.441 loss: 0.127 acc: 0.980, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.540\n",
      "Epoch [60/100], noncvx_loss: 0.436 loss: 0.126 acc: 0.979, TEST noncvx_loss: 0.453 loss: 0.346 test acc: 0.541\n",
      "Epoch [61/100], noncvx_loss: 0.438 loss: 0.126 acc: 0.979, TEST noncvx_loss: 0.452 loss: 0.347 test acc: 0.541\n",
      "Epoch [62/100], noncvx_loss: 0.441 loss: 0.125 acc: 0.978, TEST noncvx_loss: 0.454 loss: 0.347 test acc: 0.540\n",
      "Epoch [63/100], noncvx_loss: 0.435 loss: 0.118 acc: 0.982, TEST noncvx_loss: 0.453 loss: 0.347 test acc: 0.539\n",
      "Epoch [64/100], noncvx_loss: 0.444 loss: 0.120 acc: 0.984, TEST noncvx_loss: 0.454 loss: 0.348 test acc: 0.538\n",
      "Epoch [65/100], noncvx_loss: 0.436 loss: 0.113 acc: 0.990, TEST noncvx_loss: 0.454 loss: 0.348 test acc: 0.539\n",
      "Epoch [66/100], noncvx_loss: 0.441 loss: 0.114 acc: 0.987, TEST noncvx_loss: 0.455 loss: 0.349 test acc: 0.539\n",
      "Epoch [67/100], noncvx_loss: 0.440 loss: 0.114 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.349 test acc: 0.537\n",
      "Epoch [68/100], noncvx_loss: 0.443 loss: 0.112 acc: 0.987, TEST noncvx_loss: 0.454 loss: 0.349 test acc: 0.539\n",
      "Epoch [69/100], noncvx_loss: 0.436 loss: 0.116 acc: 0.981, TEST noncvx_loss: 0.453 loss: 0.349 test acc: 0.540\n",
      "Epoch [70/100], noncvx_loss: 0.439 loss: 0.115 acc: 0.987, TEST noncvx_loss: 0.455 loss: 0.350 test acc: 0.540\n",
      "Epoch [71/100], noncvx_loss: 0.440 loss: 0.110 acc: 0.985, TEST noncvx_loss: 0.454 loss: 0.350 test acc: 0.537\n",
      "Epoch [72/100], noncvx_loss: 0.440 loss: 0.112 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.536\n",
      "Epoch [73/100], noncvx_loss: 0.439 loss: 0.111 acc: 0.985, TEST noncvx_loss: 0.455 loss: 0.351 test acc: 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], noncvx_loss: 0.436 loss: 0.105 acc: 0.988, TEST noncvx_loss: 0.454 loss: 0.350 test acc: 0.538\n",
      "Epoch [75/100], noncvx_loss: 0.436 loss: 0.107 acc: 0.988, TEST noncvx_loss: 0.453 loss: 0.350 test acc: 0.539\n",
      "Epoch [76/100], noncvx_loss: 0.438 loss: 0.109 acc: 0.986, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.539\n",
      "Epoch [77/100], noncvx_loss: 0.440 loss: 0.104 acc: 0.988, TEST noncvx_loss: 0.456 loss: 0.352 test acc: 0.538\n",
      "Epoch [78/100], noncvx_loss: 0.442 loss: 0.106 acc: 0.992, TEST noncvx_loss: 0.454 loss: 0.351 test acc: 0.535\n",
      "Epoch [79/100], noncvx_loss: 0.442 loss: 0.104 acc: 0.989, TEST noncvx_loss: 0.455 loss: 0.352 test acc: 0.539\n",
      "Epoch [80/100], noncvx_loss: 0.442 loss: 0.100 acc: 0.991, TEST noncvx_loss: 0.455 loss: 0.352 test acc: 0.540\n",
      "Epoch [81/100], noncvx_loss: 0.442 loss: 0.103 acc: 0.987, TEST noncvx_loss: 0.454 loss: 0.352 test acc: 0.541\n",
      "Epoch [82/100], noncvx_loss: 0.443 loss: 0.100 acc: 0.992, TEST noncvx_loss: 0.455 loss: 0.353 test acc: 0.536\n",
      "Epoch [83/100], noncvx_loss: 0.440 loss: 0.098 acc: 0.991, TEST noncvx_loss: 0.454 loss: 0.353 test acc: 0.538\n",
      "Epoch [84/100], noncvx_loss: 0.442 loss: 0.098 acc: 0.992, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.536\n",
      "Epoch [85/100], noncvx_loss: 0.437 loss: 0.094 acc: 0.997, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.538\n",
      "Epoch [86/100], noncvx_loss: 0.433 loss: 0.096 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.539\n",
      "Epoch [87/100], noncvx_loss: 0.439 loss: 0.093 acc: 0.993, TEST noncvx_loss: 0.455 loss: 0.354 test acc: 0.538\n",
      "Epoch [88/100], noncvx_loss: 0.437 loss: 0.092 acc: 0.996, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.536\n",
      "Epoch [89/100], noncvx_loss: 0.437 loss: 0.095 acc: 0.997, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.539\n",
      "Epoch [90/100], noncvx_loss: 0.441 loss: 0.094 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.535\n",
      "Epoch [91/100], noncvx_loss: 0.440 loss: 0.092 acc: 0.990, TEST noncvx_loss: 0.455 loss: 0.355 test acc: 0.538\n",
      "Epoch [92/100], noncvx_loss: 0.439 loss: 0.094 acc: 0.996, TEST noncvx_loss: 0.454 loss: 0.355 test acc: 0.537\n",
      "Epoch [93/100], noncvx_loss: 0.443 loss: 0.092 acc: 0.994, TEST noncvx_loss: 0.455 loss: 0.356 test acc: 0.536\n",
      "Epoch [94/100], noncvx_loss: 0.443 loss: 0.089 acc: 0.988, TEST noncvx_loss: 0.456 loss: 0.356 test acc: 0.536\n",
      "Epoch [95/100], noncvx_loss: 0.441 loss: 0.093 acc: 0.990, TEST noncvx_loss: 0.455 loss: 0.356 test acc: 0.534\n",
      "Epoch [96/100], noncvx_loss: 0.439 loss: 0.087 acc: 0.993, TEST noncvx_loss: 0.456 loss: 0.357 test acc: 0.538\n",
      "Epoch [97/100], noncvx_loss: 0.436 loss: 0.087 acc: 0.989, TEST noncvx_loss: 0.456 loss: 0.357 test acc: 0.534\n",
      "Epoch [98/100], noncvx_loss: 0.440 loss: 0.085 acc: 0.996, TEST noncvx_loss: 0.455 loss: 0.357 test acc: 0.535\n",
      "Epoch [99/100], noncvx_loss: 0.439 loss: 0.082 acc: 0.998, TEST noncvx_loss: 0.456 loss: 0.358 test acc: 0.536\n"
     ]
    }
   ],
   "source": [
    "cvx_save_loc = \"models/cvx_nn{:}_solver{:}_lr5e-7\".format(P['num_neurons'],P['cvx_solver'])\n",
    "\n",
    "results_cvx = cvxproblem(ds_train, test_loader, save_path=cvx_save_loc,\n",
    "                         num_epochs = P[\"cvx_num_epochs\"],\n",
    "                         num_neurons = P[\"num_neurons\"], \n",
    "                         beta = P[\"beta\"],\n",
    "                         learning_rate = P[\"cvx_learning_rate\"],\n",
    "                         batch_size = P[\"batch_size\"],\n",
    "                         rho = P[\"cvx_rho\"],\n",
    "                         u_vectors = u_vectors,\n",
    "                         solver_type = P[\"cvx_solver\"],\n",
    "                         LBFGS_param = P[\"cvx_LBFGS_param\"],\n",
    "                         verbose = P[\"verbose\"],\n",
    "                         num_classes = P[\"num_classes\"],\n",
    "                         dim_in = P[\"dim_in\"],\n",
    "                         n = n,\n",
    "                         test_len = P['cvx_test_len'],\n",
    "                         device = P[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642aed5e",
   "metadata": {},
   "source": [
    "# Save Parameters (USE SAME FILENAME STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405be258",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_save_loc = 'models/nn{:}_solver{:}_lr_PARAMS.json'.format(P['num_neurons'],P['cvx_solver'])\n",
    "save_params(P,param_save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d8359",
   "metadata": {},
   "source": [
    "# Save Training Data (Uses same filename style as model files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a470160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NonConvex output to DataFrame\n",
    "ncvx_model = results_ncvx[\"model\"]\n",
    "del results_ncvx[\"model\"]\n",
    "\n",
    "ncvx_iters = int(np.ceil(P[\"ncvx_train_len\"] / P[\"batch_size\"]))\n",
    "results_ncvx[\"losses_test\"] = np.repeat(results_ncvx[\"losses_test\"][1:],ncvx_iters)\n",
    "results_ncvx[\"accs_test\"] = np.repeat(results_ncvx[\"accs_test\"][1:],ncvx_iters)\n",
    "results_ncvx[\"epoch\"] = np.repeat(np.arange(P[\"ncvx_num_epochs\"]),ncvx_iters)\n",
    "\n",
    "df_ncvx = pd.DataFrame.from_dict(results_ncvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac64338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncvx.to_csv(ncvx_save_loc+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex output to DataFrame\n",
    "cvx_model = results_cvx[\"model\"]\n",
    "del results_cvx[\"model\"]\n",
    "\n",
    "cvx_iters = int(np.ceil(P[\"cvx_n\"] / P[\"batch_size\"]))\n",
    "results_cvx[\"losses_test\"] = np.repeat(results_cvx[\"losses_test\"][1:],cvx_iters)\n",
    "results_cvx[\"accs_test\"] = np.repeat(results_cvx[\"accs_test\"][1:],cvx_iters)\n",
    "results_cvx[\"noncvx_losses_test\"] = np.repeat(results_cvx[\"noncvx_losses_test\"][1:],cvx_iters)\n",
    "results_cvx[\"epoch\"] = np.repeat(np.arange(P[\"cvx_num_epochs\"]),cvx_iters)\n",
    "\n",
    "df_cvx = pd.DataFrame.from_dict(results_cvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427ccc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cvx.to_csv(cvx_save_loc+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7aa7ac",
   "metadata": {},
   "source": [
    "# Lottery Ticket \n",
    "Borrows from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c66e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6843cdb9",
   "metadata": {},
   "source": [
    "# TODO: \n",
    " * Better plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
